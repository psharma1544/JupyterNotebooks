{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:08.846236Z",
     "start_time": "2025-05-01T13:36:08.830270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import os.path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.errors import EmptyDataError\n",
    "from scipy import stats\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ],
   "outputs": [],
   "execution_count": 108
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- Work on the seed value under function generate_ema. The value is currently coming for very first date 1/1/2020 which was a holiday. Need to get valye for the first business day somehow.\n",
    "- Take care of the warnings. Currently they are suppressed. Take out the option \"pd.options.mode.chained_assignment = None\" and \n",
    "    \"warnings.simplefilter(action='ignore', category=FutureWarning)\"\n",
    "- To generate records for the first time, the 'dt_target' is manully set to a certain date. The 'dfrmm' under generate_mean() is the full DFRM for whatever SQL has for that symbol. It is possible that manually set dt_target may be older than the oldest record in SQL for a symbol. Need to handle that scenario better."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:08.856655Z",
     "start_time": "2025-05-01T13:36:08.853650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BASE_DIR = '../../../../workspace/HelloPython/HistoricalMarketData/TechnicalIndicators'\n",
    "TABLE_EQUITIES_DATA = 'equities_historic_data'\n",
    "DURATIONS = (14, 30, 90, 200)  # Roughly for bi-weekly, monthly, quarterly, and 200 days running averages"
   ],
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:08.888161Z",
     "start_time": "2025-05-01T13:36:08.880986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    DB = os.environ[\"DB\"]\n",
    "    DB_USER = os.environ[\"DB_USER\"]\n",
    "    DB_PWD = os.environ[\"DB_PWD\"]\n",
    "except KeyError:\n",
    "    raise Exception(\"Required environment variables DB_USER and DB_PWD not set\")\n",
    "DB_URL = 'mysql+mysqlconnector://' + DB_USER + ':' + DB_PWD + '@localhost/' + DB\n",
    "ENGINE = create_engine(DB_URL)"
   ],
   "outputs": [],
   "execution_count": 110
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:08.905846Z",
     "start_time": "2025-05-01T13:36:08.903427Z"
    }
   },
   "source": [
    "# def executeQuery(connection, query):\n",
    "#     dfrm = pd.DataFrame()\n",
    "#     try:\n",
    "#         dfrm = pd.read_sql(query, connection)\n",
    "#     except ProgrammingError as err:\n",
    "#         print(err)\n",
    "#     return dfrm"
   ],
   "outputs": [],
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:08.924599Z",
     "start_time": "2025-05-01T13:36:08.920808Z"
    }
   },
   "source": [
    "def retrieve_cataloged_market_data(symbol, tblName, startDate, endDate=datetime.strftime(datetime.today(), '%Y%m%d')):\n",
    "    query = \"select date, symbol, open, high, low, close, netChange, pcntChange, volume from \" + tblName + \" \\\n",
    "        where symbol like '\" + symbol + \"' and date > '\" + startDate + \"' and date < '\" + endDate + \"'\"\n",
    "\n",
    "    with ENGINE.connect() as conn:\n",
    "        res = conn.execute(text(query))\n",
    "    dfrm = pd.DataFrame(res.mappings().all())\n",
    "    dfrm.set_index('date', inplace=True)\n",
    "    return dfrm"
   ],
   "outputs": [],
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:08.942893Z",
     "start_time": "2025-05-01T13:36:08.939456Z"
    }
   },
   "source": [
    "def generate_file_path(symbol, date=None):\n",
    "    \"\"\"\n",
    "    Generates a file path for a given symbol\n",
    "    to retrieve market data from\n",
    "    :param symbol: ticker\n",
    "    :param date: date embedded in the file name\n",
    "    :return: file name and path\n",
    "    \"\"\"\n",
    "    if date is not None:\n",
    "        str_date = datetime.strftime(date, '%Y%m%d')\n",
    "        file_name = symbol.lower() + '_' + str_date + '.csv'\n",
    "    else:\n",
    "        file_name = symbol.lower() + '.csv'\n",
    "    file_path = os.path.join(BASE_DIR, file_name)\n",
    "    if file_path is None:\n",
    "        print('Could not find file for symbol:{}'.format(symbol))\n",
    "    # print(file_path)\n",
    "    return file_path, file_name"
   ],
   "outputs": [],
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:08.962152Z",
     "start_time": "2025-05-01T13:36:08.956547Z"
    }
   },
   "source": [
    "def generate_mean(dfrm, dt_target, duration):\n",
    "    \"\"\"\n",
    "    generate means for daily 'close' values\n",
    "    :param dfrm: A DataFrame that must contain date as index and 'close' values minimally among others\n",
    "    :param dt_target: Datetime.datetime() date that confirms the starting point for our calculations\n",
    "    :param duration: # of days as int that denote bi-weekly, monthly, quartlery... cycles\n",
    "    :return dfrm_returned: DataFrame with means values added as column \n",
    "    \"\"\"\n",
    "    # Calculate mean for the last N days with current date as the last date\n",
    "    dt_start = dt_target - timedelta(days=duration)\n",
    "    \"\"\"\n",
    "    For date in range (dt_target to end):\n",
    "        calculate some_stat(some_daily_values) for (current date - duration) through current date\n",
    "    Then reduce the size otherwise length of new column will not match\n",
    "    \"\"\"\n",
    "    # dt_target is datetime.datetime type but dfrm has datetime.date objects as index. Requires a conversion before other ops\n",
    "    means = [dfrm.loc[date - timedelta(days=duration): date, 'close'].mean() for date in\n",
    "             dfrm[datetime.date(dt_target):].index]\n",
    "    dfrm_reduced = dfrm.loc[datetime.date(dt_target):]\n",
    "    dfrm_reduced.loc[:, 'mean_' + str(duration)] = means\n",
    "\n",
    "    \"\"\" \n",
    "    More adjustments required because of the need to return data \n",
    "    for '- timedelta(days=duration)' as well for downstream calculations.\n",
    "    Retrieve stats from smaller 'reduced' dfrm and populate the 'returned' dfrm. \n",
    "    \"\"\"\n",
    "    dfrm_returned = dfrm.loc[datetime.date(dt_target) - timedelta(days=duration):]\n",
    "    dfrm_returned.loc[:, 'mean_' + str(duration)] = [np.nan if date not in dfrm_reduced.index\n",
    "                                                     else dfrm_reduced.loc[date, 'mean_' + str(duration)]\n",
    "                                                     for date in dfrm_returned.index]\n",
    "    return dfrm_returned"
   ],
   "outputs": [],
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:08.998002Z",
     "start_time": "2025-05-01T13:36:08.992764Z"
    }
   },
   "source": [
    "def generate_std_dev(dfrm, dt_target, duration):\n",
    "    dt_start = dt_target - timedelta(days=duration)\n",
    "    \"\"\"\n",
    "    For date in range (dt_target to end):\n",
    "        calculate some_stat(some_daily_values) for (current date - duration) through current date\n",
    "    Then reduce the size otherwise length of new column will not match\n",
    "    \"\"\"\n",
    "    stddevs = [dfrm.loc[date - timedelta(days=duration): date, 'close'].std() for date in\n",
    "               dfrm[datetime.date(dt_target):].index]\n",
    "    dfrm_reduced = dfrm[datetime.date(dt_target):]  # Reduce the size otherwise length of new column will not match\n",
    "    dfrm_reduced['stddev_' + str(duration)] = stddevs\n",
    "    \"\"\" \n",
    "    More adjustments required because of the need to return data \n",
    "    for '- timedelta(days=duration)' as well for downstream calculations.\n",
    "    Retrieve stats from smaller 'reduced' dfrm and populate the 'returned' dfrm. \n",
    "    \"\"\"\n",
    "    dfrm_returned = dfrm[datetime.date(dt_target) - timedelta(days=duration):]\n",
    "    dfrm_returned['stddev_' + str(duration)] = [np.nan if date not in dfrm_reduced.index\n",
    "                                                else dfrm_reduced.loc[date, 'stddev_' + str(duration)]\n",
    "                                                for date in dfrm_returned.index]\n",
    "    return dfrm_returned"
   ],
   "outputs": [],
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:09.006273Z",
     "start_time": "2025-05-01T13:36:09.002589Z"
    }
   },
   "source": [
    "def generate_pcntle_closing(dfrm, dt_target, duration):\n",
    "    dt_start = dt_target - timedelta(days=duration)\n",
    "    # Use scipp.stats.\n",
    "    # Give entire rolling range and calculate percentile of the last value in that range\n",
    "    pcntles = [\n",
    "        stats.percentileofscore(dfrm.loc[date - timedelta(days=duration): date, 'close'], dfrm.loc[date, 'close'])\n",
    "        for date in dfrm[datetime.date(dt_target):].index]\n",
    "    dfrm_reduced = dfrm[datetime.date(dt_target):]  # Reduce the size otherwise length of new column will not match\n",
    "    dfrm_reduced['pcntleClosing_' + str(duration)] = pcntles\n",
    "    dfrm_returned = dfrm[datetime.date(dt_target) - timedelta(days=duration):]\n",
    "    dfrm_returned['pcntleClosing_' + str(duration)] = [np.nan if date not in dfrm_reduced.index\n",
    "                                                       else dfrm_reduced.loc[date, 'pcntleClosing_' + str(duration)]\n",
    "                                                       for date in dfrm_returned.index]\n",
    "    return dfrm_returned"
   ],
   "outputs": [],
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:09.025247Z",
     "start_time": "2025-05-01T13:36:09.020793Z"
    }
   },
   "source": [
    "def generate_pcntle_volume(dfrm, dt_target, duration):\n",
    "    dt_start = dt_target - timedelta(days=duration)\n",
    "    # Use scipy.stats.\n",
    "    # Give entire rolling range and calculate percentile of the last value in that range\n",
    "    pcntles = [\n",
    "        stats.percentileofscore(dfrm.loc[date - timedelta(days=duration): date, 'volume'], dfrm.loc[date, 'volume'])\n",
    "        for date in dfrm[datetime.date(dt_target):].index]\n",
    "    dfrm_reduced = dfrm[datetime.date(dt_target):]  # Reduce the size otherwise length of new column will not match\n",
    "    dfrm_reduced['pcntleVolume_' + str(duration)] = pcntles\n",
    "    dfrm_returned = dfrm[datetime.date(dt_target) - timedelta(days=duration):]\n",
    "    dfrm_returned['pcntleVolume_' + str(duration)] = [np.nan if date not in dfrm_reduced.index\n",
    "                                                      else dfrm_reduced.loc[date, 'pcntleVolume_' + str(duration)]\n",
    "                                                      for date in dfrm_returned.index]\n",
    "    return dfrm_returned"
   ],
   "outputs": [],
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:09.044459Z",
     "start_time": "2025-05-01T13:36:09.040250Z"
    }
   },
   "source": [
    "def generate_pcntle_std_devs(dfrm, dt_target, duration):\n",
    "    dt_start = dt_target - timedelta(days=duration)\n",
    "    # Use scipp.stats.\n",
    "    # Give entire rolling range and calculate percentile of the last value in that range\n",
    "    pcntles = [stats.percentileofscore(dfrm.loc[date - timedelta(days=duration): date, 'stddev_' + str(duration)],\n",
    "                                       dfrm.loc[date, 'stddev_' + str(duration)])\n",
    "               for date in dfrm[datetime.date(dt_target):].index]\n",
    "\n",
    "    dfrm_reduced = dfrm[datetime.date(dt_target):]  # Reduce the size otherwise length of new column will not match\n",
    "    dfrm_reduced['pcntleStdDevs_' + str(duration)] = pcntles\n",
    "    dfrm_returned = dfrm[datetime.date(dt_target) - timedelta(days=duration):]\n",
    "    dfrm_returned['pcntleStdDevs_' + str(duration)] = [np.nan if date not in dfrm_reduced.index\n",
    "                                                       else dfrm_reduced.loc[date, 'pcntleStdDevs_' + str(duration)]\n",
    "                                                       for date in dfrm_returned.index]\n",
    "    return dfrm_returned"
   ],
   "outputs": [],
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:09.063324Z",
     "start_time": "2025-05-01T13:36:09.058404Z"
    }
   },
   "source": [
    "def generate_stcstc_oscillator(dfrm, dt_target, duration):\n",
    "    dt_start = dt_target - timedelta(days=duration)\n",
    "    \"\"\"\n",
    "    LOGIC: oscillator is:\n",
    "        np.nan if (highestHigh == lowestLow)\n",
    "        else ((dayClose - lowestLow) / (highestHigh - lowestLow)) * (100)\n",
    "        for date in dfrm.index     \n",
    "    \"\"\"\n",
    "    oscillators = [np.nan if\n",
    "                   dfrm.loc[date - timedelta(days=duration): date, 'close'].max() == dfrm.loc[date - timedelta(\n",
    "                       days=duration): date, 'close'].min()\n",
    "                   else (100) *\n",
    "                        (dfrm.loc[date, 'close'] - dfrm.loc[date - timedelta(days=duration): date, 'close'].min()) /\n",
    "                        (dfrm.loc[date - timedelta(days=duration): date, 'close'].max() - dfrm.loc[date - timedelta(\n",
    "                            days=duration): date, 'close'].min())\n",
    "                   for date in dfrm[datetime.date(dt_target):].index]\n",
    "    dfrm_reduced = dfrm[datetime.date(dt_target):]\n",
    "    dfrm_reduced['oscillator_' + str(duration)] = oscillators\n",
    "    dfrm_returned = dfrm[datetime.date(dt_target) - timedelta(days=duration):]\n",
    "    dfrm_returned['oscillator_' + str(duration)] = [np.nan if date not in dfrm_reduced.index\n",
    "                                                    else dfrm_reduced.loc[date, 'oscillator_' + str(duration)]\n",
    "                                                    for date in dfrm_returned.index]\n",
    "    return dfrm_returned"
   ],
   "outputs": [],
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:09.081897Z",
     "start_time": "2025-05-01T13:36:09.076814Z"
    }
   },
   "source": [
    "def generate_williams_r(dfrm, dt_target, duration):\n",
    "    dt_start = dt_target - timedelta(days=duration)\n",
    "    \"\"\"\n",
    "    LOGIC: oscillator is:\n",
    "        np.nan if (highestHigh == lowestLow)\n",
    "        else ((dayClose - lowestLow) / (highestHigh - lowestLow)) * (100)\n",
    "        for date in dfrm.index     \n",
    "    \"\"\"\n",
    "    williamsr = [np.nan if\n",
    "                 dfrm.loc[date - timedelta(days=duration): date, 'close'].max() == dfrm.loc[date - timedelta(\n",
    "                     days=duration): date, 'close'].min()\n",
    "                 else (-100) *\n",
    "                      (dfrm.loc[date - timedelta(days=duration): date, 'close'].max() - (dfrm.loc[date, 'close'])) /\n",
    "                      (dfrm.loc[date - timedelta(days=duration): date, 'close'].max() - dfrm.loc[date - timedelta(\n",
    "                          days=duration): date, 'close'].min())\n",
    "                 for date in dfrm[datetime.date(dt_target):].index]\n",
    "    dfrm_reduced = dfrm[datetime.date(dt_target):]\n",
    "    dfrm_reduced['williamsr_' + str(duration)] = williamsr\n",
    "    dfrm_returned = dfrm[datetime.date(dt_target) - timedelta(days=duration):]\n",
    "    dfrm_returned['williamsr_' + str(duration)] = [np.nan if date not in dfrm_reduced.index\n",
    "                                                   else dfrm_reduced.loc[date, 'williamsr_' + str(duration)]\n",
    "                                                   for date in dfrm_returned.index]\n",
    "    return dfrm_returned"
   ],
   "outputs": [],
   "execution_count": 120
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:09.101709Z",
     "start_time": "2025-05-01T13:36:09.096044Z"
    }
   },
   "source": [
    "def generate_accumulation_dist(dfrm, dt_target, duration):\n",
    "    dt_start = dt_target - timedelta(days=duration)\n",
    "    # https://www.tradingview.com/support/solutions/43000501770-accumulation-distribution-adl/\n",
    "    # Accumulation/Distribution = ((Close – Low) – (High – Close)) / (High – Low) * Period Volume\n",
    "    accu_dist = [np.nan if\n",
    "                 dfrm.loc[date - timedelta(days=duration): date, 'close'].max() == dfrm.loc[date - timedelta(\n",
    "                     days=duration): date, 'close'].min()\n",
    "                 else dfrm.loc[date - timedelta(days=duration): date, 'volume'].mean() *\n",
    "                      ((dfrm.loc[date, 'close'] - dfrm.loc[date - timedelta(days=duration): date, 'close'].min()) -\n",
    "                       (dfrm.loc[date - timedelta(days=duration): date, 'close'].max() - dfrm.loc[date, 'close']))\n",
    "                      /\n",
    "                      (dfrm.loc[date - timedelta(days=duration): date, 'close'].max() - dfrm.loc[date - timedelta(\n",
    "                          days=duration): date, 'close'].min())\n",
    "                 for date in dfrm[datetime.date(dt_target):].index]\n",
    "    dfrm_reduced = dfrm[datetime.date(dt_target):]\n",
    "    dfrm_reduced['accu_dist_' + str(duration)] = accu_dist\n",
    "    dfrm_returned = dfrm[datetime.date(dt_target) - timedelta(days=duration):]\n",
    "    dfrm_returned['accu_dist_' + str(duration)] = [np.nan if date not in dfrm_reduced.index\n",
    "                                                   else dfrm_reduced.loc[date, 'accu_dist_' + str(duration)]\n",
    "                                                   for date in dfrm_returned.index]\n",
    "    return dfrm_returned"
   ],
   "outputs": [],
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:09.121392Z",
     "start_time": "2025-05-01T13:36:09.115776Z"
    }
   },
   "source": [
    "def generate_bollinger_bands(dfrm, dt_target, duration):\n",
    "    dt_start = dt_target - timedelta(days=duration)\n",
    "    # https://www.tradingview.com/support/solutions/43000501770-accumulation-distribution-adl/\n",
    "    # Accumulation/Distribution = ((Close – Low) – (High – Close)) / (High – Low) * Period Volume\n",
    "    bbUpper = [dfrm.loc[date, 'mean_' + str(duration)] + (2 * dfrm.loc[date, 'stddev_' + str(duration)])\n",
    "               for date in dfrm[datetime.date(dt_target):].index]\n",
    "    bbLower = [dfrm.loc[date, 'mean_' + str(duration)] - (2 * dfrm.loc[date, 'stddev_' + str(duration)])\n",
    "               for date in dfrm[datetime.date(dt_target):].index]\n",
    "\n",
    "    dfrm_reduced = dfrm[datetime.date(dt_target):]\n",
    "    dfrm_reduced['bollingerUpper_' + str(duration)] = bbUpper\n",
    "    dfrm_reduced['bollingerLower_' + str(duration)] = bbLower\n",
    "    dfrm_returned = dfrm[datetime.date(dt_target) - timedelta(days=duration):]\n",
    "    dfrm_returned['bollingerUpper_' + str(duration)] = [np.nan if date not in dfrm_reduced.index\n",
    "                                                        else dfrm_reduced.loc[date, 'bollingerUpper_' + str(duration)]\n",
    "                                                        for date in dfrm_returned.index]\n",
    "    dfrm_returned['bollingerLower_' + str(duration)] = [np.nan if date not in dfrm_reduced.index\n",
    "                                                        else dfrm_reduced.loc[date, 'bollingerLower_' + str(duration)]\n",
    "                                                        for date in dfrm_returned.index]\n",
    "    return dfrm_returned"
   ],
   "outputs": [],
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:09.137496Z",
     "start_time": "2025-05-01T13:36:09.134913Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:09.179908Z",
     "start_time": "2025-05-01T13:36:09.171462Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "def generateRelativeStrengthIndex(dfrm, dt_target, duration):\n",
    "    # https://www.fidelity.com/learning-center/trading-investing/technical-analysis/technical-indicator-guide/RSI#:~:text=Description,and%20oversold%20when%20below%2030.\n",
    "    # https://www.investopedia.com/terms/r/rsi.asp\n",
    "    dt_start = dt_target - timedelta(days=duration)\n",
    "    # RSI = 100 – [100 / ( 1 + (Average of Upward Price Change / Average of Downward Price Change ) ) ]\n",
    "    gains = [ closing_value \n",
    "             for date in dfrm[datetime.date(dt_target):].index ]\n",
    "        \n",
    "    losses = [\n",
    "             for date in dfrm[datetime.date(dt_target):].index ]\n",
    "    \n",
    "    dfrm_reduced = dfrm[datetime.date(dt_target):]\n",
    "    dfrm_reduced['forceIndex_'+str(duration)] = force_index\n",
    "    \n",
    "    dfrm_returned = dfrm[datetime.date(dt_target) - timedelta(days=duration):]\n",
    "    dfrm_returned['forceIndex_'+str(duration)] = [ np.nan if date not in dfrm_reduced.index\n",
    "                                             else dfrm_reduced.loc[date, 'forceIndex_'+str(duration)] \n",
    "                                            for date in dfrm_returned.index ]\n",
    "    return dfrm_returned\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef generateRelativeStrengthIndex(dfrm, dt_target, duration):\\n    # https://www.fidelity.com/learning-center/trading-investing/technical-analysis/technical-indicator-guide/RSI#:~:text=Description,and%20oversold%20when%20below%2030.\\n    # https://www.investopedia.com/terms/r/rsi.asp\\n    dt_start = dt_target - timedelta(days=duration)\\n    # RSI = 100 – [100 / ( 1 + (Average of Upward Price Change / Average of Downward Price Change ) ) ]\\n    gains = [ closing_value \\n             for date in dfrm[datetime.date(dt_target):].index ]\\n        \\n    losses = [\\n             for date in dfrm[datetime.date(dt_target):].index ]\\n    \\n    dfrm_reduced = dfrm[datetime.date(dt_target):]\\n    dfrm_reduced['forceIndex_'+str(duration)] = force_index\\n    \\n    dfrm_returned = dfrm[datetime.date(dt_target) - timedelta(days=duration):]\\n    dfrm_returned['forceIndex_'+str(duration)] = [ np.nan if date not in dfrm_reduced.index\\n                                             else dfrm_reduced.loc[date, 'forceIndex_'+str(duration)] \\n                                            for date in dfrm_returned.index ]\\n    return dfrm_returned\\n\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:09.212195Z",
     "start_time": "2025-05-01T13:36:09.208736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filePath, _ = generate_file_path(\"LLY\")\n",
    "filePath"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../../workspace/HelloPython/HistoricalMarketData/TechnicalIndicators/lly.csv'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 124
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:09.250710Z",
     "start_time": "2025-05-01T13:36:09.242563Z"
    }
   },
   "source": [
    "def generate_ema(dfrm, dt_target, duration):\n",
    "    # TODO: Requires some adjustment. Right now EMA is not moving closely with daily 'close' values\n",
    "    dt_start = dt_target - timedelta(days=duration)\n",
    "    # Handle Exponential Moving Averages (EMA)\n",
    "    emaMultipler = round((2 / (duration + 1)), 2)\n",
    "    emas = list()\n",
    "    # Need to pre-populate the first value for ema\n",
    "    # Use mean for that day as the seed for first value for EMA\n",
    "    \"\"\"\n",
    "    print(dfrm)\n",
    "    print(duration)\n",
    "    print(dt_target)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        seed = dfrm.loc[datetime.date(dt_target):datetime.date(dt_target), 'mean_' + str(duration)][0]\n",
    "    except IndexError as err:\n",
    "        print('IndexError: Validate that data exists for start date: {}'.format(dt_target))\n",
    "        return\n",
    "\n",
    "    emas.append(seed)  # Need first value prepopulated\n",
    "    # ema = (sliceClosings.loc[i]-seriesEMA[i-1]) * emaMultipler + seriesEMA[i-1]\n",
    "    # EMA=Price(t)×k+EMA(y)×(1−k)\n",
    "\n",
    "    emas_remainder_list = [(dfrm.loc[date, 'close'] * emaMultipler) + (emas[-1] * (1 - emaMultipler))\n",
    "                           for date in dfrm[datetime.date(dt_target + timedelta(days=1)):].index]\n",
    "\n",
    "    #emas_remainder_list = [ (dfrm.loc[date, 'close'] - emas[-1]) * emaMultipler + emas[-1]\n",
    "    #for date in dfrm[datetime.date(dt_target+timedelta(days=1)):].index ]\n",
    "    emas.extend(emas_remainder_list)\n",
    "    emas = [round(ema, 2) for ema in emas]\n",
    "    # emas = dfrm.ewm(span=duration).mean()\n",
    "    # print(emas)\n",
    "    dfrm_reduced = dfrm[datetime.date(dt_target):]  # Reduce the size otherwise length of new column will not match\n",
    "    # print(len(emas))\n",
    "    # print(len(dfrm_reduced))\n",
    "    dfrm_reduced['ema_' + str(duration)] = emas\n",
    "    dfrm_returned = dfrm[datetime.date(dt_target) - timedelta(days=duration):]\n",
    "    dfrm_returned['ema_' + str(duration)] = [np.nan if date not in dfrm_reduced.index\n",
    "                                             else dfrm_reduced.loc[date, 'ema_' + str(duration)]\n",
    "                                             for date in dfrm_returned.index]\n",
    "    return dfrm_returned"
   ],
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:09.278646Z",
     "start_time": "2025-05-01T13:36:09.270632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_force_index(dfrm, dt_target, duration):\n",
    "    # https://www.investopedia.com/terms/f/force-index.asp\n",
    "    # Used SMA rather than EMA since EMA algo is not working for me properly\n",
    "    \"\"\"\n",
    "    This is somewhat complicated. First calculate (todayClose - lastClose) * volume from dt_target through end.\n",
    "    Now, we need to calculate mean of the first calculation. But we need to wait until we have 'duration' number\n",
    "    of calculations before we start calculating means. We therefore, have to mark initial force index with 0 achieved\n",
    "    here through 'end_offset'. Once we have 'duration' number of \"(todayClose - lastClose) * volume\" then\n",
    "    we capture the proper force index.\n",
    "    Also remember that first few rows before dt_start index will have zero values for indicators. All calculations\n",
    "    mentioned above are after that point on.\n",
    "    \"\"\"\n",
    "    dt_start = dt_target - timedelta(days=duration)\n",
    "    force_dly_index = list()\n",
    "    force_dly_index = [\n",
    "        (dfrm.iloc[index, dfrm.columns.get_loc(\"close\")] - dfrm.iloc[index - 1, dfrm.columns.get_loc(\"close\")])\n",
    "        * dfrm.iloc[index, dfrm.columns.get_loc(\"volume\")]\n",
    "        for index in range(dfrm.index.get_loc(datetime.date(dt_target)), len(dfrm))]\n",
    "    #force_dly_index.extend(force_dly_index_rem_list)\n",
    "    dfrm_reduced = dfrm[datetime.date(dt_target):]\n",
    "    dfrm_reduced['forceDailyIndex_' + str(duration)] = force_dly_index\n",
    "    force_means_index = list()\n",
    "\n",
    "    indices = dfrm_reduced.index\n",
    "    end_offset = indices.get_loc(datetime.date(dt_target + timedelta(days=duration)))\n",
    "    print(\"End offset value is: {}\".format(end_offset))\n",
    "    force_means_index[:end_offset] = [0] * end_offset\n",
    "    force_means_index_rem_list = [\n",
    "        dfrm_reduced.loc[date - timedelta(days=duration): date, 'forceDailyIndex_' + str(duration)].mean()\n",
    "        for date in dfrm_reduced[datetime.date(dt_target + timedelta(days=duration)):].index]\n",
    "    force_means_index.extend(force_means_index_rem_list)\n",
    "    dfrm_reduced['forceIndex_' + str(duration)] = force_means_index\n",
    "    dfrm_returned = dfrm[datetime.date(dt_target) - timedelta(days=duration):]\n",
    "    dfrm_returned['forceIndex_' + str(duration)] = [np.nan if date not in dfrm_reduced.index\n",
    "                                                    else dfrm_reduced.loc[date, 'forceIndex_' + str(duration)]\n",
    "                                                    for date in dfrm_returned.index]\n",
    "    return dfrm_returned"
   ],
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:09.297938Z",
     "start_time": "2025-05-01T13:36:09.293380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_last_date_for_moving_avgs_by_symbol(symbol):\n",
    "    str_baseline_date = \"1999-12-31\"\n",
    "    dt_baseline = datetime.strptime(str_baseline_date, '%Y-%m-%d')\n",
    "\n",
    "    filePath, _ = generate_file_path(symbol)\n",
    "    print(filePath)\n",
    "    if (filePath is not None):\n",
    "        try:\n",
    "            dfrm = pd.read_csv(filePath)\n",
    "            # date_indices_formatted = [ datetime.strftime(datetime.strptime(index, '%m/%d/%Y'), '%Y-%m-%d')\n",
    "            #         if validate_date_format(str(index)) else index for index in dfrm.index ]\n",
    "            # dfrm.index = date_indices_formatted\n",
    "            dfrm['date'] = pd.to_datetime(dfrm['date'])\n",
    "            dfrm.set_index('date', inplace=True)\n",
    "            dfrm.sort_index(ascending=True)\n",
    "            dfrm.index.name = 'date'\n",
    "            # dfrm.set_index('date', inplace=True)\n",
    "            # dfrm.sort_index(inplace=True, ascending = False)\n",
    "            return dfrm.index[-1]\n",
    "        except FileNotFoundError as e:\n",
    "            print('Exception reading input data for symbol {}. Generating metadata starting from baseline date.'.format(symbol.upper()))\n",
    "            return dt_baseline\n",
    "        except EmptyDataError as e:\n",
    "            print(f'No technical indicators found for {symbol.upper()}. Generating metadata starting from baseline date.')\n",
    "            print('Generating all records.')\n",
    "            return dt_baseline\n",
    "    else:\n",
    "        return dt_baseline\n"
   ],
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:25.301407Z",
     "start_time": "2025-05-01T13:36:09.319622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    QUERY = True  # Keep it disabled and manage the list manually. Enabling this will retrieve data for all of SnP500\n",
    "    EXPORT = True\n",
    "    EXPERIMENTAL = False\n",
    "\n",
    "    macd_durations = [14, 30]\n",
    "    if QUERY:  # The index is datetime.date and not string or datetime.datetime object.\n",
    "        # query = 'select distinct symbol from industrybackground where SnP500 like 1'\n",
    "        query = 'select distinct symbol from industrybackground where SnP500 like 1 and symbol not \\\n",
    "        in (select distinct symbol from equities_historic_data where date like \"2019-12-20\")'\n",
    "\n",
    "        with ENGINE.connect() as conn:\n",
    "            res = conn.execute(text(query))\n",
    "        dfrm = pd.DataFrame(res.mappings().all())\n",
    "        symbols = dfrm['symbol'].tolist()\n",
    "    else:\n",
    "        symbols = ['BAC', 'JPM', 'C', 'MS', 'GS', 'WFC', 'FB', 'MSFT', 'GOOGL', 'NFLX', 'AAPL', 'AMZN', 'TSLA', 'MRK',\n",
    "                   'PFE', 'NKE', 'INTC', 'NVDA', 'ADM', 'TSM', 'MU', 'QCOM']\n",
    "        s2 = ['RE', 'ACGL', 'AXS', 'CB', 'THG', 'PGR', 'RNR', 'SIGI', 'TRV', 'WRB']\n",
    "\n",
    "        symbols.extend(s2)\n",
    "        # symbols = [\"USB\", \"TFC\", \"PNC\", \"BK\", \"STT\", \"AMTD\"]\n",
    "        # symbols = ('TSLA', 'MSFT')\n",
    "        # symbols = ['BAC', 'JPM', 'C', 'MS', 'GS', 'WFC']\n",
    "        # symbols = [\"FSLR\", \"VRT\", \"COIN\", \"MRVL\", \"CRWD\", \"AVGO\", \"DDOG\", \"SMCI\", \"GOOGL\", \"AMZN\", \"SHAK\", \"APO\", \"DJT\",\n",
    "        #            \"FCX\", \"LLY\", \"META\"]\n",
    "        # symbols = [\"LLY\", \"META\"]\n",
    "        # symbols = [\"VRT\", \"NVDA\", \"MRVL\", \"SMCI\", \"SHAK\"]\n",
    "        # symbols = [\"SNOW\", \"PFE\", \"MRK\", \"JNJ\", \"REGN\", \"NVO\", \"AAPL\"]\n",
    "        symbols = [\"PFE\", \"MRK\", \"JNJ\", \"REGN\", \"NVO\", \"AAPL\", \"LLY\", \"AVGO\", \"FCX\", \"FSLR\", \"NKE\"]\n",
    "\n",
    "    for symbol in symbols:\n",
    "        print('Generating data for technical indicators for symbol: {}'.format(symbol))\n",
    "        dt_returned = get_last_date_for_moving_avgs_by_symbol(symbol)\n",
    "\n",
    "        # dt_target = datetime.strptime(strDateReturned, '%Y-%m-%d')\n",
    "        dt_target = dt_returned + timedelta(days=1)  # Start with next day\n",
    "\n",
    "        print('Generating data since {}'.format(dt_target))\n",
    "        list_dfrm = list()\n",
    "        dfrm = retrieve_cataloged_market_data(symbol, TABLE_EQUITIES_DATA, '2000-01-01')\n",
    "        #indices = [datetime.strptime(dt, '%Y-%m-%d').date() for dt in dfrm.index]\n",
    "        #dfrm.set_index(indices, inplace=True)\n",
    "        base_dfrm = dfrm.loc[:, ('symbol', 'close', 'volume')]\n",
    "        dfrm_final = pd.DataFrame()\n",
    "        for duration in DURATIONS:\n",
    "            dfrm_running = generate_mean(base_dfrm, dt_target, duration)\n",
    "            dfrm_running = generate_std_dev(dfrm_running, dt_target, duration)\n",
    "            #dfrm_running = generate_ema(dfrm_running, dt_target, duration)\n",
    "            dfrm_running = generate_pcntle_std_devs(dfrm_running, dt_target, duration)\n",
    "            dfrm_running = generate_pcntle_volume(dfrm_running, dt_target, duration)\n",
    "            dfrm_running = generate_pcntle_closing(dfrm_running, dt_target, duration)\n",
    "            dfrm_running = generate_stcstc_oscillator(dfrm_running, dt_target, duration)\n",
    "            # dfrm_running = generate_force_index(dfrm_running, dt_target, duration)\n",
    "            # dfrm_running = generate_williams_r(dfrm_running, dt_target, duration) # Very similar to Stcstc Oscillator. Keep one\n",
    "            dfrm_running = generate_accumulation_dist(dfrm_running, dt_target, duration)\n",
    "            dfrm_running = generate_bollinger_bands(dfrm_running, dt_target, duration)\n",
    "\n",
    "            # Keep rows with non-NaN values\n",
    "            dfrm_running = dfrm_running[dfrm_running['mean_' + str(duration)].notna()]\n",
    "            # dfrm_running\n",
    "            if len(dfrm_final) == 0:\n",
    "                dfrm_final = dfrm_running.copy()\n",
    "            else:\n",
    "                columns = dfrm_running.columns.difference(dfrm_final.columns)\n",
    "                dfrm_final = pd.merge(dfrm_final, dfrm_running[columns], left_index=True, right_index=True)\n",
    "                # if macd_durations in DURATIONS:\n",
    "    if EXPORT:\n",
    "            if dfrm_final is not None and len(dfrm_final) > 0:\n",
    "                filePath, _ = generate_file_path(symbol)\n",
    "                print('Saving data to {}'.format(filePath))\n",
    "                dfrm = pd.DataFrame()\n",
    "                if os.path.exists(filePath):  # Append data to existing CSV\n",
    "                    dfrm_existing = pd.read_csv(filePath)\n",
    "                    print(dfrm_existing.tail(10))\n",
    "                    if dfrm_existing is not None and len(dfrm_existing) > 0:\n",
    "                        print(len(dfrm_existing))\n",
    "                        dfrm_existing.set_index('date', inplace=True)\n",
    "                        frames = [dfrm_existing, dfrm_final]\n",
    "                        dfrm = pd.concat(frames)\n",
    "                    print(len(dfrm_final))\n",
    "                    print(len(dfrm))\n",
    "                    dfrm.to_csv(filePath, sep=',')\n",
    "                    print(f'Appended data to {filePath}')\n",
    "                else:  # Directly write final dfrm to CSV file\n",
    "                    dfrm_final.to_csv(filePath, sep=',')\n",
    "                    print(f'Added new file at {filePath}')\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data for technical indicators for symbol: A\n",
      "../../../../workspace/HelloPython/HistoricalMarketData/TechnicalIndicators/a.csv\n",
      "Exception reading input data for symbol A. Generating metadata starting from baseline date.\n",
      "Generating data since 2000-01-01 00:00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[128], line 51\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m#dfrm_running = generate_ema(dfrm_running, dt_target, duration)\u001B[39;00m\n\u001B[1;32m     50\u001B[0m dfrm_running \u001B[38;5;241m=\u001B[39m generate_pcntle_std_devs(dfrm_running, dt_target, duration)\n\u001B[0;32m---> 51\u001B[0m dfrm_running \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_pcntle_volume\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdfrm_running\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdt_target\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mduration\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m dfrm_running \u001B[38;5;241m=\u001B[39m generate_pcntle_closing(dfrm_running, dt_target, duration)\n\u001B[1;32m     53\u001B[0m dfrm_running \u001B[38;5;241m=\u001B[39m generate_stcstc_oscillator(dfrm_running, dt_target, duration)\n",
      "Cell \u001B[0;32mIn[117], line 5\u001B[0m, in \u001B[0;36mgenerate_pcntle_volume\u001B[0;34m(dfrm, dt_target, duration)\u001B[0m\n\u001B[1;32m      2\u001B[0m dt_start \u001B[38;5;241m=\u001B[39m dt_target \u001B[38;5;241m-\u001B[39m timedelta(days\u001B[38;5;241m=\u001B[39mduration)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Use scipy.stats.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Give entire rolling range and calculate percentile of the last value in that range\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m pcntles \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      6\u001B[0m     stats\u001B[38;5;241m.\u001B[39mpercentileofscore(dfrm\u001B[38;5;241m.\u001B[39mloc[date \u001B[38;5;241m-\u001B[39m timedelta(days\u001B[38;5;241m=\u001B[39mduration): date, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvolume\u001B[39m\u001B[38;5;124m'\u001B[39m], dfrm\u001B[38;5;241m.\u001B[39mloc[date, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvolume\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m date \u001B[38;5;129;01min\u001B[39;00m dfrm[datetime\u001B[38;5;241m.\u001B[39mdate(dt_target):]\u001B[38;5;241m.\u001B[39mindex]\n\u001B[1;32m      8\u001B[0m dfrm_reduced \u001B[38;5;241m=\u001B[39m dfrm[datetime\u001B[38;5;241m.\u001B[39mdate(dt_target):]  \u001B[38;5;66;03m# Reduce the size otherwise length of new column will not match\u001B[39;00m\n\u001B[1;32m      9\u001B[0m dfrm_reduced[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpcntleVolume_\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(duration)] \u001B[38;5;241m=\u001B[39m pcntles\n",
      "Cell \u001B[0;32mIn[117], line 6\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      2\u001B[0m dt_start \u001B[38;5;241m=\u001B[39m dt_target \u001B[38;5;241m-\u001B[39m timedelta(days\u001B[38;5;241m=\u001B[39mduration)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Use scipy.stats.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Give entire rolling range and calculate percentile of the last value in that range\u001B[39;00m\n\u001B[1;32m      5\u001B[0m pcntles \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m----> 6\u001B[0m     stats\u001B[38;5;241m.\u001B[39mpercentileofscore(dfrm\u001B[38;5;241m.\u001B[39mloc[date \u001B[38;5;241m-\u001B[39m timedelta(days\u001B[38;5;241m=\u001B[39mduration): date, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvolume\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[43mdfrm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvolume\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m date \u001B[38;5;129;01min\u001B[39;00m dfrm[datetime\u001B[38;5;241m.\u001B[39mdate(dt_target):]\u001B[38;5;241m.\u001B[39mindex]\n\u001B[1;32m      8\u001B[0m dfrm_reduced \u001B[38;5;241m=\u001B[39m dfrm[datetime\u001B[38;5;241m.\u001B[39mdate(dt_target):]  \u001B[38;5;66;03m# Reduce the size otherwise length of new column will not match\u001B[39;00m\n\u001B[1;32m      9\u001B[0m dfrm_reduced[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpcntleVolume_\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(duration)] \u001B[38;5;241m=\u001B[39m pcntles\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llm_venv/lib/python3.9/site-packages/pandas/core/indexing.py:1183\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1181\u001B[0m     key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(com\u001B[38;5;241m.\u001B[39mapply_if_callable(x, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m key)\n\u001B[1;32m   1182\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_scalar_access(key):\n\u001B[0;32m-> 1183\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtakeable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_takeable\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1184\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_tuple(key)\n\u001B[1;32m   1185\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1186\u001B[0m     \u001B[38;5;66;03m# we by definition only have the 0th axis\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llm_venv/lib/python3.9/site-packages/pandas/core/frame.py:4214\u001B[0m, in \u001B[0;36mDataFrame._get_value\u001B[0;34m(self, index, col, takeable)\u001B[0m\n\u001B[1;32m   4211\u001B[0m     series \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ixs(col, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m   4212\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m series\u001B[38;5;241m.\u001B[39m_values[index]\n\u001B[0;32m-> 4214\u001B[0m series \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_item_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4215\u001B[0m engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39m_engine\n\u001B[1;32m   4217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex, MultiIndex):\n\u001B[1;32m   4218\u001B[0m     \u001B[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001B[39;00m\n\u001B[1;32m   4219\u001B[0m     \u001B[38;5;66;03m#  results if our categories are integers that dont match our codes\u001B[39;00m\n\u001B[1;32m   4220\u001B[0m     \u001B[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llm_venv/lib/python3.9/site-packages/pandas/core/frame.py:4628\u001B[0m, in \u001B[0;36mDataFrame._get_item_cache\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   4626\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_get_item_cache\u001B[39m(\u001B[38;5;28mself\u001B[39m, item: Hashable) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Series:\n\u001B[1;32m   4627\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 4628\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m using_copy_on_write() \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mwarn_copy_on_write\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   4629\u001B[0m         loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(item)\n\u001B[1;32m   4630\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ixs(loc, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llm_venv/lib/python3.9/site-packages/pandas/_config/__init__.py:45\u001B[0m, in \u001B[0;36mwarn_copy_on_write\u001B[0;34m()\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mwarn_copy_on_write\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m     43\u001B[0m     _mode_options \u001B[38;5;241m=\u001B[39m _global_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m---> 45\u001B[0m         \u001B[43m_mode_options\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcopy_on_write\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarn\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m _mode_options[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblock\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     47\u001B[0m     )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:25.303977Z",
     "start_time": "2025-05-01T13:34:48.520863Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T13:36:25.305734Z",
     "start_time": "2025-05-01T13:34:48.565761Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
