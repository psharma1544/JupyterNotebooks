{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.673922Z",
     "start_time": "2025-04-30T21:57:33.285133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "! pip install finnhub-python\n",
    "! pip install sqlalchemy"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: finnhub-python in /opt/anaconda3/lib/python3.12/site-packages (2.4.23)\r\n",
      "Requirement already satisfied: requests>=2.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from finnhub-python) (2.32.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.22.0->finnhub-python) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.22.0->finnhub-python) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.22.0->finnhub-python) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.22.0->finnhub-python) (2024.8.30)\r\n",
      "Requirement already satisfied: sqlalchemy in /opt/anaconda3/lib/python3.12/site-packages (2.0.34)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from sqlalchemy) (4.11.0)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.807470Z",
     "start_time": "2025-04-30T21:57:34.680179Z"
    }
   },
   "source": [
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "from datetime import datetime, timedelta, time\n",
    "from os import path, listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import finnhub\n",
    "import pandas as pd\n",
    "from sqlalchemy import MetaData, create_engine, Table, text\n",
    "from sqlalchemy.exc import IntegrityError, SQLAlchemyError"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'finnhub'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mos\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m path, listdir\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mos\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpath\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m isfile, join\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mfinnhub\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msqlalchemy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m MetaData, create_engine, Table, text\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'finnhub'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.809392Z",
     "start_time": "2025-04-30T15:46:38.920583Z"
    }
   },
   "source": "BASE_DIR = '/Users/peeyushsharma/Dropbox/workspace/HelloPython/HistoricalMarketData/ByDateRange'",
   "outputs": [],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.823870Z",
     "start_time": "2025-04-30T15:46:38.927454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(os.environ.get('SHELL')) # Check shell to set the right profile for keys\n",
    "api_key = os.environ.get('FINNHUB_API_KEY')\n",
    "if api_key is None:\n",
    "    raise ValueError(\"Finn Hub API key not found. Please set the FINNHUB_API_KEY environment variable.\")\n",
    "finnhub_client = finnhub.Client(api_key=api_key)"
   ],
   "outputs": [],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.837988Z",
     "start_time": "2025-04-30T15:46:38.936963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    DB = os.environ[\"DB\"]\n",
    "    DB_USER = os.environ[\"DB_USER\"]\n",
    "    DB_PWD = os.environ[\"DB_PWD\"]\n",
    "except KeyError:\n",
    "    raise Exception(\"Required environment variables DB_USER and DB_PWD not set\")\n",
    "DB_URL = 'mysql+mysqlconnector://' + DB_USER + ':' + DB_PWD + '@localhost/'+DB\n",
    "ENGINE = create_engine(DB_URL)"
   ],
   "outputs": [],
   "execution_count": 190
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.838604Z",
     "start_time": "2025-04-30T15:46:38.945831Z"
    }
   },
   "source": [
    "def generate_file_path(symbol, start, end):\n",
    "    start_date_str = datetime.strftime(start, '%Y%m%d')\n",
    "    end_date_str = datetime.strftime(end, '%Y%m%d')\n",
    "    file_name = symbol.lower() + '_' + start_date_str + '_' + end_date_str + '.csv'\n",
    "    file_path = os.path.join(BASE_DIR, file_name)\n",
    "    if file_path is None:\n",
    "        print('Could not find file for symbol:{}'.format(symbol))\n",
    "    # print(file_path)\n",
    "    return file_path"
   ],
   "outputs": [],
   "execution_count": 191
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.845432Z",
     "start_time": "2025-04-30T15:46:38.958855Z"
    }
   },
   "source": [
    "def generate_map_for_insert(dfrm, symbol):\n",
    "    data = list()\n",
    "    dfrm.sort_index(ascending=True)\n",
    "    dates = dfrm.index\n",
    "    clm_names = dfrm.columns\n",
    "\n",
    "    for date in dates:\n",
    "        row = dict()  # Dict for row values\n",
    "        for column in clm_names:\n",
    "            \"\"\"\n",
    "            During testing found that sometimes there can be two lines for the same\n",
    "            date. Will get TypeError for those days because series can't converted into\n",
    "            floats\n",
    "            \"\"\"\n",
    "            # print(column)\n",
    "            # print(date)\n",
    "            # print(float(dfrm.loc[date, column]))\n",
    "\n",
    "            row[column] = float(dfrm.loc[date, column])\n",
    "        row['symbol'] = symbol.upper()  # Needs symbol as pk\n",
    "        row['date'] = date\n",
    "        #row['date'] = datetime.strftime(datetime.strptime(date, '%m/%d/%y'), '%Y-%m-%d')  # Needs date as pk\n",
    "        data.append(row)  # Append row to list\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": 192
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.845978Z",
     "start_time": "2025-04-30T15:46:38.964333Z"
    }
   },
   "source": [
    "def insert_symbol_data(symbol, file_path):\n",
    "    data_inserted = False\n",
    "    metadata = MetaData()\n",
    "    # Reflect the table\n",
    "    table_historic_data = Table(\"equities_historic_data\", metadata, autoload_with=ENGINE)\n",
    "\n",
    "    try:\n",
    "        print(f'Retrieving historic data from file path: {file_path}')\n",
    "        dfrm_new_read_from_csv = pd.read_csv(file_path)\n",
    "        dfrm_new_read_from_csv['date'] = pd.to_datetime(dfrm_new_read_from_csv['date'])\n",
    "        dfrm_new_read_from_csv.set_index('date', inplace=True)\n",
    "        # date_indices_formatted = [datetime.strftime(datetime.strptime(index, '%m/%d/%Y'), '%Y-%m-%d')\n",
    "        #                           if validate_date_format(str(index)) else index for index in dfrm_new_read_from_csv.index]\n",
    "        # dfrm_new_read_from_csv.index = date_indices_formatted\n",
    "        dfrm_new_read_from_csv.sort_index(ascending=True)\n",
    "\n",
    "        data = generate_map_for_insert(dfrm_new_read_from_csv, symbol)\n",
    "        if (data is not None) and (len(data) > 0):\n",
    "            try:\n",
    "                with ENGINE.connect() as conn:\n",
    "                    conn.execute(table_historic_data.insert(), data)\n",
    "                    conn.commit()\n",
    "                    print('Inserted data for symbol: {}'.format(symbol.upper()))\n",
    "                    data_inserted = True\n",
    "            except SQLAlchemyError as e:\n",
    "                print(f\"Error inserting values: {e}\")\n",
    "        else:\n",
    "            print('No new datasets for symbol: {}'.format(symbol))\n",
    "\n",
    "    except pd.errors.EmptyDataError as err:\n",
    "        print(f'Empty records for symbol: {symbol.upper()}. Skipping insert')\n",
    "    except FileNotFoundError as err:\n",
    "        print('FileNotFoundError:{}'.format(err))\n",
    "    except IntegrityError as err:\n",
    "        print('Caught IntegrityError. A row with primary key already exists for symbol: {}'.format(symbol))\n",
    "        print('{}'.format(err))\n",
    "    except IndexError as err:\n",
    "        print('Caught IndexError (likely no records for insert!) while processing symbol: {}'.format(symbol))\n",
    "        print('{}'.format(err))\n",
    "    except UnicodeDecodeError as err:\n",
    "        print('Caught UnicodeDecodeError while processing file: {}'.format(file_path))\n",
    "        print('{}'.format(err))\n",
    "\n",
    "    return data_inserted"
   ],
   "outputs": [],
   "execution_count": 193
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.846119Z",
     "start_time": "2025-04-30T15:46:38.975037Z"
    }
   },
   "source": [
    "def insert_daily_mkt_data():\n",
    "    file_paths = [file for file in listdir(BASE_DIR) if isfile(join(BASE_DIR, file))]\n",
    "    for file_path in file_paths:\n",
    "        symbol = file_path.split('_')[0]\n",
    "        if symbol is not None and symbol != '':\n",
    "            data_inserted = insert_symbol_data(symbol, os.path.join(BASE_DIR, file_path))\n",
    "            if data_inserted:  # Move files to archive folder if DB insert succeeded\n",
    "                shutil.move(str(os.path.join(BASE_DIR, file_path)), os.path.join(BASE_DIR, 'Archive'))\n",
    "                data_inserted = False"
   ],
   "outputs": [],
   "execution_count": 194
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.846244Z",
     "start_time": "2025-04-30T15:46:38.983160Z"
    }
   },
   "source": [
    "def retrieve_symbol_data_finnhub(symbol, dt_start, dt_end):\n",
    "    file_path = generate_file_path(symbol, dt_start, dt_end)\n",
    "    if (file_path is not None) and (not path.exists(file_path)):\n",
    "        try:\n",
    "            print(\n",
    "                f'Retrieving data for symbol: {symbol} for period {datetime.strftime(dt_start, \"%Y-%m-%d\")} through {datetime.strftime(dt_end, \"%Y-%m-%d\")}')\n",
    "            res = finnhub_client.stock_candles(symbol, 'D', int(dt_start.timestamp()), int(dt_end.timestamp()))\n",
    "            dfrm_candles = pd.DataFrame.from_dict(res)\n",
    "            if dfrm_candles is not None:\n",
    "                dfrm_candles['date'] = [(datetime.strftime(datetime.fromtimestamp(ts) + timedelta(days=1), '%m/%d/%Y'))\n",
    "                                        for ts in dfrm_candles['t']]\n",
    "                dfrm_candles.set_index('t', inplace=True, drop=True)\n",
    "                dfrm_candles.sort_index(inplace=True, ascending=True)\n",
    "                dfrm_candles.set_index('date', inplace=True)\n",
    "                dfrm_candles.drop('s', inplace=True, axis=1)\n",
    "                dfrm_candles.rename(columns={'o': 'open',\n",
    "                                             'h': 'high',\n",
    "                                             'l': 'low',\n",
    "                                             'c': 'close',\n",
    "                                             'v': 'volume'},\n",
    "                                    inplace=True\n",
    "                                    )\n",
    "                dfrm_candles.to_csv(file_path, sep=',')\n",
    "                print('Generated records for symbol {} in file: {}'.format(symbol, file_path))\n",
    "            else:\n",
    "                print('No new records received for symbol:{}'.format(symbol))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('No records received for symbol:{}'.format(symbol))\n",
    "    else:\n",
    "        print('Records already retrieved for symbol:{} under path:{}'.format(symbol, file_path))\n",
    "    return file_path"
   ],
   "outputs": [],
   "execution_count": 195
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.848457Z",
     "start_time": "2025-04-30T15:46:38.993410Z"
    }
   },
   "source": [
    "def retrieve_sql_date_for_last_entry(symbol):\n",
    "    dt_last_entry = None\n",
    "    with ENGINE.connect() as conn:\n",
    "        res = conn.execute(text('select * from equities_historic_data where \\\n",
    "                    symbol like \\'' + symbol + '\\' order by date desc limit 0,1'))\n",
    "    dfrm_existing = pd.DataFrame(res.mappings().all())\n",
    "    if dfrm_existing is not None and len(dfrm_existing) > 0:  # Don't do anything if nothing exists for symbol\n",
    "        dfrm_existing.set_index('date', inplace=True)\n",
    "        dt_last_entry = dfrm_existing.index[0]\n",
    "        #strDateLatest = datetime.strftime(dateLastEntry, '%Y-%m-%d') # Increase date by 1 day\n",
    "    else:\n",
    "        print(f\"No records in DB for '{symbol}'. Downloading data starting from {dt_last_entry}.\")\n",
    "    return dt_last_entry"
   ],
   "outputs": [],
   "execution_count": 196
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.848887Z",
     "start_time": "2025-04-30T15:46:39.009751Z"
    }
   },
   "source": [
    "def retrieve_daily_mkt_data(symbol, dt_start=None, dt_end=None):\n",
    "    file_paths = list()\n",
    "    yesterday = (datetime.today() - timedelta(days=1)).date()\n",
    "    baseline_date = datetime(2020, 1, 1).date()  # For new symbol, use some baseline date beyond which to get all data\n",
    "\n",
    "    dt_last_catalog_entry = retrieve_sql_date_for_last_entry(symbol)\n",
    "    print('{}: Date for last cataloged entry - {}'.format(symbol.upper(), dt_last_catalog_entry))\n",
    "    if dt_last_catalog_entry is None:\n",
    "        print(f\"Using baseline date of {baseline_date} to retrieve data for {symbol.upper()}\")\n",
    "        dt_last_catalog_entry = baseline_date\n",
    "    elif dt_last_catalog_entry >= yesterday:\n",
    "        print(f\"Data already in catalog through {yesterday} for {symbol.upper()}. Skipping\")\n",
    "        return None\n",
    "\n",
    "    # dt_last_catalog_entry = datetime.combine(dt_last_catalog_entry, datetime.min.time())\n",
    "    # dt_start = dt_last_catalog_entry + timedelta(days =1) # We already have data for 'dt_last_catalog_entry'\n",
    "\n",
    "    # Set start date to last cataloged entry (fallback to baseline). This ensures no gaps in data\n",
    "    dt_start = dt_last_catalog_entry\n",
    "    print(f'Start date for retrieval set to: {dt_start}')\n",
    "\n",
    "    if dt_end is None:\n",
    "        dt_end = yesterday\n",
    "\n",
    "    if dt_end > dt_start:  # Last date must be later than start date\n",
    "        file_path = retrieve_symbol_data_finnhub(symbol, datetime.combine(dt_start, time.min), datetime.combine(dt_end, time.min))\n",
    "        if file_path is not None:\n",
    "            file_paths.append(file_path)\n",
    "    else:\n",
    "        print('End Date {} same or earlier than Start Date {} for symbol: {}. '\n",
    "              'Skipping retrieval'.format(dt_end, dt_start, symbol))\n",
    "\n",
    "    return file_paths  # Preserves option to use list of paths for DB insert"
   ],
   "outputs": [],
   "execution_count": 197
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.849192Z",
     "start_time": "2025-04-30T15:46:39.017299Z"
    }
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    RETRIEVAL = False  # Controls retrieval from IEX\n",
    "    INSERTION = True  # Controls insert into local DB\n",
    "    QUERY = False  # Controls whether data for entire SnP500 or a selected subset needs to be retrieved\n",
    "    date_start = None\n",
    "    date_end = None\n",
    "\n",
    "    if QUERY:\n",
    "        query = 'select distinct symbol from industrybackground where SnP500 like 1'\n",
    "        # query = 'select distinct symbol from industrybackground where SnP500 like 1 and symbol not in (select distinct symbol from equities_historic_data where date like \"2019-12-20\")'\n",
    "\n",
    "        with ENGINE.connect() as connection:\n",
    "            result = connection.execute(text(query))\n",
    "        dfrm_lst_symbols = pd.DataFrame(result.mappings().all())\n",
    "        tickers = dfrm_lst_symbols['symbol'].tolist()\n",
    "        # Add some other symbols that I usually track\n",
    "        additional_symbols = ['TWTR', 'TSLA', 'BIDU', 'BABA', 'ROKU', 'NKLA', 'AMTD', 'TFC', 'BX', 'KKR', 'APO', 'ARES',\n",
    "                              'CG']\n",
    "        tickers.extend(additional_symbols)\n",
    "    else:\n",
    "        # tickers = ['BAC', 'JPM', 'C', 'MS', 'GS', 'WFC', 'FB', 'MSFT', 'GOOGL', 'NFLX', 'AAPL', 'AMZN', 'TSLA', 'MRK', 'PFE', 'NKE', 'INTC', 'NVDA', 'ADM', 'TSM', 'MU', 'QCOM']\n",
    "        # tickers.append(['RE', 'ACGL', 'AXS', 'CB', 'THG', 'PGR', 'RNR', 'SIGI', 'TRV', 'WRB'])\n",
    "        # tickers = ['BAC', 'JPM', 'C', 'MS', 'GS', 'WFC', 'GOOGL', 'SMCI', 'NVDA']\n",
    "        tickers = [\"FSLR\", \"VRT\", \"COIN\", \"MRVL\", \"CRWD\", \"AVGO\", \"DDOG\", \"SMCI\", \"GOOGL\", \"AMZN\", \"SHAK\", \"APO\", \"DJT\",\n",
    "                   \"FCX\", \"LLY\", \"META\", \"AVGO\"]\n",
    "\n",
    "        tickers = [\"PFE\", \"MRK\", \"JNJ\", \"REGN\", \"NVO\", \"AAPL\", \"LLY\", \"AVGO\", \"FCX\", \"SBUX\", \"FSLR\", \"NKE\"]\n",
    "\n",
    "    if RETRIEVAL:\n",
    "        # # To be used if manual retrieval for a particular date range is required for certain symbol(s)\n",
    "        # date_start = datetime(2000, 8, 1) # Starting date\n",
    "        # date_start = datetime.combine(date_start, datetime.min.time())\n",
    "        # date_end = datetime(2025, 12, 31) # End date\n",
    "        # date_end = datetime.combine(date_end, datetime.min.time())\n",
    "        # ticker = 'GOOG'\n",
    "        # tickers = [ticker]\n",
    "        date_start = datetime(2000, 8, 1)  # Starting date\n",
    "        for ticker in set(tickers):\n",
    "            retrieve_daily_mkt_data(ticker, date_start, date_end)\n",
    "    if INSERTION:\n",
    "        insert_daily_mkt_data()\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving historic data from file path: /Users/peeyushsharma/Dropbox/workspace/HelloPython/HistoricalMarketData/ByDateRange/nke_20200813_20250429.csv\n",
      "Inserted data for symbol: NKE\n",
      "Retrieving historic data from file path: /Users/peeyushsharma/Dropbox/workspace/HelloPython/HistoricalMarketData/ByDateRange/fslr_20250428_20250429.csv\n",
      "Inserted data for symbol: FSLR\n"
     ]
    }
   ],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.849369Z",
     "start_time": "2025-04-30T15:46:39.138119Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T21:57:34.849592Z",
     "start_time": "2025-04-30T15:46:39.146464Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
