{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXbkEguT0mjv"
   },
   "source": [
    "# NLP Financials Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cpjUpw5X0spA"
   },
   "source": [
    "This notebook will look to generate required NLP data of a financial return in the form that is accepted by the upstream program. I already have a Java program generating NLP data using Stanford CoreNLP, but I now realize that the quality of data generated in Python is better than what I currently have. It also provides more in-depth data and better control of the output. Therefore, it is pragmatic to take a hit right now and invest time in generating NLP data for catalog in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "94ZXBsZL0q4s"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-147-acf557226e00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tflearn'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import requests\n",
    "import string\n",
    "import datetime\n",
    "import pattern\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim import corpora\n",
    "from sklearn.manifold import TSNE\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.utils import lemmatize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcjalK_fJsFa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pshar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pshar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\pshar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pshar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQ-no4lu1oW5"
   },
   "source": [
    "Each word CSV file must have: Symbol name, filing type, word, lemma, POS, NER, fileName, s#, and absolute path\n",
    "Each sentence CSV file must have: Symbol name, filing type, fileName, s#, filePath (local or url), and actual sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w2EQAff229xG"
   },
   "source": [
    "Next, define a function that reads the input HTML file and filters out text devoid of any HTML text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ah3MowZ3A32"
   },
   "outputs": [],
   "source": [
    "def retrieveTxt(htmlPage):\n",
    "  soup = BeautifulSoup(htmlPage, \"lxml\")\n",
    "  tagTypes = ['div', 'p']\n",
    "  tags = (soup).find_all(tagTypes)\n",
    "  origTxt = ''\n",
    "  for t in tags:\n",
    "    origTxt += (t.text+\" \")\n",
    "  return origTxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "javwzJURHdB_"
   },
   "source": [
    "Clean the raw non-HTML text taking out stop words, punctuation marks, some special UTF-8 characters etc... Note that the cleanup here is result of earlier testing that has given me the best possible results thus far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdXxxxL7Hmt8"
   },
   "outputs": [],
   "source": [
    "def genIntermediateTokens(origTxt):\n",
    "  # Start with taking out a UTF-8 token that seems to be prevalent\n",
    "  intermediateTxt = origTxt.replace(u'\\xa0', u' ')\n",
    "  intermediateTokens = nltk.word_tokenize(intermediateTxt)\n",
    "  return intermediateTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJTHn1ed5fIg"
   },
   "source": [
    "The next function takes care of the stop words. But the value of rest of the code in function is somewhat arguable. There are repeated for loops here to avoid one specific condition where words at the end of sentence is getting merged with word at the start of next sentence. TODO:  Look more into the problem, and avoid this double for loop. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iV_XQPYw31gP"
   },
   "outputs": [],
   "source": [
    "def cleanedWordsTxt(intermediateTokens):\n",
    "  stopWords = set(stopwords.words('english') + list(string.punctuation))\n",
    "  cleanTokens = []\n",
    "  for w in intermediateTokens:\n",
    "      if w not in stopWords:\n",
    "          cleanTokens.append(w.lower())\n",
    "          cleanTokens.append(' ') # Need to append a single space for cases where words are losing space in between\n",
    "          \n",
    "  cleanedTxt = ''\n",
    "  cleanedTxtLst = []\n",
    "  for token in cleanTokens:\n",
    "    if token != ' ':\n",
    "      cleanedTxtLst.append(token)\n",
    "      cleanedTxt += (token)\n",
    "  cleanedTxt = cleanedTxt.replace('  ', ' ')  \n",
    "  return cleanedTxtLst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5DADrBXvxZW"
   },
   "source": [
    "Thus far we have tokenized text into words and cleaned them. Now, do the same but keep the unit at sentence level rather than at word level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cA2aeEZY31jC"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "def cleanedSentsTxt(origTxt):\n",
    "  intermediateTxt = origTxt.replace(u'\\xa0', u' ')\n",
    "  sents_tokenized = sent_tokenize(intermediateTxt)\n",
    "  sents_ClnTknzd = []\n",
    "  punctuations = list(string.punctuation) # only remove punctuations. Keep stop words for phrases and un-abbreviated forms. Ex: United States \"of\" America. \n",
    "\n",
    "  for sent in sents_tokenized:\n",
    "    tempStr = ''\n",
    "    tempTokens = nltk.word_tokenize(sent)\n",
    "    for token in tempTokens:\n",
    "      if token not in punctuations:\n",
    "          tempStr += (token)\n",
    "          tempStr += ' '\n",
    "          #cleanTokens.append(' ') # Need to append a single space for cases where words are losing space in between\n",
    "\n",
    "    sents_ClnTknzd.append(tempStr.strip()) # Helps remove the space at the end of tempStr\n",
    "\n",
    "  return sents_ClnTknzd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a function to generate metadata such as word itself, lemma, NER, PoS etc... for individual words in a sentence. Earlier, sentences and words in those sentences were being pushed to two separate files for each symbol and filing. However, that was causing the unique ID to be different upon indexing for a sentence and words within it. That, in turn, would not allow sentences and words to both appear under a single result and there was no way to then group them. To avoid all that, the words metadata must get indexed with the sentence that includes those words in a single unique identifier within Solr. It presents a challenge in that while a sentence is unique with in a single row, now the \"word\" or \"pos\" or \"ner\" will no longer be unique even within a single document as Solr defines it. Each row will contain all the words and their respective pos within a single sentence. This will not only increase the size of input file for indexing, but will also increase duration and complexity. I spent few days thinking about alternate solutions but couldn't figure out a way to combine sentence and words annotation metadata if they were indexed at separate times, therefore, this effort now as an option of last resort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "enableNER = False # Keep it disabled unless Stanford CoreNLP server is running locally and you are on a loaded (CPU, mem) instance\n",
    "stopWords = set(stopwords.words('english') + list(string.punctuation))\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# create an object of stemming function\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "if (enableNER):\n",
    "  from nltk.parse import CoreNLPParser\n",
    "  try: # Careful when enabling NER generation. It can increase compute times by order of 20-30 times \n",
    "    ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n",
    "  except:\n",
    "    print(\"NER_Tagger not available. Ensure that local CoreNLP Server is running.\")\n",
    "\n",
    "def genLemmas(sent):\n",
    "    i = 0\n",
    "    tokens = word_tokenize(sent)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    if (enableNER):\n",
    "      ner = ner_tagger.tag(tokens)\n",
    "    posLength = len(pos)\n",
    "    # Higher level list which will contain map with token as \"k\" and list of lemma, ner, pos as \"v\"\n",
    "    annListForSent = []\n",
    "       \n",
    "    try:\n",
    "      for token in tokens:\n",
    "        if token not in stopWords:\n",
    "          tknEntry = {}\n",
    "          lstTemp = [] \n",
    "          # lemma = lemmatizer.lemmatize(token) # Didn't work \n",
    "          lemma = stemmer.stem(token)\n",
    "          posWord = pos[i][1] \n",
    "          if (enableNER): nerWord = ner[i][1] \n",
    "          else: nerWord = 'ner'\n",
    "\n",
    "          lstTemp.append(lemma); lstTemp.append(posWord); lstTemp.append(nerWord) # 0:lemma, 1:pos, 2:ner, \n",
    "          tknEntry[token] = lstTemp\n",
    "\n",
    "\n",
    "          annListForSent.append(tknEntry)\n",
    "        i += 1 # Must increase value of i regardless of whether the word is in stopWords or not. Recall that POS and NER were computed with stopwords included. \n",
    "        \n",
    "    except IndexError:\n",
    "      print(\"List index out of range likely for PoS or NER tagger\")\n",
    "      print(\"Sentence where the error occured is: '\\\"+sent+\\\"'\")\n",
    "      print(\"Skipping indexing of this sentence.\")\n",
    "    \n",
    "    return annListForSent\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsent = \"The Company has entered into a guarantee for the benefit of PSA Group and pursuant to     which the Company has agreed to guarantee the Seller\\'s obligation to indemnify PSA Group for certain     losses resulting from any inaccuracy of certain representations and warranties or breaches of our covenants     in the Agreement and for certain other liabilities.\"\\nannListForSent = genLemmas(sent)\\nannListForSent\\n'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sent = \"The Company has entered into a guarantee for the benefit of PSA Group and pursuant to \\\n",
    "    which the Company has agreed to guarantee the Seller's obligation to indemnify PSA Group for certain \\\n",
    "    losses resulting from any inaccuracy of certain representations and warranties or breaches of our covenants \\\n",
    "    in the Agreement and for certain other liabilities.\"\n",
    "annListForSent = genLemmas(sent)\n",
    "annListForSent\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g657mDRYsDfN"
   },
   "source": [
    "It is important to use sentences to figure out Parts of Speech. If you determine PoS just based on list of words then the context is likely to be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jFyOhOU31mh"
   },
   "outputs": [],
   "source": [
    "def genAnnotations(symbol, filingType, cleanedSentsList, nameInputFile, url):\n",
    "  annList = []\n",
    "  sentList = []\n",
    "\n",
    "  sCount = 0\n",
    "  for sent in cleanedSentsList:\n",
    "    sCount += 1\n",
    "    \n",
    "    \n",
    "    lstSent = []\n",
    "    lstSent.append(symbol); lstSent.append(filingType); lstSent.append(nameInputFile)\n",
    "    lstSent.append('s'+str(sCount)); lstSent.append(url); lstSent.append(sent.replace(\",\", \"\"))\n",
    "    # This is purely to group search results by sentence. Solr won't allow grouping by large text fields, so hash it. \n",
    "    hash = hashlib.sha224(bytes(sent.replace(\",\", \"\"), encoding='utf-8')).hexdigest()\n",
    "    lstSent.append(hash)\n",
    "    # Now, generate annotations at individual word level\n",
    "    annListForSent = genLemmas(sent)\n",
    "    cntWords = len(annListForSent)\n",
    "    lstSent.append(cntWords)\n",
    "    for kvPair in annListForSent:\n",
    "        for key, value in kvPair.items():\n",
    "            lstSent.append(key)\n",
    "            lstSent.append(value[0])\n",
    "            lstSent.append(value[1])\n",
    "            lstSent.append(value[2])\n",
    "\n",
    "    #tempSentStr = symbol + \",\" + filingType + \",\" + nameInputFile + \",\" + str(sCount) + \",\" + url + \",\" + sent.replace(\",\", \"\")\n",
    "    sentList.append(lstSent)\n",
    "    \n",
    "  return sentList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ck2WbUl31ul"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "filesLocal = True\n",
    "dirNLPOut = 'C:/Users/pshar/Dropbox/Programming/SampleTexts/NLP_Output'\n",
    "\"\"\"\n",
    "urls = []\n",
    "urls.append(\"https://www.sec.gov/Archives/edgar/data/886982/000156459019008879/gs-424b2.htm\")\n",
    "# Manual settings for the test run. \n",
    "symbol = 'GS'\n",
    "filingType = '10-K'\n",
    "nameInputFile = 'd480167d10k.htm'\n",
    "\"\"\"\n",
    "\n",
    "page = ''\n",
    "def traverseFilingFiles(symbol, filingType, filePath):\n",
    "    (parentDir, nameInputFile) = os.path.split(filePath)\n",
    "    origTxt = ''\n",
    "    if (filesLocal == False):\n",
    "        # The filePath here should be URL. \n",
    "        # TODO: Handle upstream function to send URL or localPath both\n",
    "        htmlPage = requests.get(filePath).text # Retrieve text for one file at one point\n",
    "        origTxt = retrieveTxt(htmlPage)\n",
    "    else: \n",
    "        try:\n",
    "            with open(filePath, 'r', encoding='utf8') as content_file:\n",
    "                htmlPage = content_file.read()\n",
    "                origTxt = retrieveTxt(htmlPage)\n",
    "        except UnicodeDecodeError: \n",
    "            print (\"UnicodeDecodeError reading HTML: '\"+filePath+\"' for symbol:\"+symbol)\n",
    "        \n",
    "    \"\"\"\n",
    "    # Commenting out individual words based function calls, while keeping analysis at the sentence level\n",
    "    intermediateTokens = genIntermediateTokens(origTxt)\n",
    "    cleanedWordsList = cleanedWordsTxt(intermediateTokens)\n",
    "    \"\"\"\n",
    "    cleanedSentsList = cleanedSentsTxt(origTxt)\n",
    "    sentList = genAnnotations(symbol, filingType, cleanedSentsList, nameInputFile, filePath)\n",
    "    #print(sentList)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    time = str(now)[:19].replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "\n",
    "    \"\"\"\n",
    "    # Disabling Annotations (words) output for the time-being until Solr cataloging is resolved\n",
    "    \n",
    "    fileWords = 'Annotations_'+symbol+'_'+filingType+'_'+nameInputFile+'_'+time+'.csv'\n",
    "    absPathFileWords = os.path.join(dirNLPOut, fileWords)\n",
    "    print(fileWords)\n",
    "    dfrmWords = pd.DataFrame(annList)\n",
    "    dfrmWords.to_csv(absPathFileWords, sep =\",\", index = None, header=False, quoting=csv.QUOTE_NONE, escapechar=\"\\\\\")\n",
    "    \"\"\"\n",
    "    \n",
    "    fileSents = 'Sentences_'+symbol+'_'+filingType+'_'+nameInputFile+'_'+time+'.csv'\n",
    "    absPathFileSents = os.path.join(dirNLPOut, fileSents)\n",
    "    print(fileSents)\n",
    "    dfrmSents = pd.DataFrame(sentList)#, columns = labelSents)\n",
    "    #print(dfrmSents)\n",
    "    dfrmSents.to_csv(absPathFileSents, sep =\",\", index = None, header=False, quoting=csv.QUOTE_NONE, escapechar=\"\\\\\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve & Traverse Filings Previously Downloaded "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base functions to traverse symbols, then filing types (10-k, 10-Q...), and then individual filings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with listing symbols whose filings were earlier downloaded in a dir listing arranged by symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4pBnHc6PJhST"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isdir\n",
    "from os.path import abspath\n",
    "\"\"\"\n",
    "The base location locally \n",
    "\"\"\"\n",
    "def genListDirsSymbols(basePath):\n",
    "    symbols = []\n",
    "    dirsSymbols = os.listdir(basePath)\n",
    "    for dirSymbol in dirsSymbols:\n",
    "        (head, tail) = os.path.split(dirSymbol)\n",
    "        symbols.append(tail)\n",
    "    return symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each symbol, generate listing of folders underneath. This listing will correspond to filing types (10-k etc...) downloaded for each of these symbols. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def genListFilingsDirsForSymbol(basePathPlusSymbol):\n",
    "    filingDirs = []\n",
    "    dirsFilings = os.listdir(basePathPlusSymbol)\n",
    "    for dirFilings in dirsFilings:\n",
    "        (head, tail) = os.path.split(dirFilings)\n",
    "        filingDirs.append(tail)\n",
    "    return filingDirs   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve listing of files for given symbol and given filing type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genListFilingsForSymbol(basePathPlusSymbolPlusFilingDir):\n",
    "    filings = []\n",
    "    files = os.listdir(basePathPlusSymbolPlusFilingDir)\n",
    "    for file in files:\n",
    "        (head, tail) = os.path.split(file)\n",
    "        if (tail != 'txt'): \n",
    "            filings.append(file)\n",
    "    return filings\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have functions defined to list symbols, filing types, and actual files defined, go through them as a list and traverse these files grouping them first by symbol and then by filing type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences_BAC_10-K_bac1231201610xkhtm_2019-04-02_21-27-38.csv\n",
      "Sentences_BAC_10-K_bac1231201710xkhtm_2019-04-02_21-30-09.csv\n",
      "Sentences_BAC_10-K_bac1231201810xkhtm_2019-04-02_21-31-01.csv\n",
      "Sentences_BAC_10-Q_bac331201810xqhtm_2019-04-02_21-31-42.csv\n",
      "Sentences_BAC_10-Q_bac630201710xqhtm_2019-04-02_21-32-38.csv\n",
      "Sentences_BAC_10-Q_bac630201810xqhtm_2019-04-02_21-33-28.csv\n",
      "Sentences_BAC_10-Q_bac930201710xqhtm_2019-04-02_21-34-19.csv\n",
      "Sentences_BAC_10-Q_bac930201810xqhtm_2019-04-02_21-35-08.csv\n",
      "Sentences_C_10-K_c12312016x10khtm_2019-04-02_21-36-38.csv\n",
      "Sentences_C_10-K_c12312017x10khtm_2019-04-02_21-40-24.csv\n",
      "Sentences_C_10-K_c12312018x10khtm_2019-04-02_21-43-47.csv\n",
      "Sentences_C_10-Q_c3312018x10qhtm_2019-04-02_21-46-30.csv\n",
      "Sentences_C_10-Q_c6302017x10qhtm_2019-04-02_21-47-35.csv\n",
      "Sentences_C_10-Q_c6302018x10qhtm_2019-04-02_21-49-08.csv\n",
      "Sentences_C_10-Q_c9302017x10qhtm_2019-04-02_21-50-43.csv\n",
      "Sentences_C_10-Q_c9302018x10qhtm_2019-04-02_21-52-14.csv\n",
      "Sentences_GS_10-K_d308759d10khtm_2019-04-02_21-53-59.csv\n",
      "Sentences_GS_10-K_d480167d10khtm_2019-04-02_21-55-29.csv\n",
      "Sentences_GS_10-K_d669877d10khtm_2019-04-02_21-57-24.csv\n",
      "Sentences_GS_10-Q_d352886d10qhtm_2019-04-02_21-58-31.csv\n",
      "Sentences_GS_10-Q_d416122d10qhtm_2019-04-02_21-59-19.csv\n",
      "Sentences_GS_10-Q_d464365d10qhtm_2019-04-02_22-00-22.csv\n",
      "Sentences_GS_10-Q_d535211d10qhtm_2019-04-02_22-01-22.csv\n",
      "Sentences_GS_10-Q_d631824d10qhtm_2019-04-02_22-02-22.csv\n",
      "Sentences_JPM_10-K_corp10k2016htm_2019-04-02_22-04-05.csv\n",
      "Sentences_JPM_10-K_corp10k2017htm_2019-04-02_22-07-34.csv\n",
      "Sentences_JPM_10-K_corp10k2018htm_2019-04-02_22-09-41.csv\n",
      "Sentences_JPM_10-Q_corpq12018htm_2019-04-02_22-10-23.csv\n",
      "Sentences_JPM_10-Q_corpq22017htm_2019-04-02_22-11-37.csv\n",
      "Sentences_JPM_10-Q_corpq22018htm_2019-04-02_22-14-06.csv\n",
      "Sentences_JPM_10-Q_corpq32017htm_2019-04-02_22-16-25.csv\n",
      "Sentences_JPM_10-Q_corpq32018htm_2019-04-02_22-18-31.csv\n",
      "Sentences_MS_10-K_d328282d10khtm_2019-04-02_22-19-12.csv\n",
      "Sentences_MS_10-K_d500533d10khtm_2019-04-02_22-20-21.csv\n",
      "Sentences_MS_10-K_d707577d10khtm_2019-04-02_22-21-53.csv\n",
      "Sentences_MS_10-Q_d421308d10qhtm_2019-04-02_22-22-54.csv\n",
      "Sentences_MS_10-Q_d437627d10qhtm_2019-04-02_22-23-25.csv\n",
      "Sentences_MS_10-Q_d556270d10qhtm_2019-04-02_22-23-59.csv\n",
      "Sentences_MS_10-Q_d576100d10qhtm_2019-04-02_22-24-39.csv\n",
      "Sentences_MS_10-Q_d645593d10qhtm_2019-04-02_22-25-13.csv\n",
      "Sentences_WFC_10-K_wfc12312016x10khtm_2019-04-02_22-25-29.csv\n",
      "Sentences_WFC_10-K_wfc12312017x10khtm_2019-04-02_22-25-37.csv\n",
      "Sentences_WFC_10-K_wfc12312018x10khtm_2019-04-02_22-25-44.csv\n",
      "Sentences_WFC_10-Q_wfc03312018x10qhtm_2019-04-02_22-26-29.csv\n",
      "Sentences_WFC_10-Q_wfc06302017x10qhtm_2019-04-02_22-27-30.csv\n",
      "Sentences_WFC_10-Q_wfc06302018x10qhtm_2019-04-02_22-28-37.csv\n",
      "Sentences_WFC_10-Q_wfc09302017x10qhtm_2019-04-02_22-29-41.csv\n",
      "Sentences_WFC_10-Q_wfc09302018x10qhtm_2019-04-02_22-31-06.csv\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "dirBase = 'C:/Users/pshar/Dropbox/WebServices/mysite/polls/templates/polls/FilingsBySymbols'\n",
    "filingsFullLst = []\n",
    "symbols = genListDirsSymbols(dirBase)\n",
    "for symbol in symbols:\n",
    "    basePathPlusSymbol = os.path.join(dirBase, symbol.upper())\n",
    "    filingDirs = genListFilingsDirsForSymbol(basePathPlusSymbol)\n",
    "    for filingDir in filingDirs:\n",
    "        basePathPlusSymbolPlusFilingDir =  os.path.join(basePathPlusSymbol, filingDir)\n",
    "        filings = genListFilingsForSymbol(basePathPlusSymbolPlusFilingDir)\n",
    "        for filing in filings:\n",
    "            filePath = os.path.join(basePathPlusSymbolPlusFilingDir, filing)\n",
    "            traverseFilingFiles(symbol, filingDir, filePath)\n",
    "               \n",
    "            \n",
    "    \n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Enhancements\n",
    "\n",
    "- P1: The Lemmatization API being called is yielding poor results. Not relying on lemma right now but need to correct that later. \n",
    "- P1: Keep reference to NLP_Financials_Cloud.ipynb for other possible enhancements.\n",
    "- P1: Can we introduce topic modeling for each paragraph? The p HTML tag is already defined.  \n",
    "- P1: Bring complete cycle (download, generate NLP, and indexing) here rather than just the middle part. \n",
    "- P1: If not full cycle, then keep the ability to generate NLP from both local files as well as from EDGAR. Couple of function may require little tweaking. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of NLP_Financials_Watson.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1cjKxpZEdGZs6mrA1YgsHfvlwuBwqNcp8",
     "timestamp": 1553284494143
    },
    {
     "file_id": "15ldwpg8GAFQvyfBI5oyrfHNDn35ogFdK",
     "timestamp": 1553267652188
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
