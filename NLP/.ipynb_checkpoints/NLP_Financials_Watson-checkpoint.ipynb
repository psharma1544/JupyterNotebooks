{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YXbkEguT0mjv"
   },
   "source": [
    "# NLP Financials Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cpjUpw5X0spA"
   },
   "source": [
    "This notebook will look to generate required NLP data of a financial return in the form that is accepted by the upstream program. I already have a Java program generating NLP data using Stanford CoreNLP, but I now realize that the quality of data generated in Python is better than what I currently have. It also provides more in-depth data and better control of the output. Therefore, it is pragmatic to take a hit right now and invest time in generating NLP data for catalog in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "94ZXBsZL0q4s"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import requests\n",
    "import string\n",
    "import datetime\n",
    "import pattern\n",
    "import hashlib\n",
    "\n",
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim import corpora\n",
    "from sklearn.manifold import TSNE\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim.utils import lemmatize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcjalK_fJsFa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pshar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pshar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\pshar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pshar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQ-no4lu1oW5"
   },
   "source": [
    "Each word CSV file must have: Symbol name, filing type, word, lemma, POS, NER, fileName, s#, and absolute path\n",
    "Each sentence CSV file must have: Symbol name, filing type, fileName, s#, filePath (local or url), and actual sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w2EQAff229xG"
   },
   "source": [
    "Next, define a function that reads the input HTML file and filters out text devoid of any HTML text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ah3MowZ3A32"
   },
   "outputs": [],
   "source": [
    "def retrieveTxt(htmlPage):\n",
    "  soup = BeautifulSoup(htmlPage, \"lxml\")\n",
    "  tagTypes = ['div', 'p']\n",
    "  tags = (soup).find_all(tagTypes)\n",
    "  origTxt = ''\n",
    "  for t in tags:\n",
    "    origTxt += (t.text+\" \")\n",
    "  return origTxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "javwzJURHdB_"
   },
   "source": [
    "Clean the raw non-HTML text taking out stop words, punctuation marks, some special UTF-8 characters etc... Note that the cleanup here is result of earlier testing that has given me the best possible results thus far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdXxxxL7Hmt8"
   },
   "outputs": [],
   "source": [
    "def genIntermediateTokens(origTxt):\n",
    "  # Start with taking out a UTF-8 token that seems to be prevalent\n",
    "  intermediateTxt = origTxt.replace(u'\\xa0', u' ')\n",
    "  intermediateTokens = nltk.word_tokenize(intermediateTxt)\n",
    "  return intermediateTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJTHn1ed5fIg"
   },
   "source": [
    "The next function takes care of the stop words. But the value of rest of the code in function is somewhat arguable. There are repeated for loops here to avoid one specific condition where words at the end of sentence is getting merged with word at the start of next sentence. TODO:  Look more into the problem, and avoid this double for loop. \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iV_XQPYw31gP"
   },
   "outputs": [],
   "source": [
    "def cleanedWordsTxt(intermediateTokens):\n",
    "  stopWords = set(stopwords.words('english') + list(string.punctuation))\n",
    "  cleanTokens = []\n",
    "  for w in intermediateTokens:\n",
    "      if w not in stopWords:\n",
    "          cleanTokens.append(w.lower())\n",
    "          cleanTokens.append(' ') # Need to append a single space for cases where words are losing space in between\n",
    "          \n",
    "  cleanedTxt = ''\n",
    "  cleanedTxtLst = []\n",
    "  for token in cleanTokens:\n",
    "    if token != ' ':\n",
    "      cleanedTxtLst.append(token)\n",
    "      cleanedTxt += (token)\n",
    "  cleanedTxt = cleanedTxt.replace('  ', ' ')  \n",
    "  return cleanedTxtLst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5DADrBXvxZW"
   },
   "source": [
    "Thus far we have tokenized text into words and cleaned them. Now, do the same but keep the unit at sentence level rather than at word level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cA2aeEZY31jC"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "def cleanedSentsTxt(origTxt):\n",
    "  intermediateTxt = origTxt.replace(u'\\xa0', u' ')\n",
    "  sents_tokenized = sent_tokenize(intermediateTxt)\n",
    "  sents_ClnTknzd = []\n",
    "  punctuations = list(string.punctuation) # only remove punctuations. Keep stop words for phrases and un-abbreviated forms. Ex: United States \"of\" America. \n",
    "\n",
    "  for sent in sents_tokenized:\n",
    "    tempStr = ''\n",
    "    tempTokens = nltk.word_tokenize(sent)\n",
    "    for token in tempTokens:\n",
    "      if token not in punctuations:\n",
    "          tempStr += (token)\n",
    "          tempStr += ' '\n",
    "          #cleanTokens.append(' ') # Need to append a single space for cases where words are losing space in between\n",
    "\n",
    "    sents_ClnTknzd.append(tempStr.strip()) # Helps remove the space at the end of tempStr\n",
    "\n",
    "  return sents_ClnTknzd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g657mDRYsDfN"
   },
   "source": [
    "It is important to use sentences to figure out Parts of Speech. If you determine PoS just based on list of words then the context is likely to be lost. This also presents a slight problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jFyOhOU31mh"
   },
   "outputs": [],
   "source": [
    "enableNER = False\n",
    "stopWords = set(stopwords.words('english') + list(string.punctuation))\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# create an object of stemming function\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "if (enableNER):\n",
    "  from nltk.parse import CoreNLPParser\n",
    "  try:\n",
    "    ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n",
    "  except:\n",
    "    print(\"NER_Tagger not available. Ensure that local CoreNLP Server is running.\")\n",
    "\n",
    "def genAnnotations(symbol, filingType, cleanedSentsList, nameInputFile, url):\n",
    "  annList = []\n",
    "  sentList = []\n",
    "\n",
    "  sCount = 0\n",
    "  for sent in cleanedSentsList:\n",
    "    sCount += 1\n",
    "    i = 0\n",
    "    \n",
    "    # Disabling Annotations (words) output for the time-being until Solr cataloging is resolved\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sent)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    if (enableNER):\n",
    "      ner = ner_tagger.tag(tokens)\n",
    "    posLength = len(pos)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    lstSent = []\n",
    "    lstSent.append(symbol); lstSent.append(filingType); lstSent.append(nameInputFile)\n",
    "    lstSent.append('s'+str(sCount)); lstSent.append(url); lstSent.append(sent.replace(\",\", \"\"))\n",
    "    # This is purely to group search results by sentence. Solr won't allow grouping by large text fields, so hash it. \n",
    "    hash = hashlib.sha224(bytes(sent.replace(\",\", \"\"), encoding='utf-8')).hexdigest()\n",
    "    lstSent.append(hash)\n",
    "    #tempSentStr = symbol + \",\" + filingType + \",\" + nameInputFile + \",\" + str(sCount) + \",\" + url + \",\" + sent.replace(\",\", \"\")\n",
    "    sentList.append(lstSent)\n",
    "\n",
    "    # Disabling Annotations (words) output for the time-being until Solr cataloging is resolved\n",
    "    \"\"\"\n",
    "    try:\n",
    "      for token in tokens:\n",
    "        if token not in stopWords:\n",
    "          # lemma = lemmatizer.lemmatize(token) # Didn't work \n",
    "          lemma = stemmer.stem(token)\n",
    "          posWord = pos[i][1] \n",
    "          if (enableNER):\n",
    "            nerWord = ner[i][1] \n",
    "          else:\n",
    "            nerWord = 'ner'\n",
    "                 \n",
    "          lstWords = []\n",
    "          lstWords.append(symbol); lstWords.append(filingType); lstWords.append(token)\n",
    "          lstWords.append(lemma); lstWords.append(posWord); lstWords.append(nerWord)\n",
    "          lstWords.append(nameInputFile); lstWords.append('s'+str(sCount)); lstWords.append(url)\n",
    "          \n",
    "          annList.append(lstWords)\n",
    "        i += 1 # Must increase value of i regardless of whether the word is in stopWords or not. Recall that POS and NER were computed with stopwords included. \n",
    "        \n",
    "    except IndexError:\n",
    "      print(\"List index out of range likely for PoS or NER tagger\")\n",
    "      print(\"Sentence where the error occured is: '\\\"+sent+\\\"'\")\n",
    "      print(\"Skipping indexing of this sentence.\")\n",
    "    \"\"\"\n",
    "  return (annList, sentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ck2WbUl31ul"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "filesLocal = True\n",
    "dirNLPOut = 'C:/Users/pshar/Dropbox/Programming/SampleTexts/NLP_Output'\n",
    "\"\"\"\n",
    "urls = []\n",
    "urls.append(\"https://www.sec.gov/Archives/edgar/data/886982/000156459019008879/gs-424b2.htm\")\n",
    "# Manual settings for the test run. \n",
    "symbol = 'GS'\n",
    "filingType = '10-K'\n",
    "nameInputFile = 'd480167d10k.htm'\n",
    "\"\"\"\n",
    "\n",
    "page = ''\n",
    "def traverseFilingFiles(symbol, filingType, filePath):\n",
    "    (parentDir, nameInputFile) = os.path.split(filePath)\n",
    "    origTxt = ''\n",
    "    if (filesLocal == False):\n",
    "        # The filePath here should be URL. \n",
    "        # TODO: Handle upstream function to send URL or localPath both\n",
    "        htmlPage = requests.get(filePath).text # Retrieve text for one file at one point\n",
    "        origTxt = retrieveTxt(htmlPage)\n",
    "    else: \n",
    "        try:\n",
    "            with open(filePath, 'r', encoding='utf8') as content_file:\n",
    "                htmlPage = content_file.read()\n",
    "                origTxt = retrieveTxt(htmlPage)\n",
    "        except UnicodeDecodeError: \n",
    "            print (\"UnicodeDecodeError reading HTML: '\"+filePath+\"' for symbol:\"+symbol)\n",
    "        \n",
    "    \"\"\"\n",
    "    # Commenting out individual words based function calls, while keeping analysis at the sentence level\n",
    "    intermediateTokens = genIntermediateTokens(origTxt)\n",
    "    cleanedWordsList = cleanedWordsTxt(intermediateTokens)\n",
    "    \"\"\"\n",
    "    cleanedSentsList = cleanedSentsTxt(origTxt)\n",
    "    (annList, sentList) = genAnnotations(symbol, filingType, cleanedSentsList, nameInputFile, filePath)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    time = str(now)[:19].replace(\":\", \"-\").replace(\" \", \"_\")\n",
    "\n",
    "    \n",
    "    # Disabling Annotations (words) output for the time-being until Solr cataloging is resolved\n",
    "    \"\"\"\n",
    "    #fileWords = 'Annotations_'+symbol+'_'+filingType+'_'+nameInputFile+'_'+time+'.csv'\n",
    "    #absPathFileWords = os.path.join(dirNLPOut, fileWords)\n",
    "    #print(fileWords)\n",
    "    #dfrmWords = pd.DataFrame(annList)\n",
    "    #dfrmWords.to_csv(absPathFileWords, sep =\",\", index = None, header=False, quoting=csv.QUOTE_NONE, escapechar=\"\\\\\")\n",
    "    \"\"\"\n",
    "    fileSents = 'Sentences_'+symbol+'_'+filingType+'_'+nameInputFile+'_'+time+'.csv'\n",
    "    absPathFileSents = os.path.join(dirNLPOut, fileSents)\n",
    "    print(fileSents)\n",
    "    dfrmSents = pd.DataFrame(sentList)#, columns = labelSents)\n",
    "    dfrmSents.to_csv(absPathFileSents, sep =\",\", index = None, header=False, quoting=csv.QUOTE_NONE, escapechar=\"\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve & Traverse Filings Previously Downloaded "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base functions to traverse symbols, then filing types (10-k, 10-Q...), and then individual filings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with listing symbols whose filings were earlier downloaded in a dir listing arranged by symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4pBnHc6PJhST"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isdir\n",
    "from os.path import abspath\n",
    "\"\"\"\n",
    "The base location locally \n",
    "\"\"\"\n",
    "def genListDirsSymbols(basePath):\n",
    "    symbols = []\n",
    "    dirsSymbols = os.listdir(basePath)\n",
    "    for dirSymbol in dirsSymbols:\n",
    "        (head, tail) = os.path.split(dirSymbol)\n",
    "        symbols.append(tail)\n",
    "    return symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each symbol, generate listing of folders underneath. This listing will correspond to filing types (10-k etc...) downloaded for each of these symbols. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def genListFilingsDirsForSymbol(basePathPlusSymbol):\n",
    "    filingDirs = []\n",
    "    dirsFilings = os.listdir(basePathPlusSymbol)\n",
    "    for dirFilings in dirsFilings:\n",
    "        (head, tail) = os.path.split(dirFilings)\n",
    "        filingDirs.append(tail)\n",
    "    return filingDirs   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve listing of files for given symbol and given filing type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genListFilingsForSymbol(basePathPlusSymbolPlusFilingDir):\n",
    "    filings = []\n",
    "    files = os.listdir(basePathPlusSymbolPlusFilingDir)\n",
    "    for file in files:\n",
    "        (head, tail) = os.path.split(file)\n",
    "        if (tail != 'txt'): \n",
    "            filings.append(file)\n",
    "    return filings\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have functions defined to list symbols, filing types, and actual files defined, go through them as a list and traverse these files grouping them first by symbol and then by filing type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences_JPM_10-K_corp10k2016htm_2019-03-28_11-02-57.csv\n",
      "Sentences_JPM_10-K_corp10k2017htm_2019-03-28_11-03-40.csv\n",
      "Sentences_JPM_10-K_corp10k2018htm_2019-03-28_11-03-43.csv\n",
      "Sentences_JPM_10-Q_corpq12018htm_2019-03-28_11-04-03.csv\n",
      "Sentences_JPM_10-Q_corpq22017htm_2019-03-28_11-04-24.csv\n",
      "Sentences_JPM_10-Q_corpq22018htm_2019-03-28_11-04-48.csv\n",
      "Sentences_JPM_10-Q_corpq32017htm_2019-03-28_11-05-17.csv\n",
      "Sentences_JPM_10-Q_corpq32018htm_2019-03-28_11-05-18.csv\n",
      "Sentences_MRK_10-K_mrk1231201610khtm_2019-03-28_11-05-27.csv\n",
      "Sentences_MRK_10-K_mrk1231201710khtm_2019-03-28_11-05-35.csv\n",
      "Sentences_MRK_10-K_mrk1231201810khtm_2019-03-28_11-05-44.csv\n",
      "Sentences_MRK_10-Q_mrk0331201810qhtm_2019-03-28_11-05-50.csv\n",
      "Sentences_MRK_10-Q_mrk0630201710qhtm_2019-03-28_11-05-54.csv\n",
      "Sentences_MRK_10-Q_mrk0630201810qhtm_2019-03-28_11-05-59.csv\n",
      "Sentences_MRK_10-Q_mrk0930201710qhtm_2019-03-28_11-06-03.csv\n",
      "Sentences_MRK_10-Q_mrk0930201810qhtm_2019-03-28_11-06-11.csv\n",
      "Sentences_MS_10-K_d328282d10khtm_2019-03-28_11-06-31.csv\n",
      "Sentences_MS_10-K_d500533d10khtm_2019-03-28_11-06-48.csv\n",
      "Sentences_MS_10-K_d707577d10khtm_2019-03-28_11-07-04.csv\n",
      "Sentences_MS_10-Q_d421308d10qhtm_2019-03-28_11-07-11.csv\n",
      "Sentences_MS_10-Q_d437627d10qhtm_2019-03-28_11-07-20.csv\n",
      "Sentences_MS_10-Q_d556270d10qhtm_2019-03-28_11-07-28.csv\n",
      "Sentences_MS_10-Q_d576100d10qhtm_2019-03-28_11-07-36.csv\n",
      "Sentences_MS_10-Q_d645593d10qhtm_2019-03-28_11-07-46.csv\n",
      "Sentences_MSFT_10-K_d187868d10khtm_2019-03-28_11-07-49.csv\n",
      "Sentences_MSFT_10-K_msft10k_20170630htm_2019-03-28_11-07-55.csv\n",
      "Sentences_MSFT_10-K_msft10k_20180630htm_2019-03-28_11-08-01.csv\n",
      "Sentences_MSFT_10-Q_msft10q_20170930htm_2019-03-28_11-08-05.csv\n",
      "Sentences_MSFT_10-Q_msft10q_20171231htm_2019-03-28_11-08-09.csv\n",
      "Sentences_MSFT_10-Q_msft10q_20180331htm_2019-03-28_11-08-14.csv\n",
      "Sentences_MSFT_10-Q_msft10q_20180930htm_2019-03-28_11-08-17.csv\n",
      "Sentences_MSFT_10-Q_msft10q_20181231htm_2019-03-28_11-08-22.csv\n",
      "Sentences_NFLX_10-K_form10k_q418htm_2019-03-28_11-08-25.csv\n",
      "Sentences_NFLX_10-Q_nflx033118x10qxdochtm_2019-03-28_11-08-27.csv\n",
      "Sentences_NFLX_10-Q_nflx063017x10qxdochtm_2019-03-28_11-08-30.csv\n",
      "Sentences_NFLX_10-Q_nflx063018x10qxdochtm_2019-03-28_11-08-32.csv\n",
      "Sentences_NFLX_10-Q_nflx093017x10qxdochtm_2019-03-28_11-08-34.csv\n",
      "Sentences_NFLX_10-Q_nflx093018x10qxdochtm_2019-03-28_11-08-38.csv\n",
      "Sentences_PFE_10-K_pfe12312016x10kshellhtm_2019-03-28_11-08-40.csv\n",
      "Sentences_PFE_10-K_pfe12312017x10kshellhtm_2019-03-28_11-08-43.csv\n",
      "Sentences_PFE_10-K_pfe12312018x10kshellhtm_2019-03-28_11-08-45.csv\n",
      "Sentences_PFE_10-Q_pfe04012018x10qhtm_2019-03-28_11-08-51.csv\n",
      "Sentences_PFE_10-Q_pfe07012018x10qhtm_2019-03-28_11-08-58.csv\n",
      "Sentences_PFE_10-Q_pfe07022017x10qhtm_2019-03-28_11-08-58.csv\n",
      "Sentences_PFE_10-Q_pfe09302018x10qhtm_2019-03-28_11-09-05.csv\n",
      "Sentences_PFE_10-Q_pfe10012017x10qhtm_2019-03-28_11-09-06.csv\n",
      "Sentences_WFC_10-K_wfc12312016x10khtm_2019-03-28_11-09-07.csv\n",
      "Sentences_WFC_10-K_wfc12312017x10khtm_2019-03-28_11-09-10.csv\n",
      "Sentences_WFC_10-K_wfc12312018x10khtm_2019-03-28_11-09-12.csv\n",
      "Sentences_WFC_10-Q_wfc03312018x10qhtm_2019-03-28_11-09-28.csv\n",
      "Sentences_WFC_10-Q_wfc06302017x10qhtm_2019-03-28_11-09-44.csv\n",
      "Sentences_WFC_10-Q_wfc06302018x10qhtm_2019-03-28_11-10-01.csv\n",
      "Sentences_WFC_10-Q_wfc09302017x10qhtm_2019-03-28_11-10-18.csv\n",
      "Sentences_WFC_10-Q_wfc09302018x10qhtm_2019-03-28_11-10-35.csv\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "dirBase = 'C:/Users/pshar/Dropbox/WebServices/mysite/polls/templates/polls/FilingsBySymbols'\n",
    "filingsFullLst = []\n",
    "symbols = genListDirsSymbols(dirBase)\n",
    "for symbol in symbols:\n",
    "    basePathPlusSymbol = os.path.join(dirBase, symbol.upper())\n",
    "    filingDirs = genListFilingsDirsForSymbol(basePathPlusSymbol)\n",
    "    for filingDir in filingDirs:\n",
    "        basePathPlusSymbolPlusFilingDir =  os.path.join(basePathPlusSymbol, filingDir)\n",
    "        filings = genListFilingsForSymbol(basePathPlusSymbolPlusFilingDir)\n",
    "        for filing in filings:\n",
    "            filePath = os.path.join(basePathPlusSymbolPlusFilingDir, filing)\n",
    "            traverseFilingFiles(symbol, filingDir, filePath)\n",
    "    \n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Enhancements\n",
    "\n",
    "- P1: The Lemmatization API being called is yielding poor results. Not relying on lemma right now but need to correct that later. \n",
    "- P1: Keep reference to NLP_Financials_Cloud.ipynb for other possible enhancements.\n",
    "- P1: Can we introduce topic modeling for each paragraph? The p HTML tag is already defined.  \n",
    "- P1: Bring complete cycle (download, generate NLP, and indexing) here rather than just the middle part. \n",
    "- P1: If not full cycle, then keep the ability to generate NLP from both local files as well as from EDGAR. Couple of function may require little tweaking. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of NLP_Financials_Watson.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1cjKxpZEdGZs6mrA1YgsHfvlwuBwqNcp8",
     "timestamp": 1553284494143
    },
    {
     "file_id": "15ldwpg8GAFQvyfBI5oyrfHNDn35ogFdK",
     "timestamp": 1553267652188
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
