{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Small LLM (<10 B parameters) local notebook execution without any fine tuning\n",
    "Author: **Peeyush Sharma**; Feedback: **PSharma3@gmail.com**\n"
   ],
   "id": "1820418bd2aea7c7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-01T14:39:57.314737Z",
     "start_time": "2025-05-01T14:39:57.311358Z"
    }
   },
   "source": [
    "# !pip install datasets==3.4.1\n",
    "# !pip install py7zr\n",
    "# !pip install transformers==4.49.0\n",
    "# !pip3 install torch torchvision\n",
    "# !pip install accelerate==1.5.2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:44:28.868584Z",
     "start_time": "2025-05-01T14:44:23.202364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_NAME = \"../../../../../PycharmProjects/llms/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# 1) Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# 2) Load model onto MPS device or CPU\n",
    "# device_map=\"auto\" tries MPS if available. If you prefer CPU only, set device_map={\"\": \"cpu\"}.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,   # Or \"auto\"\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ],
   "id": "32f5aba54ab2502a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.77s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T14:48:55.124790Z",
     "start_time": "2025-05-01T14:48:55.070443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3) Generate text\n",
    "prompt = \"Is there a free hosted LLM I can point to with python code and use in my local notebooks?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
   ],
   "id": "2d601cbdc19435b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-01T14:48:59.086330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    output_tokens = model.generate(**inputs, max_new_tokens=5000)\n",
    "print(tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ],
   "id": "6d201869f847d307",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/Users/peeyushsharma/opt/anaconda3/envs/llm_venv/lib/python3.9/site-packages/transformers/pytorch_utils.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T22:54:54.758621Z",
     "start_time": "2025-03-20T22:54:54.715219Z"
    }
   },
   "cell_type": "code",
   "source": "print(tokenizer.decode(output_tokens[0], skip_special_tokens=True))",
   "id": "c3d7cb01f4fadda",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a free hosted LLM I can point to with python code and use in my local notebooks? I've been using the Google Cloud LLM and it's been great, but I'd like to try out some other options.\n",
      "\n",
      "There are several free and open-source large language models (LLMs) that you can use in your local notebooks without hosting them yourself. Here are a few options:\n",
      "\n",
      "1. Hugging Face Transformers: Hugging Face provides a popular open-source library for working with pre-trained LLMs. You can install it using pip and then load the models locally in your notebook. They offer a wide range of models, including BERT, RoBERTa, DistilBERT, and many more. Here's an example of how to use it:\n",
      "\n",
      "```python\n",
      "!pip install transformers\n",
      "\n",
      "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
      "\n",
      "text = \"The cat in the hat sat on the mat.\"\n",
      "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
      "output = model(input_ids)[0]\n",
      "\n",
      "predicted_index = output.argmax(axis=-1)\n",
      "predicted_token = tokenizer.decode(predicted_index[0])\n",
      "print(f\"Predicted token: {predicted_token}\")\n",
      "```\n",
      "\n",
      "2. TensorFlow Text: TensorFlow Text is a library for working with text data in TensorFlow. It includes pre-trained models for various NLP tasks, including masked language modeling. Here's an example of how to use it:\n",
      "\n",
      "```python\n",
      "!pip install tensorflow-text\n",
      "\n",
      "import tensorflow as tf\n",
      "import tensorflow_text\n",
      "\n",
      "text = \"The cat in the hat sat on the mat.\"\n",
      "tokenizer = tf_text.experimental.load_vocab(\"path/to/vocab/file\")\n",
      "model = tf_text.experimental.BertModel.from_pretrained(\"path/to/bert/model\")\n",
      "\n",
      "encoded_text = tf_text.experimental.convert_to_tensor([text], session=tf.Session())\n",
      "input_ids = tokenizer(encoded_text)[0]\n",
      "output = model(input_ids)[0]\n",
      "\n",
      "predicted_index = tf.argmax(output, axis=-1).numpy()\n",
      "predicted_token = tokenizer.tokens[predicted_index[0]]\n",
      "print(f\"Predicted token: {predicted_token}\")\n",
      "```\n",
      "\n",
      "3. spaCy: spaCy is a popular open-source NLP library that includes pre-trained models for various tasks, including masked language modeling. However, it's worth noting that spaCy's masked language modeling models are not as large or powerful as some of the other options mentioned here. Here's an example of how to use it:\n",
      "\n",
      "```python\n",
      "!pip install spacy\n",
      "\n",
      "import spacy\n",
      "\n",
      "nlp = spacy.load(\"en_core_web_md\")\n",
      "\n",
      "text = \"The cat in the hat sat on the mat.\"\n",
      "doc = nlp(text)\n",
      "\n",
      "predicted_token = doc[-1].head.text\n",
      "print(f\"Predicted token: {predicted_token}\")\n",
      "```\n",
      "\n",
      "I hope this helps! Let me know if you have any questions.\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a8735f05a7708563"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
