{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Predictive Modeling w/ PyTorch\n",
    "This notebook is a work-in-progress.\n",
    "\n",
    "- Code to predict stock market movements with large scale technical indicator data used in PyTorch layers.\n",
    "- Model choice will likely be LSTM but TBD.\n",
    "- The original data is 'close' pricing and daily 'volume'. These are further extended with many technical indicators.\n",
    "- The technical indicator data is then pulled for stocks that have close correlations in return with a 'given' input ticker.\n",
    "- Data for several of these tickers (and their corrs) is then concatenated to construct training dataset.\n",
    "- In-progress: Train and test in PyTorch NN layers"
   ],
   "id": "ae9f3bb52ee7df1f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-14T15:21:30.279869Z",
     "start_time": "2025-05-14T15:21:13.435929Z"
    }
   },
   "source": [
    "import os\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "from typing import Union, Any\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from numpy.array_api import arange\n",
    "from pandas.errors import EmptyDataError\n",
    "from sqlalchemy import create_engine, text\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "pd.options.mode.chained_assignment = None"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3v/2n8y1tvs3jl2w00mdxjkf7pm0000gn/T/ipykernel_75422/503567439.py:9: UserWarning: The numpy.array_api submodule is still experimental. See NEP 47.\n",
      "  from numpy.array_api import arange\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:21:30.286652Z",
     "start_time": "2025-05-14T15:21:30.284396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BASE_DIR = '../../../../workspace/HelloPython/HistoricalMarketData/TechnicalIndicators'\n",
    "TABLE_EQUITIES_DATA = 'equities_historic_data'\n",
    "DURATIONS = (14, 30, 90, 200)  # Roughly for bi-weekly, monthly, quarterly, and 200 days running averages"
   ],
   "id": "5c7d0bc06cd8e093",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:21:30.663528Z",
     "start_time": "2025-05-14T15:21:30.424494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    DB = os.environ[\"DB\"]\n",
    "    DB_USER = os.environ[\"DB_USER\"]\n",
    "    DB_PWD = os.environ[\"DB_PWD\"]\n",
    "except KeyError:\n",
    "    raise Exception(\"Required environment variables DB_USER and DB_PWD not set\")\n",
    "DB_URL = 'mysql+mysqlconnector://' + DB_USER + ':' + DB_PWD + '@localhost/' + DB\n",
    "ENGINE = create_engine(DB_URL)"
   ],
   "id": "a121183d99b5b5fc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:21:30.676725Z",
     "start_time": "2025-05-14T15:21:30.671494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_file_path(symbol, date=None):\n",
    "    \"\"\"\n",
    "    Generates a file path for a given symbol\n",
    "    to retrieve calculated tech indicator data\n",
    "    from local CSV records\n",
    "    :param symbol: ticker\n",
    "    :param date: date embedded in the file name\n",
    "    :return: file name and path\n",
    "    \"\"\"\n",
    "    if date is not None:\n",
    "        str_date = datetime.strftime(date, '%Y%m%d')\n",
    "        file_name = symbol.lower() + '_' + str_date + '.csv'\n",
    "    else:\n",
    "        file_name = symbol.lower() + '.csv'\n",
    "    file_path = os.path.join(BASE_DIR, file_name)\n",
    "    if file_path is None:\n",
    "        print('Could not find file for symbol:{}'.format(symbol))\n",
    "    # print(file_path)\n",
    "    return file_path, file_name"
   ],
   "id": "3a8381cef052068f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:21:30.793173Z",
     "start_time": "2025-05-14T15:21:30.684091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_technical_indicator_data_for_symbol(symbol, columns = None, feature=None)\\\n",
    "        -> Any:\n",
    "    \"\"\"\n",
    "    Retrieves Technical Indicator Data for a given symbol\n",
    "    for given columns and labels them as Y (dependent) or\n",
    "    X (input features).\n",
    "    :param symbol: Symbol for which to retrieve Technical Indicator Data\n",
    "    :param columns: Subset of Technical Indicator Data to retrieve\n",
    "    :param feature: Y or X_i label where 'i' is feature number\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    file_path, _ = generate_file_path(symbol)\n",
    "    if file_path is not None:\n",
    "        try:\n",
    "            dfrm = pd.read_csv(file_path)\n",
    "            dfrm['date'] = pd.to_datetime(dfrm['date'])\n",
    "            dfrm.set_index('date', inplace=True)\n",
    "            dfrm.sort_index(ascending=True)\n",
    "            dfrm.index.name = 'date'\n",
    "            if feature is None:\n",
    "                feature = symbol.upper()\n",
    "            if columns is not None:\n",
    "                dfrm = dfrm[columns]\n",
    "                dfrm.columns = [\n",
    "                feature.upper()+'_'+column\n",
    "                for column in dfrm.columns\n",
    "                if column in columns\n",
    "                ]\n",
    "            else:\n",
    "                dfrm.columns = [\n",
    "                feature.upper()+'_'+column\n",
    "                for column in dfrm.columns\n",
    "                ]\n",
    "            return dfrm\n",
    "        except FileNotFoundError as e:\n",
    "            print('Exception reading input data for symbol {}. Generating metadata starting from baseline date.'.format(symbol.upper()))\n",
    "            return None\n",
    "        except EmptyDataError as e:\n",
    "            print(f'No technical indicators found for {symbol.upper()}. Generating metadata starting from baseline date.')\n",
    "            print('Generating all records.')\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"No technical indicators data in DB for '{symbol}'.\")\n",
    "    return None\n",
    "\n",
    "tmp_df = retrieve_technical_indicator_data_for_symbol('AAPL', ['volume', 'close'])\n",
    "tmp_df.tail(10)"
   ],
   "id": "7b7abfb3f9ed274",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            AAPL_volume  AAPL_close\n",
       "date                               \n",
       "2025-04-22     52976371      199.74\n",
       "2025-04-23     52929165      204.60\n",
       "2025-04-24     47310989      208.37\n",
       "2025-04-25     38222258      209.28\n",
       "2025-04-28     37626816      210.14\n",
       "2025-04-29     36827633      211.21\n",
       "2025-04-30     52286454      212.50\n",
       "2025-05-01     57365675      213.32\n",
       "2025-05-02    101010621      205.35\n",
       "2025-05-05     69018452      198.89"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL_volume</th>\n",
       "      <th>AAPL_close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-04-22</th>\n",
       "      <td>52976371</td>\n",
       "      <td>199.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-23</th>\n",
       "      <td>52929165</td>\n",
       "      <td>204.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-24</th>\n",
       "      <td>47310989</td>\n",
       "      <td>208.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-25</th>\n",
       "      <td>38222258</td>\n",
       "      <td>209.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-28</th>\n",
       "      <td>37626816</td>\n",
       "      <td>210.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-29</th>\n",
       "      <td>36827633</td>\n",
       "      <td>211.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-30</th>\n",
       "      <td>52286454</td>\n",
       "      <td>212.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-01</th>\n",
       "      <td>57365675</td>\n",
       "      <td>213.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-02</th>\n",
       "      <td>101010621</td>\n",
       "      <td>205.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-05</th>\n",
       "      <td>69018452</td>\n",
       "      <td>198.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:21:30.812598Z",
     "start_time": "2025-05-14T15:21:30.807252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_symbols_w_highest_correlations(symbol, correlations, count):\n",
    "    \"\"\"\n",
    "    Finds tickers whose return have highest correlation\n",
    "    with the returns of given symbol\n",
    "    :param symbol:\n",
    "    :param correlations:\n",
    "    :param count:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if len(correlations[correlations['level_0'] == symbol.upper()+'_close']) <= 0:\n",
    "        print(\"No correlations found for symbol:{}.\".format(symbol.upper()))\n",
    "        print(\"Likely data does not go far enough back.\")\n",
    "        return None\n",
    "    corrs = correlations[correlations['level_0'] == symbol.upper()+'_close']\n",
    "    corrs.sort_values(0, ascending = False, inplace = True)\n",
    "    return_symbols = [symbol.replace('_close', '') for symbol in corrs.loc[:,'level_1'].tolist()]\n",
    "    return return_symbols[0:count]"
   ],
   "id": "3cd1509498de9123",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:21:30.849093Z",
     "start_time": "2025-05-14T15:21:30.845192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gen_corrs(dfrm):\n",
    "    \"\"\"\n",
    "    Generate correlations numbers for entire\n",
    "    input dataframe consisting of daily close values\n",
    "    :param dfrm:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    correlations = dfrm[dfrm.columns].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\n",
    "    correlations = correlations[correlations['level_0'] != correlations['level_1']]\n",
    "    #print(correlations)`\n",
    "    return correlations"
   ],
   "id": "acc6538fb527c3a6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Dataset\n",
    "Generate training dataset by finding symbols that have the highest correlation (based on daily 'close' values) with the given symbol. Then aggregate their returns over the years. The daily closing price of our target symbol becomes independent variable y and rest of the data including closing values of other symbols and their volumes make up the 'features' or 'X'."
   ],
   "id": "c1d48039c04a18b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:22:02.237040Z",
     "start_time": "2025-05-14T15:21:30.895269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MIN_ROW_COUNT = 5000 # Number of days to retrieve training data for\n",
    "COMP_COUNT = 10 # Number of stocks to find high correlations with\n",
    "\n",
    "query = 'SELECT symbol FROM equities_historic_data GROUP BY symbol HAVING COUNT(*) > '+str(MIN_ROW_COUNT)+''\n",
    "with ENGINE.connect() as conn:\n",
    "    res = conn.execute(text(query))\n",
    "dfrm_symbols_list = pd.DataFrame(res.mappings().all())\n",
    "symbols = dfrm_symbols_list['symbol'].tolist() # this is our universe of symbol tickers\n",
    "\n",
    "# First calculate universal correlations among ALL these symbols\n",
    "dfrm_list_daily_closes = list()\n",
    "for symbol in symbols:\n",
    "    dfrm = retrieve_technical_indicator_data_for_symbol(symbol, ['close', 'volume'])\n",
    "    dfrm_list_daily_closes.append(dfrm)\n",
    "merged_dfrm_daily_close_values = reduce(lambda left, right: pd.merge(left, right, on='date'), dfrm_list_daily_closes)\n",
    "correlations = gen_corrs(merged_dfrm_daily_close_values)\n",
    "\n",
    "# Now merge a symbol's returns with returns of symbols\n",
    "# it has the highest correlations with. Then stack (concat)\n",
    "# all these returns across symbols to come up with our\n",
    "# training dataset\n",
    "\n",
    "# Data for a symbol and its corrs matches\n",
    "# Think of this as a single line in a matrix\n",
    "dfrm_list_single_row_data = list()\n",
    "# Data for all symbol and their corrs matches\n",
    "# Think of this as a matrix\n",
    "dfrm_list_full_matrix_data = list() # Data for\n",
    "tgt_symbols_list = ['JPM', 'C', 'MS', 'GS', 'WFC', 'BAC'] # Generate training data for this set only\n",
    "for symbol in tgt_symbols_list:\n",
    "    # Our dependent variable Y - generalize the name so we can stack up many symbols for training dataset\n",
    "    dfrm_symbol = retrieve_technical_indicator_data_for_symbol(symbol, None, 'Y')\n",
    "    dfrm_list_single_row_data.append(dfrm_symbol)\n",
    "    symbols_w_highest_corrs = find_symbols_w_highest_correlations(symbol, correlations, COMP_COUNT)\n",
    "    if symbols_w_highest_corrs is None:\n",
    "        continue # Likely not enough data for a symbol. Continue with rest\n",
    "    print(f'Symbols with highest correlations with {symbol.upper()} are {symbols_w_highest_corrs}')\n",
    "    counter = 1\n",
    "    for comp_symbol in symbols_w_highest_corrs:\n",
    "        feature = 'X_'+str(counter)\n",
    "        dfrm_tmp = retrieve_technical_indicator_data_for_symbol(comp_symbol, None, feature)\n",
    "        dfrm_list_single_row_data.append(dfrm_tmp)\n",
    "        counter += 1\n",
    "    # Now merge the symbol and all its corrs data against the same date\n",
    "    merged_df = reduce(lambda left, right: pd.merge(left, right, on='date'), dfrm_list_single_row_data)\n",
    "    dfrm_list_full_matrix_data.append(merged_df)\n",
    "    dfrm_list_single_row_data.clear()\n",
    "# Now concatenate data. This means dates are repeated\n",
    "dfrm_aggr_training_data = pd.concat(dfrm_list_full_matrix_data)"
   ],
   "id": "c97a6d0e9ccbe76d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbols with highest correlations with JPM are ['TXN', 'PNC', 'MAR', 'SPGI', 'SCHW', 'SIVB', 'SYK', 'WM', 'RJF', 'SNPS']\n",
      "Symbols with highest correlations with C are ['AIG', 'RF', 'KEY', 'ZION', 'HIG', 'HBAN', 'ARNC', 'FITB', 'KIM', 'XRX']\n",
      "Symbols with highest correlations with MS are ['HIG', 'DRE', 'ZION', 'KEY', 'FITB', 'LNC', 'MGM', 'RF', 'XRX', 'MET']\n",
      "Symbols with highest correlations with GS are ['BK', 'STT', 'PRU', 'NTRS', 'CMA', 'CME', 'LNC', 'CBRE', 'SCHW', 'PFG']\n",
      "Symbols with highest correlations with WFC are ['USB', 'DIS', 'PPG', 'UPS', 'SNA', 'IPG', 'OMC', 'MDLZ', 'AMGN', 'WHR']\n",
      "No correlations found for symbol:BAC.\n",
      "Likely data does not go far enough back.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:22:02.257026Z",
     "start_time": "2025-05-14T15:22:02.241232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dfrm_single_symbol_training_data = dfrm_list_full_matrix_data[4]\n",
    "dfrm_single_symbol_training_data"
   ],
   "id": "7021ec213aec1847",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           Y_symbol  Y_close  Y_volume  Y_mean_200  Y_stddev_200  \\\n",
       "date                                                               \n",
       "2002-02-20      WFC   22.970   8890000   21.957782      1.087244   \n",
       "2002-02-21      WFC   22.875   8520000   21.964627      1.086044   \n",
       "2002-02-22      WFC   23.030   7470000   21.972519      1.085862   \n",
       "2002-02-25      WFC   23.300   6900000   21.959586      1.089206   \n",
       "2002-02-26      WFC   23.315   7350000   21.963571      1.093211   \n",
       "...             ...      ...       ...         ...           ...   \n",
       "2025-04-29      WFC   71.100  24563554   71.636912      5.238784   \n",
       "2025-04-30      WFC   71.010  15882226   71.710588      5.157773   \n",
       "2025-05-01      WFC   71.810  19672472   71.711314      5.138782   \n",
       "2025-05-02      WFC   73.800  18347422   71.726449      5.123080   \n",
       "2025-05-05      WFC   73.850  16070390   71.933971      4.993174   \n",
       "\n",
       "            Y_pcntleStdDevs_200  Y_pcntleVolume_200  Y_pcntleClosing_200  \\\n",
       "date                                                                       \n",
       "2002-02-20            48.120301           53.759398            75.939850   \n",
       "2002-02-21            46.268657           47.761194            73.880597   \n",
       "2002-02-22            45.925926           28.888889            78.518519   \n",
       "2002-02-25            50.375940           21.052632            87.593985   \n",
       "2002-02-26            52.631579           27.067669            87.969925   \n",
       "...                         ...                 ...                  ...   \n",
       "2025-04-29                  NaN           93.382353            42.647059   \n",
       "2025-04-30                  NaN           84.558824            41.176471   \n",
       "2025-05-01                  NaN           90.510949            51.094891   \n",
       "2025-05-02                  NaN           88.405797            66.666667   \n",
       "2025-05-05                  NaN           83.823529            66.176471   \n",
       "\n",
       "            Y_oscillator_200  Y_accu_dist_200  ...  X_10_stddev_90  \\\n",
       "date                                           ...                   \n",
       "2002-02-20         79.662921     5.616507e+06  ...        2.900307   \n",
       "2002-02-21         77.528090     5.208397e+06  ...        2.905524   \n",
       "2002-02-22         81.011236     5.858275e+06  ...        2.895570   \n",
       "2002-02-25         87.078652     7.053475e+06  ...        2.886939   \n",
       "2002-02-26         87.415730     7.121648e+06  ...        2.897748   \n",
       "...                      ...              ...  ...             ...   \n",
       "2025-04-29         49.510763    -8.045819e+04  ...       10.567088   \n",
       "2025-04-30         49.070450    -1.539589e+05  ...        9.680139   \n",
       "2025-05-01         52.984344     4.992517e+05  ...        9.674165   \n",
       "2025-05-02         62.720157     2.146362e+06  ...        9.675321   \n",
       "2025-05-05         62.964775     2.207723e+06  ...        9.781888   \n",
       "\n",
       "            X_10_accu_dist_200  X_10_bollingerLower_200  \\\n",
       "date                                                      \n",
       "2002-02-20       192787.319606                54.530495   \n",
       "2002-02-21       222302.823790                54.577250   \n",
       "2002-02-22       199716.880497                54.621371   \n",
       "2002-02-25       340260.394450                54.521475   \n",
       "2002-02-26       667194.117647                54.494894   \n",
       "...                        ...                      ...   \n",
       "2025-04-29      -598296.095588                75.759582   \n",
       "2025-04-30      -603249.014706                75.143738   \n",
       "2025-05-01      -608610.401460                74.635283   \n",
       "2025-05-02      -567017.191027                74.216127   \n",
       "2025-05-05      -582884.426713                73.447350   \n",
       "\n",
       "            X_10_bollingerUpper_200  X_10_mean_200  X_10_oscillator_200  \\\n",
       "date                                                                      \n",
       "2002-02-20                76.725144      65.627820            63.616558   \n",
       "2002-02-21                76.689914      65.633582            65.708061   \n",
       "2002-02-22                76.651814      65.636593            64.139434   \n",
       "2002-02-25                76.671306      65.596391            73.769063   \n",
       "2002-02-26                76.809166      65.652030            96.209150   \n",
       "...                             ...            ...                  ...   \n",
       "2025-04-29               133.181007     104.470294             0.000000   \n",
       "2025-04-30               133.372880     104.258309             0.000000   \n",
       "2025-05-01               133.464571     104.049927             0.000000   \n",
       "2025-05-02               133.504308     103.860217             3.761100   \n",
       "2025-05-05               133.871327     103.659338             2.455163   \n",
       "\n",
       "            X_10_pcntleClosing_200  X_10_pcntleStdDevs_200  \\\n",
       "date                                                         \n",
       "2002-02-20               43.609023               47.368421   \n",
       "2002-02-21               47.761194               46.268657   \n",
       "2002-02-22               45.185185               45.925926   \n",
       "2002-02-25               64.661654               48.872180   \n",
       "2002-02-26               97.744361               54.135338   \n",
       "...                            ...                     ...   \n",
       "2025-04-29                0.735294              100.000000   \n",
       "2025-04-30                0.735294              100.000000   \n",
       "2025-05-01                0.729927                     NaN   \n",
       "2025-05-02                7.971014                     NaN   \n",
       "2025-05-05                4.411765                     NaN   \n",
       "\n",
       "            X_10_pcntleVolume_200  X_10_stddev_200  \n",
       "date                                                \n",
       "2002-02-20              51.127820         5.548662  \n",
       "2002-02-21              67.910448         5.528166  \n",
       "2002-02-22              46.666667         5.507611  \n",
       "2002-02-25              92.857143         5.537458  \n",
       "2002-02-26              97.744361         5.578568  \n",
       "...                           ...              ...  \n",
       "2025-04-29              91.176471        14.355356  \n",
       "2025-04-30              90.441176        14.557286  \n",
       "2025-05-01              88.321168        14.707322  \n",
       "2025-05-02              87.681159        14.822045  \n",
       "2025-05-05              77.205882        15.105994  \n",
       "\n",
       "[5823 rows x 429 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y_symbol</th>\n",
       "      <th>Y_close</th>\n",
       "      <th>Y_volume</th>\n",
       "      <th>Y_mean_200</th>\n",
       "      <th>Y_stddev_200</th>\n",
       "      <th>Y_pcntleStdDevs_200</th>\n",
       "      <th>Y_pcntleVolume_200</th>\n",
       "      <th>Y_pcntleClosing_200</th>\n",
       "      <th>Y_oscillator_200</th>\n",
       "      <th>Y_accu_dist_200</th>\n",
       "      <th>...</th>\n",
       "      <th>X_10_stddev_90</th>\n",
       "      <th>X_10_accu_dist_200</th>\n",
       "      <th>X_10_bollingerLower_200</th>\n",
       "      <th>X_10_bollingerUpper_200</th>\n",
       "      <th>X_10_mean_200</th>\n",
       "      <th>X_10_oscillator_200</th>\n",
       "      <th>X_10_pcntleClosing_200</th>\n",
       "      <th>X_10_pcntleStdDevs_200</th>\n",
       "      <th>X_10_pcntleVolume_200</th>\n",
       "      <th>X_10_stddev_200</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-02-20</th>\n",
       "      <td>WFC</td>\n",
       "      <td>22.970</td>\n",
       "      <td>8890000</td>\n",
       "      <td>21.957782</td>\n",
       "      <td>1.087244</td>\n",
       "      <td>48.120301</td>\n",
       "      <td>53.759398</td>\n",
       "      <td>75.939850</td>\n",
       "      <td>79.662921</td>\n",
       "      <td>5.616507e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.900307</td>\n",
       "      <td>192787.319606</td>\n",
       "      <td>54.530495</td>\n",
       "      <td>76.725144</td>\n",
       "      <td>65.627820</td>\n",
       "      <td>63.616558</td>\n",
       "      <td>43.609023</td>\n",
       "      <td>47.368421</td>\n",
       "      <td>51.127820</td>\n",
       "      <td>5.548662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-21</th>\n",
       "      <td>WFC</td>\n",
       "      <td>22.875</td>\n",
       "      <td>8520000</td>\n",
       "      <td>21.964627</td>\n",
       "      <td>1.086044</td>\n",
       "      <td>46.268657</td>\n",
       "      <td>47.761194</td>\n",
       "      <td>73.880597</td>\n",
       "      <td>77.528090</td>\n",
       "      <td>5.208397e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.905524</td>\n",
       "      <td>222302.823790</td>\n",
       "      <td>54.577250</td>\n",
       "      <td>76.689914</td>\n",
       "      <td>65.633582</td>\n",
       "      <td>65.708061</td>\n",
       "      <td>47.761194</td>\n",
       "      <td>46.268657</td>\n",
       "      <td>67.910448</td>\n",
       "      <td>5.528166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-22</th>\n",
       "      <td>WFC</td>\n",
       "      <td>23.030</td>\n",
       "      <td>7470000</td>\n",
       "      <td>21.972519</td>\n",
       "      <td>1.085862</td>\n",
       "      <td>45.925926</td>\n",
       "      <td>28.888889</td>\n",
       "      <td>78.518519</td>\n",
       "      <td>81.011236</td>\n",
       "      <td>5.858275e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.895570</td>\n",
       "      <td>199716.880497</td>\n",
       "      <td>54.621371</td>\n",
       "      <td>76.651814</td>\n",
       "      <td>65.636593</td>\n",
       "      <td>64.139434</td>\n",
       "      <td>45.185185</td>\n",
       "      <td>45.925926</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>5.507611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-25</th>\n",
       "      <td>WFC</td>\n",
       "      <td>23.300</td>\n",
       "      <td>6900000</td>\n",
       "      <td>21.959586</td>\n",
       "      <td>1.089206</td>\n",
       "      <td>50.375940</td>\n",
       "      <td>21.052632</td>\n",
       "      <td>87.593985</td>\n",
       "      <td>87.078652</td>\n",
       "      <td>7.053475e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.886939</td>\n",
       "      <td>340260.394450</td>\n",
       "      <td>54.521475</td>\n",
       "      <td>76.671306</td>\n",
       "      <td>65.596391</td>\n",
       "      <td>73.769063</td>\n",
       "      <td>64.661654</td>\n",
       "      <td>48.872180</td>\n",
       "      <td>92.857143</td>\n",
       "      <td>5.537458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-26</th>\n",
       "      <td>WFC</td>\n",
       "      <td>23.315</td>\n",
       "      <td>7350000</td>\n",
       "      <td>21.963571</td>\n",
       "      <td>1.093211</td>\n",
       "      <td>52.631579</td>\n",
       "      <td>27.067669</td>\n",
       "      <td>87.969925</td>\n",
       "      <td>87.415730</td>\n",
       "      <td>7.121648e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.897748</td>\n",
       "      <td>667194.117647</td>\n",
       "      <td>54.494894</td>\n",
       "      <td>76.809166</td>\n",
       "      <td>65.652030</td>\n",
       "      <td>96.209150</td>\n",
       "      <td>97.744361</td>\n",
       "      <td>54.135338</td>\n",
       "      <td>97.744361</td>\n",
       "      <td>5.578568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-29</th>\n",
       "      <td>WFC</td>\n",
       "      <td>71.100</td>\n",
       "      <td>24563554</td>\n",
       "      <td>71.636912</td>\n",
       "      <td>5.238784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.382353</td>\n",
       "      <td>42.647059</td>\n",
       "      <td>49.510763</td>\n",
       "      <td>-8.045819e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>10.567088</td>\n",
       "      <td>-598296.095588</td>\n",
       "      <td>75.759582</td>\n",
       "      <td>133.181007</td>\n",
       "      <td>104.470294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>91.176471</td>\n",
       "      <td>14.355356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-30</th>\n",
       "      <td>WFC</td>\n",
       "      <td>71.010</td>\n",
       "      <td>15882226</td>\n",
       "      <td>71.710588</td>\n",
       "      <td>5.157773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.558824</td>\n",
       "      <td>41.176471</td>\n",
       "      <td>49.070450</td>\n",
       "      <td>-1.539589e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>9.680139</td>\n",
       "      <td>-603249.014706</td>\n",
       "      <td>75.143738</td>\n",
       "      <td>133.372880</td>\n",
       "      <td>104.258309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>90.441176</td>\n",
       "      <td>14.557286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-01</th>\n",
       "      <td>WFC</td>\n",
       "      <td>71.810</td>\n",
       "      <td>19672472</td>\n",
       "      <td>71.711314</td>\n",
       "      <td>5.138782</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.510949</td>\n",
       "      <td>51.094891</td>\n",
       "      <td>52.984344</td>\n",
       "      <td>4.992517e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>9.674165</td>\n",
       "      <td>-608610.401460</td>\n",
       "      <td>74.635283</td>\n",
       "      <td>133.464571</td>\n",
       "      <td>104.049927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.729927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.321168</td>\n",
       "      <td>14.707322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-02</th>\n",
       "      <td>WFC</td>\n",
       "      <td>73.800</td>\n",
       "      <td>18347422</td>\n",
       "      <td>71.726449</td>\n",
       "      <td>5.123080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.405797</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>62.720157</td>\n",
       "      <td>2.146362e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>9.675321</td>\n",
       "      <td>-567017.191027</td>\n",
       "      <td>74.216127</td>\n",
       "      <td>133.504308</td>\n",
       "      <td>103.860217</td>\n",
       "      <td>3.761100</td>\n",
       "      <td>7.971014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.681159</td>\n",
       "      <td>14.822045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-05</th>\n",
       "      <td>WFC</td>\n",
       "      <td>73.850</td>\n",
       "      <td>16070390</td>\n",
       "      <td>71.933971</td>\n",
       "      <td>4.993174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.823529</td>\n",
       "      <td>66.176471</td>\n",
       "      <td>62.964775</td>\n",
       "      <td>2.207723e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>9.781888</td>\n",
       "      <td>-582884.426713</td>\n",
       "      <td>73.447350</td>\n",
       "      <td>133.871327</td>\n",
       "      <td>103.659338</td>\n",
       "      <td>2.455163</td>\n",
       "      <td>4.411765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.205882</td>\n",
       "      <td>15.105994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5823 rows × 429 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Normalization and Train-Test split",
   "id": "eb17ea23b2359d4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:22:02.387056Z",
     "start_time": "2025-05-14T15:22:02.381149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Start with separating independent vector matrix (X) and dependent vector (y)\n",
    "# Take care of X first\n",
    "batch_size = 5\n",
    "len_data = len(dfrm_single_symbol_training_data)\n",
    "tgt_field_column_name = 'Y_close'\n",
    "tgt_symbol_column_name = 'Y_symbol'\n",
    "train_to_total_ratio = 0.8\n",
    "test_to_total_ratio = 1- train_to_total_ratio\n",
    "\n",
    "# The train and test dataset must be perfect multiple of batch size\n",
    "# Try to find some API to simplify this.\n",
    "offset_test_data_start = len_data - int(test_to_total_ratio * len_data)\n",
    "offset_test_data_start = offset_test_data_start - (offset_test_data_start % batch_size)\n",
    "offset_test_date_end = offset_test_data_start + int(test_to_total_ratio * len_data)\n",
    "offset_test_date_end = offset_test_date_end - (offset_test_date_end % batch_size)\n",
    "\n",
    "dfrm_training = dfrm_single_symbol_training_data.iloc[batch_size:offset_test_data_start, :]\n",
    "dfrm_test = dfrm_single_symbol_training_data.iloc[offset_test_data_start:offset_test_date_end, :]\n",
    "print(len(dfrm_training))\n",
    "print(len(dfrm_test))"
   ],
   "id": "441a6687a8ed96fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4650\n",
      "1160\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:22:02.440480Z",
     "start_time": "2025-05-14T15:22:02.400484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# x_columns = [column for column in dfrm_training.columns if column not in [tgt_field_column_name] and 'symbol' not in column]\n",
    "x_columns = [column for column in dfrm_training.columns if column not in [tgt_field_column_name] and 'close' in column]\n",
    "Y_train = dfrm_training[tgt_field_column_name]\n",
    "X_train = dfrm_training[x_columns]\n",
    "# null_indices = X_train[X_train.isnull().any(axis=1)].index\n",
    "# X_train = X_train.drop(null_indices)\n",
    "# Y_train = Y_train.drop(null_indices)\n",
    "\n",
    "Y_test = dfrm_test[tgt_field_column_name]\n",
    "X_test = dfrm_test[x_columns]\n",
    "\n",
    "# Cap the numeric values within (-1, +1) range\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "transformed_Y_train = scaler.fit_transform(Y_train.values.reshape(-1,1))\n",
    "transformed_Y_test = scaler.fit_transform(Y_test.values.reshape(-1,1))\n",
    "\n",
    "transformed_X_train = X_train.copy()\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "for column_name in X_train.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    transformed_X_train[column_name] = scaler.fit_transform(X_train[[column_name]])\n",
    "\n",
    "transformed_X_test = X_test.copy()\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "for column_name in X_test.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    transformed_X_test[column_name] = scaler.fit_transform(X_test[[column_name]])"
   ],
   "id": "eb9d484f9b0f89a3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:22:02.497817Z",
     "start_time": "2025-05-14T15:22:02.478900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train = torch.from_numpy(transformed_X_train.to_numpy()).to(torch.float32)\n",
    "x_test = torch.from_numpy(transformed_X_test.to_numpy()).to(torch.float32)\n",
    "y_train = torch.from_numpy(transformed_Y_train).to(torch.float32)\n",
    "y_test = torch.from_numpy(transformed_Y_test).to(torch.float32)\n",
    "\n",
    "x_train_loader = DataLoader(x_train, batch_size=5, shuffle=False) # Keep shuffle false to preserve the direction\n",
    "y_train_loader = DataLoader(y_train, batch_size=5, shuffle=False) # Keep shuffle false to preserve the direction\n",
    "x_test_loader = DataLoader(x_test, batch_size=5, shuffle=False) # Keep shuffle false to preserve the direction\n",
    "y_test_loader = DataLoader(y_test, batch_size=5, shuffle=False) # Keep shuffle false to preserve the direction"
   ],
   "id": "b1951e01a7ee7ef8",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:22:02.674859Z",
     "start_time": "2025-05-14T15:22:02.669671Z"
    }
   },
   "cell_type": "code",
   "source": "x_train.shape",
   "id": "549f5249e90dd2f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4650, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:22:02.815044Z",
     "start_time": "2025-05-14T15:22:02.811750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_dim = len(X_train.columns) # Number of features\n",
    "#hidden_dim = int(len(X_train)) # Number of samples or units\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "num_epochs = 2 # Intentionally keeping low for initial debugging\n",
    "seq_len = 10"
   ],
   "id": "49caeb4978c6b59a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Definition and Predictions",
   "id": "f519e09993e3c4fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:22:02.964249Z",
     "start_time": "2025-05-14T15:22:02.959037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure x has batch dimension\n",
    "        #print(\"Input size:\", x.size(0))\n",
    "        # print(f'X:{x}')\n",
    "        x = self.dropout(x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "id": "df82a418e1a54b34",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:22:04.395370Z",
     "start_time": "2025-05-14T15:22:03.046105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, num_layers= num_layers, output_dim=output_dim, )\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "model.lstm"
   ],
   "id": "7be06430e33359e3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(10, 512, num_layers=2, batch_first=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:22:40.455275Z",
     "start_time": "2025-05-14T15:22:04.405738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hist = np.zeros(num_epochs)\n",
    "start_time = time.time()\n",
    "lstm = []\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    counter = 1\n",
    "    for batch in x_train_loader:\n",
    "        batch = batch.reshape(5, 1, input_dim)\n",
    "        y_train_pred = model(batch)\n",
    "        y_train_observed = next(iter(y_train_loader))\n",
    "        loss = criterion(y_train_pred, y_train_observed)\n",
    "        print(f'Epoch:{t:}, Batch:{counter}, Loss: {loss.item():.4f}')\n",
    "        # hist[t] = loss.item()\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        counter += 1\n",
    "\n",
    "training_time = time.time()-start_time\n",
    "print(\"Training time: {}\".format(training_time))"
   ],
   "id": "81a24212c9180f90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Batch:1, Loss: 0.8422\n",
      "Epoch:0, Batch:2, Loss: 0.0063\n",
      "Epoch:0, Batch:3, Loss: 5.5775\n",
      "Epoch:0, Batch:4, Loss: 0.0458\n",
      "Epoch:0, Batch:5, Loss: 1.2953\n",
      "Epoch:0, Batch:6, Loss: 1.2147\n",
      "Epoch:0, Batch:7, Loss: 0.5069\n",
      "Epoch:0, Batch:8, Loss: 0.0184\n",
      "Epoch:0, Batch:9, Loss: 0.2593\n",
      "Epoch:0, Batch:10, Loss: 0.3838\n",
      "Epoch:0, Batch:11, Loss: 0.2073\n",
      "Epoch:0, Batch:12, Loss: 0.0117\n",
      "Epoch:0, Batch:13, Loss: 0.1830\n",
      "Epoch:0, Batch:14, Loss: 0.3797\n",
      "Epoch:0, Batch:15, Loss: 0.4052\n",
      "Epoch:0, Batch:16, Loss: 0.2607\n",
      "Epoch:0, Batch:17, Loss: 0.0676\n",
      "Epoch:0, Batch:18, Loss: 0.0047\n",
      "Epoch:0, Batch:19, Loss: 0.0982\n",
      "Epoch:0, Batch:20, Loss: 0.1988\n",
      "Epoch:0, Batch:21, Loss: 0.1428\n",
      "Epoch:0, Batch:22, Loss: 0.0245\n",
      "Epoch:0, Batch:23, Loss: 0.0103\n",
      "Epoch:0, Batch:24, Loss: 0.0538\n",
      "Epoch:0, Batch:25, Loss: 0.0955\n",
      "Epoch:0, Batch:26, Loss: 0.1080\n",
      "Epoch:0, Batch:27, Loss: 0.0884\n",
      "Epoch:0, Batch:28, Loss: 0.0494\n",
      "Epoch:0, Batch:29, Loss: 0.0147\n",
      "Epoch:0, Batch:30, Loss: 0.0016\n",
      "Epoch:0, Batch:31, Loss: 0.0135\n",
      "Epoch:0, Batch:32, Loss: 0.0336\n",
      "Epoch:0, Batch:33, Loss: 0.0417\n",
      "Epoch:0, Batch:34, Loss: 0.0307\n",
      "Epoch:0, Batch:35, Loss: 0.0153\n",
      "Epoch:0, Batch:36, Loss: 0.0041\n",
      "Epoch:0, Batch:37, Loss: 0.0010\n",
      "Epoch:0, Batch:38, Loss: 0.0061\n",
      "Epoch:0, Batch:39, Loss: 0.0144\n",
      "Epoch:0, Batch:40, Loss: 0.0212\n",
      "Epoch:0, Batch:41, Loss: 0.0221\n",
      "Epoch:0, Batch:42, Loss: 0.0143\n",
      "Epoch:0, Batch:43, Loss: 0.0056\n",
      "Epoch:0, Batch:44, Loss: 0.0022\n",
      "Epoch:0, Batch:45, Loss: 0.0014\n",
      "Epoch:0, Batch:46, Loss: 0.0048\n",
      "Epoch:0, Batch:47, Loss: 0.0075\n",
      "Epoch:0, Batch:48, Loss: 0.0075\n",
      "Epoch:0, Batch:49, Loss: 0.0081\n",
      "Epoch:0, Batch:50, Loss: 0.0045\n",
      "Epoch:0, Batch:51, Loss: 0.0013\n",
      "Epoch:0, Batch:52, Loss: 0.0012\n",
      "Epoch:0, Batch:53, Loss: 0.0014\n",
      "Epoch:0, Batch:54, Loss: 0.0040\n",
      "Epoch:0, Batch:55, Loss: 0.0051\n",
      "Epoch:0, Batch:56, Loss: 0.0046\n",
      "Epoch:0, Batch:57, Loss: 0.0041\n",
      "Epoch:0, Batch:58, Loss: 0.0023\n",
      "Epoch:0, Batch:59, Loss: 0.0013\n",
      "Epoch:0, Batch:60, Loss: 0.0008\n",
      "Epoch:0, Batch:61, Loss: 0.0015\n",
      "Epoch:0, Batch:62, Loss: 0.0031\n",
      "Epoch:0, Batch:63, Loss: 0.0022\n",
      "Epoch:0, Batch:64, Loss: 0.0023\n",
      "Epoch:0, Batch:65, Loss: 0.0021\n",
      "Epoch:0, Batch:66, Loss: 0.0028\n",
      "Epoch:0, Batch:67, Loss: 0.0010\n",
      "Epoch:0, Batch:68, Loss: 0.0012\n",
      "Epoch:0, Batch:69, Loss: 0.0017\n",
      "Epoch:0, Batch:70, Loss: 0.0023\n",
      "Epoch:0, Batch:71, Loss: 0.0029\n",
      "Epoch:0, Batch:72, Loss: 0.0017\n",
      "Epoch:0, Batch:73, Loss: 0.0010\n",
      "Epoch:0, Batch:74, Loss: 0.0021\n",
      "Epoch:0, Batch:75, Loss: 0.0021\n",
      "Epoch:0, Batch:76, Loss: 0.0019\n",
      "Epoch:0, Batch:77, Loss: 0.0021\n",
      "Epoch:0, Batch:78, Loss: 0.0020\n",
      "Epoch:0, Batch:79, Loss: 0.0017\n",
      "Epoch:0, Batch:80, Loss: 0.0013\n",
      "Epoch:0, Batch:81, Loss: 0.0008\n",
      "Epoch:0, Batch:82, Loss: 0.0015\n",
      "Epoch:0, Batch:83, Loss: 0.0012\n",
      "Epoch:0, Batch:84, Loss: 0.0016\n",
      "Epoch:0, Batch:85, Loss: 0.0014\n",
      "Epoch:0, Batch:86, Loss: 0.0017\n",
      "Epoch:0, Batch:87, Loss: 0.0020\n",
      "Epoch:0, Batch:88, Loss: 0.0009\n",
      "Epoch:0, Batch:89, Loss: 0.0013\n",
      "Epoch:0, Batch:90, Loss: 0.0018\n",
      "Epoch:0, Batch:91, Loss: 0.0016\n",
      "Epoch:0, Batch:92, Loss: 0.0016\n",
      "Epoch:0, Batch:93, Loss: 0.0014\n",
      "Epoch:0, Batch:94, Loss: 0.0016\n",
      "Epoch:0, Batch:95, Loss: 0.0012\n",
      "Epoch:0, Batch:96, Loss: 0.0013\n",
      "Epoch:0, Batch:97, Loss: 0.0010\n",
      "Epoch:0, Batch:98, Loss: 0.0014\n",
      "Epoch:0, Batch:99, Loss: 0.0012\n",
      "Epoch:0, Batch:100, Loss: 0.0012\n",
      "Epoch:0, Batch:101, Loss: 0.0013\n",
      "Epoch:0, Batch:102, Loss: 0.0014\n",
      "Epoch:0, Batch:103, Loss: 0.0012\n",
      "Epoch:0, Batch:104, Loss: 0.0017\n",
      "Epoch:0, Batch:105, Loss: 0.0017\n",
      "Epoch:0, Batch:106, Loss: 0.0017\n",
      "Epoch:0, Batch:107, Loss: 0.0016\n",
      "Epoch:0, Batch:108, Loss: 0.0008\n",
      "Epoch:0, Batch:109, Loss: 0.0014\n",
      "Epoch:0, Batch:110, Loss: 0.0013\n",
      "Epoch:0, Batch:111, Loss: 0.0013\n",
      "Epoch:0, Batch:112, Loss: 0.0010\n",
      "Epoch:0, Batch:113, Loss: 0.0017\n",
      "Epoch:0, Batch:114, Loss: 0.0013\n",
      "Epoch:0, Batch:115, Loss: 0.0008\n",
      "Epoch:0, Batch:116, Loss: 0.0012\n",
      "Epoch:0, Batch:117, Loss: 0.0012\n",
      "Epoch:0, Batch:118, Loss: 0.0010\n",
      "Epoch:0, Batch:119, Loss: 0.0013\n",
      "Epoch:0, Batch:120, Loss: 0.0017\n",
      "Epoch:0, Batch:121, Loss: 0.0013\n",
      "Epoch:0, Batch:122, Loss: 0.0015\n",
      "Epoch:0, Batch:123, Loss: 0.0015\n",
      "Epoch:0, Batch:124, Loss: 0.0013\n",
      "Epoch:0, Batch:125, Loss: 0.0012\n",
      "Epoch:0, Batch:126, Loss: 0.0015\n",
      "Epoch:0, Batch:127, Loss: 0.0014\n",
      "Epoch:0, Batch:128, Loss: 0.0010\n",
      "Epoch:0, Batch:129, Loss: 0.0012\n",
      "Epoch:0, Batch:130, Loss: 0.0016\n",
      "Epoch:0, Batch:131, Loss: 0.0013\n",
      "Epoch:0, Batch:132, Loss: 0.0011\n",
      "Epoch:0, Batch:133, Loss: 0.0010\n",
      "Epoch:0, Batch:134, Loss: 0.0009\n",
      "Epoch:0, Batch:135, Loss: 0.0008\n",
      "Epoch:0, Batch:136, Loss: 0.0009\n",
      "Epoch:0, Batch:137, Loss: 0.0013\n",
      "Epoch:0, Batch:138, Loss: 0.0014\n",
      "Epoch:0, Batch:139, Loss: 0.0013\n",
      "Epoch:0, Batch:140, Loss: 0.0014\n",
      "Epoch:0, Batch:141, Loss: 0.0017\n",
      "Epoch:0, Batch:142, Loss: 0.0013\n",
      "Epoch:0, Batch:143, Loss: 0.0014\n",
      "Epoch:0, Batch:144, Loss: 0.0012\n",
      "Epoch:0, Batch:145, Loss: 0.0018\n",
      "Epoch:0, Batch:146, Loss: 0.0014\n",
      "Epoch:0, Batch:147, Loss: 0.0011\n",
      "Epoch:0, Batch:148, Loss: 0.0011\n",
      "Epoch:0, Batch:149, Loss: 0.0010\n",
      "Epoch:0, Batch:150, Loss: 0.0007\n",
      "Epoch:0, Batch:151, Loss: 0.0014\n",
      "Epoch:0, Batch:152, Loss: 0.0010\n",
      "Epoch:0, Batch:153, Loss: 0.0013\n",
      "Epoch:0, Batch:154, Loss: 0.0012\n",
      "Epoch:0, Batch:155, Loss: 0.0011\n",
      "Epoch:0, Batch:156, Loss: 0.0016\n",
      "Epoch:0, Batch:157, Loss: 0.0013\n",
      "Epoch:0, Batch:158, Loss: 0.0010\n",
      "Epoch:0, Batch:159, Loss: 0.0010\n",
      "Epoch:0, Batch:160, Loss: 0.0012\n",
      "Epoch:0, Batch:161, Loss: 0.0022\n",
      "Epoch:0, Batch:162, Loss: 0.0013\n",
      "Epoch:0, Batch:163, Loss: 0.0016\n",
      "Epoch:0, Batch:164, Loss: 0.0014\n",
      "Epoch:0, Batch:165, Loss: 0.0010\n",
      "Epoch:0, Batch:166, Loss: 0.0012\n",
      "Epoch:0, Batch:167, Loss: 0.0012\n",
      "Epoch:0, Batch:168, Loss: 0.0013\n",
      "Epoch:0, Batch:169, Loss: 0.0014\n",
      "Epoch:0, Batch:170, Loss: 0.0014\n",
      "Epoch:0, Batch:171, Loss: 0.0011\n",
      "Epoch:0, Batch:172, Loss: 0.0013\n",
      "Epoch:0, Batch:173, Loss: 0.0011\n",
      "Epoch:0, Batch:174, Loss: 0.0015\n",
      "Epoch:0, Batch:175, Loss: 0.0017\n",
      "Epoch:0, Batch:176, Loss: 0.0014\n",
      "Epoch:0, Batch:177, Loss: 0.0014\n",
      "Epoch:0, Batch:178, Loss: 0.0014\n",
      "Epoch:0, Batch:179, Loss: 0.0018\n",
      "Epoch:0, Batch:180, Loss: 0.0012\n",
      "Epoch:0, Batch:181, Loss: 0.0014\n",
      "Epoch:0, Batch:182, Loss: 0.0013\n",
      "Epoch:0, Batch:183, Loss: 0.0014\n",
      "Epoch:0, Batch:184, Loss: 0.0014\n",
      "Epoch:0, Batch:185, Loss: 0.0016\n",
      "Epoch:0, Batch:186, Loss: 0.0012\n",
      "Epoch:0, Batch:187, Loss: 0.0015\n",
      "Epoch:0, Batch:188, Loss: 0.0010\n",
      "Epoch:0, Batch:189, Loss: 0.0009\n",
      "Epoch:0, Batch:190, Loss: 0.0010\n",
      "Epoch:0, Batch:191, Loss: 0.0011\n",
      "Epoch:0, Batch:192, Loss: 0.0019\n",
      "Epoch:0, Batch:193, Loss: 0.0011\n",
      "Epoch:0, Batch:194, Loss: 0.0009\n",
      "Epoch:0, Batch:195, Loss: 0.0015\n",
      "Epoch:0, Batch:196, Loss: 0.0013\n",
      "Epoch:0, Batch:197, Loss: 0.0014\n",
      "Epoch:0, Batch:198, Loss: 0.0014\n",
      "Epoch:0, Batch:199, Loss: 0.0013\n",
      "Epoch:0, Batch:200, Loss: 0.0009\n",
      "Epoch:0, Batch:201, Loss: 0.0009\n",
      "Epoch:0, Batch:202, Loss: 0.0033\n",
      "Epoch:0, Batch:203, Loss: 0.0012\n",
      "Epoch:0, Batch:204, Loss: 0.0011\n",
      "Epoch:0, Batch:205, Loss: 0.0015\n",
      "Epoch:0, Batch:206, Loss: 0.0013\n",
      "Epoch:0, Batch:207, Loss: 0.0011\n",
      "Epoch:0, Batch:208, Loss: 0.0010\n",
      "Epoch:0, Batch:209, Loss: 0.0017\n",
      "Epoch:0, Batch:210, Loss: 0.0019\n",
      "Epoch:0, Batch:211, Loss: 0.0016\n",
      "Epoch:0, Batch:212, Loss: 0.0011\n",
      "Epoch:0, Batch:213, Loss: 0.0014\n",
      "Epoch:0, Batch:214, Loss: 0.0007\n",
      "Epoch:0, Batch:215, Loss: 0.0014\n",
      "Epoch:0, Batch:216, Loss: 0.0012\n",
      "Epoch:0, Batch:217, Loss: 0.0011\n",
      "Epoch:0, Batch:218, Loss: 0.0015\n",
      "Epoch:0, Batch:219, Loss: 0.0018\n",
      "Epoch:0, Batch:220, Loss: 0.0016\n",
      "Epoch:0, Batch:221, Loss: 0.0009\n",
      "Epoch:0, Batch:222, Loss: 0.0011\n",
      "Epoch:0, Batch:223, Loss: 0.0013\n",
      "Epoch:0, Batch:224, Loss: 0.0007\n",
      "Epoch:0, Batch:225, Loss: 0.0016\n",
      "Epoch:0, Batch:226, Loss: 0.0012\n",
      "Epoch:0, Batch:227, Loss: 0.0011\n",
      "Epoch:0, Batch:228, Loss: 0.0012\n",
      "Epoch:0, Batch:229, Loss: 0.0017\n",
      "Epoch:0, Batch:230, Loss: 0.0011\n",
      "Epoch:0, Batch:231, Loss: 0.0008\n",
      "Epoch:0, Batch:232, Loss: 0.0013\n",
      "Epoch:0, Batch:233, Loss: 0.0011\n",
      "Epoch:0, Batch:234, Loss: 0.0014\n",
      "Epoch:0, Batch:235, Loss: 0.0011\n",
      "Epoch:0, Batch:236, Loss: 0.0018\n",
      "Epoch:0, Batch:237, Loss: 0.0015\n",
      "Epoch:0, Batch:238, Loss: 0.0014\n",
      "Epoch:0, Batch:239, Loss: 0.0011\n",
      "Epoch:0, Batch:240, Loss: 0.0024\n",
      "Epoch:0, Batch:241, Loss: 0.0018\n",
      "Epoch:0, Batch:242, Loss: 0.0012\n",
      "Epoch:0, Batch:243, Loss: 0.0014\n",
      "Epoch:0, Batch:244, Loss: 0.0017\n",
      "Epoch:0, Batch:245, Loss: 0.0016\n",
      "Epoch:0, Batch:246, Loss: 0.0015\n",
      "Epoch:0, Batch:247, Loss: 0.0014\n",
      "Epoch:0, Batch:248, Loss: 0.0013\n",
      "Epoch:0, Batch:249, Loss: 0.0012\n",
      "Epoch:0, Batch:250, Loss: 0.0016\n",
      "Epoch:0, Batch:251, Loss: 0.0012\n",
      "Epoch:0, Batch:252, Loss: 0.0010\n",
      "Epoch:0, Batch:253, Loss: 0.0009\n",
      "Epoch:0, Batch:254, Loss: 0.0013\n",
      "Epoch:0, Batch:255, Loss: 0.0012\n",
      "Epoch:0, Batch:256, Loss: 0.0013\n",
      "Epoch:0, Batch:257, Loss: 0.0018\n",
      "Epoch:0, Batch:258, Loss: 0.0020\n",
      "Epoch:0, Batch:259, Loss: 0.0017\n",
      "Epoch:0, Batch:260, Loss: 0.0013\n",
      "Epoch:0, Batch:261, Loss: 0.0013\n",
      "Epoch:0, Batch:262, Loss: 0.0013\n",
      "Epoch:0, Batch:263, Loss: 0.0016\n",
      "Epoch:0, Batch:264, Loss: 0.0014\n",
      "Epoch:0, Batch:265, Loss: 0.0018\n",
      "Epoch:0, Batch:266, Loss: 0.0020\n",
      "Epoch:0, Batch:267, Loss: 0.0007\n",
      "Epoch:0, Batch:268, Loss: 0.0010\n",
      "Epoch:0, Batch:269, Loss: 0.0015\n",
      "Epoch:0, Batch:270, Loss: 0.0017\n",
      "Epoch:0, Batch:271, Loss: 0.0012\n",
      "Epoch:0, Batch:272, Loss: 0.0013\n",
      "Epoch:0, Batch:273, Loss: 0.0017\n",
      "Epoch:0, Batch:274, Loss: 0.0013\n",
      "Epoch:0, Batch:275, Loss: 0.0013\n",
      "Epoch:0, Batch:276, Loss: 0.0011\n",
      "Epoch:0, Batch:277, Loss: 0.0010\n",
      "Epoch:0, Batch:278, Loss: 0.0014\n",
      "Epoch:0, Batch:279, Loss: 0.0012\n",
      "Epoch:0, Batch:280, Loss: 0.0009\n",
      "Epoch:0, Batch:281, Loss: 0.0018\n",
      "Epoch:0, Batch:282, Loss: 0.0010\n",
      "Epoch:0, Batch:283, Loss: 0.0012\n",
      "Epoch:0, Batch:284, Loss: 0.0013\n",
      "Epoch:0, Batch:285, Loss: 0.0007\n",
      "Epoch:0, Batch:286, Loss: 0.0014\n",
      "Epoch:0, Batch:287, Loss: 0.0012\n",
      "Epoch:0, Batch:288, Loss: 0.0012\n",
      "Epoch:0, Batch:289, Loss: 0.0013\n",
      "Epoch:0, Batch:290, Loss: 0.0013\n",
      "Epoch:0, Batch:291, Loss: 0.0015\n",
      "Epoch:0, Batch:292, Loss: 0.0014\n",
      "Epoch:0, Batch:293, Loss: 0.0011\n",
      "Epoch:0, Batch:294, Loss: 0.0013\n",
      "Epoch:0, Batch:295, Loss: 0.0015\n",
      "Epoch:0, Batch:296, Loss: 0.0013\n",
      "Epoch:0, Batch:297, Loss: 0.0014\n",
      "Epoch:0, Batch:298, Loss: 0.0013\n",
      "Epoch:0, Batch:299, Loss: 0.0010\n",
      "Epoch:0, Batch:300, Loss: 0.0011\n",
      "Epoch:0, Batch:301, Loss: 0.0014\n",
      "Epoch:0, Batch:302, Loss: 0.0020\n",
      "Epoch:0, Batch:303, Loss: 0.0016\n",
      "Epoch:0, Batch:304, Loss: 0.0017\n",
      "Epoch:0, Batch:305, Loss: 0.0013\n",
      "Epoch:0, Batch:306, Loss: 0.0013\n",
      "Epoch:0, Batch:307, Loss: 0.0012\n",
      "Epoch:0, Batch:308, Loss: 0.0015\n",
      "Epoch:0, Batch:309, Loss: 0.0014\n",
      "Epoch:0, Batch:310, Loss: 0.0011\n",
      "Epoch:0, Batch:311, Loss: 0.0014\n",
      "Epoch:0, Batch:312, Loss: 0.0014\n",
      "Epoch:0, Batch:313, Loss: 0.0015\n",
      "Epoch:0, Batch:314, Loss: 0.0013\n",
      "Epoch:0, Batch:315, Loss: 0.0016\n",
      "Epoch:0, Batch:316, Loss: 0.0013\n",
      "Epoch:0, Batch:317, Loss: 0.0013\n",
      "Epoch:0, Batch:318, Loss: 0.0013\n",
      "Epoch:0, Batch:319, Loss: 0.0013\n",
      "Epoch:0, Batch:320, Loss: 0.0012\n",
      "Epoch:0, Batch:321, Loss: 0.0013\n",
      "Epoch:0, Batch:322, Loss: 0.0013\n",
      "Epoch:0, Batch:323, Loss: 0.0013\n",
      "Epoch:0, Batch:324, Loss: 0.0012\n",
      "Epoch:0, Batch:325, Loss: 0.0015\n",
      "Epoch:0, Batch:326, Loss: 0.0013\n",
      "Epoch:0, Batch:327, Loss: 0.0014\n",
      "Epoch:0, Batch:328, Loss: 0.0014\n",
      "Epoch:0, Batch:329, Loss: 0.0014\n",
      "Epoch:0, Batch:330, Loss: 0.0013\n",
      "Epoch:0, Batch:331, Loss: 0.0011\n",
      "Epoch:0, Batch:332, Loss: 0.0013\n",
      "Epoch:0, Batch:333, Loss: 0.0012\n",
      "Epoch:0, Batch:334, Loss: 0.0013\n",
      "Epoch:0, Batch:335, Loss: 0.0013\n",
      "Epoch:0, Batch:336, Loss: 0.0012\n",
      "Epoch:0, Batch:337, Loss: 0.0015\n",
      "Epoch:0, Batch:338, Loss: 0.0016\n",
      "Epoch:0, Batch:339, Loss: 0.0013\n",
      "Epoch:0, Batch:340, Loss: 0.0013\n",
      "Epoch:0, Batch:341, Loss: 0.0015\n",
      "Epoch:0, Batch:342, Loss: 0.0015\n",
      "Epoch:0, Batch:343, Loss: 0.0015\n",
      "Epoch:0, Batch:344, Loss: 0.0013\n",
      "Epoch:0, Batch:345, Loss: 0.0013\n",
      "Epoch:0, Batch:346, Loss: 0.0012\n",
      "Epoch:0, Batch:347, Loss: 0.0014\n",
      "Epoch:0, Batch:348, Loss: 0.0014\n",
      "Epoch:0, Batch:349, Loss: 0.0014\n",
      "Epoch:0, Batch:350, Loss: 0.0013\n",
      "Epoch:0, Batch:351, Loss: 0.0013\n",
      "Epoch:0, Batch:352, Loss: 0.0014\n",
      "Epoch:0, Batch:353, Loss: 0.0012\n",
      "Epoch:0, Batch:354, Loss: 0.0013\n",
      "Epoch:0, Batch:355, Loss: 0.0012\n",
      "Epoch:0, Batch:356, Loss: 0.0012\n",
      "Epoch:0, Batch:357, Loss: 0.0012\n",
      "Epoch:0, Batch:358, Loss: 0.0013\n",
      "Epoch:0, Batch:359, Loss: 0.0012\n",
      "Epoch:0, Batch:360, Loss: 0.0013\n",
      "Epoch:0, Batch:361, Loss: 0.0014\n",
      "Epoch:0, Batch:362, Loss: 0.0014\n",
      "Epoch:0, Batch:363, Loss: 0.0012\n",
      "Epoch:0, Batch:364, Loss: 0.0013\n",
      "Epoch:0, Batch:365, Loss: 0.0012\n",
      "Epoch:0, Batch:366, Loss: 0.0012\n",
      "Epoch:0, Batch:367, Loss: 0.0012\n",
      "Epoch:0, Batch:368, Loss: 0.0010\n",
      "Epoch:0, Batch:369, Loss: 0.0014\n",
      "Epoch:0, Batch:370, Loss: 0.0012\n",
      "Epoch:0, Batch:371, Loss: 0.0012\n",
      "Epoch:0, Batch:372, Loss: 0.0014\n",
      "Epoch:0, Batch:373, Loss: 0.0011\n",
      "Epoch:0, Batch:374, Loss: 0.0013\n",
      "Epoch:0, Batch:375, Loss: 0.0012\n",
      "Epoch:0, Batch:376, Loss: 0.0012\n",
      "Epoch:0, Batch:377, Loss: 0.0012\n",
      "Epoch:0, Batch:378, Loss: 0.0013\n",
      "Epoch:0, Batch:379, Loss: 0.0015\n",
      "Epoch:0, Batch:380, Loss: 0.0012\n",
      "Epoch:0, Batch:381, Loss: 0.0013\n",
      "Epoch:0, Batch:382, Loss: 0.0012\n",
      "Epoch:0, Batch:383, Loss: 0.0013\n",
      "Epoch:0, Batch:384, Loss: 0.0011\n",
      "Epoch:0, Batch:385, Loss: 0.0011\n",
      "Epoch:0, Batch:386, Loss: 0.0012\n",
      "Epoch:0, Batch:387, Loss: 0.0013\n",
      "Epoch:0, Batch:388, Loss: 0.0014\n",
      "Epoch:0, Batch:389, Loss: 0.0012\n",
      "Epoch:0, Batch:390, Loss: 0.0012\n",
      "Epoch:0, Batch:391, Loss: 0.0013\n",
      "Epoch:0, Batch:392, Loss: 0.0015\n",
      "Epoch:0, Batch:393, Loss: 0.0013\n",
      "Epoch:0, Batch:394, Loss: 0.0014\n",
      "Epoch:0, Batch:395, Loss: 0.0013\n",
      "Epoch:0, Batch:396, Loss: 0.0012\n",
      "Epoch:0, Batch:397, Loss: 0.0013\n",
      "Epoch:0, Batch:398, Loss: 0.0014\n",
      "Epoch:0, Batch:399, Loss: 0.0012\n",
      "Epoch:0, Batch:400, Loss: 0.0013\n",
      "Epoch:0, Batch:401, Loss: 0.0014\n",
      "Epoch:0, Batch:402, Loss: 0.0013\n",
      "Epoch:0, Batch:403, Loss: 0.0013\n",
      "Epoch:0, Batch:404, Loss: 0.0013\n",
      "Epoch:0, Batch:405, Loss: 0.0013\n",
      "Epoch:0, Batch:406, Loss: 0.0012\n",
      "Epoch:0, Batch:407, Loss: 0.0013\n",
      "Epoch:0, Batch:408, Loss: 0.0013\n",
      "Epoch:0, Batch:409, Loss: 0.0013\n",
      "Epoch:0, Batch:410, Loss: 0.0012\n",
      "Epoch:0, Batch:411, Loss: 0.0013\n",
      "Epoch:0, Batch:412, Loss: 0.0012\n",
      "Epoch:0, Batch:413, Loss: 0.0013\n",
      "Epoch:0, Batch:414, Loss: 0.0012\n",
      "Epoch:0, Batch:415, Loss: 0.0014\n",
      "Epoch:0, Batch:416, Loss: 0.0011\n",
      "Epoch:0, Batch:417, Loss: 0.0012\n",
      "Epoch:0, Batch:418, Loss: 0.0013\n",
      "Epoch:0, Batch:419, Loss: 0.0011\n",
      "Epoch:0, Batch:420, Loss: 0.0014\n",
      "Epoch:0, Batch:421, Loss: 0.0013\n",
      "Epoch:0, Batch:422, Loss: 0.0012\n",
      "Epoch:0, Batch:423, Loss: 0.0013\n",
      "Epoch:0, Batch:424, Loss: 0.0013\n",
      "Epoch:0, Batch:425, Loss: 0.0012\n",
      "Epoch:0, Batch:426, Loss: 0.0012\n",
      "Epoch:0, Batch:427, Loss: 0.0012\n",
      "Epoch:0, Batch:428, Loss: 0.0013\n",
      "Epoch:0, Batch:429, Loss: 0.0014\n",
      "Epoch:0, Batch:430, Loss: 0.0013\n",
      "Epoch:0, Batch:431, Loss: 0.0011\n",
      "Epoch:0, Batch:432, Loss: 0.0015\n",
      "Epoch:0, Batch:433, Loss: 0.0013\n",
      "Epoch:0, Batch:434, Loss: 0.0014\n",
      "Epoch:0, Batch:435, Loss: 0.0011\n",
      "Epoch:0, Batch:436, Loss: 0.0013\n",
      "Epoch:0, Batch:437, Loss: 0.0012\n",
      "Epoch:0, Batch:438, Loss: 0.0012\n",
      "Epoch:0, Batch:439, Loss: 0.0017\n",
      "Epoch:0, Batch:440, Loss: 0.0011\n",
      "Epoch:0, Batch:441, Loss: 0.0012\n",
      "Epoch:0, Batch:442, Loss: 0.0010\n",
      "Epoch:0, Batch:443, Loss: 0.0012\n",
      "Epoch:0, Batch:444, Loss: 0.0014\n",
      "Epoch:0, Batch:445, Loss: 0.0013\n",
      "Epoch:0, Batch:446, Loss: 0.0012\n",
      "Epoch:0, Batch:447, Loss: 0.0011\n",
      "Epoch:0, Batch:448, Loss: 0.0011\n",
      "Epoch:0, Batch:449, Loss: 0.0012\n",
      "Epoch:0, Batch:450, Loss: 0.0012\n",
      "Epoch:0, Batch:451, Loss: 0.0012\n",
      "Epoch:0, Batch:452, Loss: 0.0011\n",
      "Epoch:0, Batch:453, Loss: 0.0013\n",
      "Epoch:0, Batch:454, Loss: 0.0013\n",
      "Epoch:0, Batch:455, Loss: 0.0013\n",
      "Epoch:0, Batch:456, Loss: 0.0016\n",
      "Epoch:0, Batch:457, Loss: 0.0012\n",
      "Epoch:0, Batch:458, Loss: 0.0012\n",
      "Epoch:0, Batch:459, Loss: 0.0011\n",
      "Epoch:0, Batch:460, Loss: 0.0015\n",
      "Epoch:0, Batch:461, Loss: 0.0015\n",
      "Epoch:0, Batch:462, Loss: 0.0012\n",
      "Epoch:0, Batch:463, Loss: 0.0014\n",
      "Epoch:0, Batch:464, Loss: 0.0014\n",
      "Epoch:0, Batch:465, Loss: 0.0011\n",
      "Epoch:0, Batch:466, Loss: 0.0012\n",
      "Epoch:0, Batch:467, Loss: 0.0012\n",
      "Epoch:0, Batch:468, Loss: 0.0013\n",
      "Epoch:0, Batch:469, Loss: 0.0013\n",
      "Epoch:0, Batch:470, Loss: 0.0013\n",
      "Epoch:0, Batch:471, Loss: 0.0011\n",
      "Epoch:0, Batch:472, Loss: 0.0013\n",
      "Epoch:0, Batch:473, Loss: 0.0013\n",
      "Epoch:0, Batch:474, Loss: 0.0012\n",
      "Epoch:0, Batch:475, Loss: 0.0012\n",
      "Epoch:0, Batch:476, Loss: 0.0013\n",
      "Epoch:0, Batch:477, Loss: 0.0013\n",
      "Epoch:0, Batch:478, Loss: 0.0012\n",
      "Epoch:0, Batch:479, Loss: 0.0013\n",
      "Epoch:0, Batch:480, Loss: 0.0013\n",
      "Epoch:0, Batch:481, Loss: 0.0013\n",
      "Epoch:0, Batch:482, Loss: 0.0012\n",
      "Epoch:0, Batch:483, Loss: 0.0012\n",
      "Epoch:0, Batch:484, Loss: 0.0012\n",
      "Epoch:0, Batch:485, Loss: 0.0014\n",
      "Epoch:0, Batch:486, Loss: 0.0013\n",
      "Epoch:0, Batch:487, Loss: 0.0012\n",
      "Epoch:0, Batch:488, Loss: 0.0012\n",
      "Epoch:0, Batch:489, Loss: 0.0013\n",
      "Epoch:0, Batch:490, Loss: 0.0012\n",
      "Epoch:0, Batch:491, Loss: 0.0014\n",
      "Epoch:0, Batch:492, Loss: 0.0012\n",
      "Epoch:0, Batch:493, Loss: 0.0013\n",
      "Epoch:0, Batch:494, Loss: 0.0011\n",
      "Epoch:0, Batch:495, Loss: 0.0013\n",
      "Epoch:0, Batch:496, Loss: 0.0011\n",
      "Epoch:0, Batch:497, Loss: 0.0013\n",
      "Epoch:0, Batch:498, Loss: 0.0012\n",
      "Epoch:0, Batch:499, Loss: 0.0013\n",
      "Epoch:0, Batch:500, Loss: 0.0013\n",
      "Epoch:0, Batch:501, Loss: 0.0012\n",
      "Epoch:0, Batch:502, Loss: 0.0015\n",
      "Epoch:0, Batch:503, Loss: 0.0013\n",
      "Epoch:0, Batch:504, Loss: 0.0012\n",
      "Epoch:0, Batch:505, Loss: 0.0014\n",
      "Epoch:0, Batch:506, Loss: 0.0012\n",
      "Epoch:0, Batch:507, Loss: 0.0012\n",
      "Epoch:0, Batch:508, Loss: 0.0013\n",
      "Epoch:0, Batch:509, Loss: 0.0013\n",
      "Epoch:0, Batch:510, Loss: 0.0014\n",
      "Epoch:0, Batch:511, Loss: 0.0012\n",
      "Epoch:0, Batch:512, Loss: 0.0008\n",
      "Epoch:0, Batch:513, Loss: 0.0016\n",
      "Epoch:0, Batch:514, Loss: 0.0014\n",
      "Epoch:0, Batch:515, Loss: 0.0013\n",
      "Epoch:0, Batch:516, Loss: 0.0014\n",
      "Epoch:0, Batch:517, Loss: 0.0012\n",
      "Epoch:0, Batch:518, Loss: 0.0013\n",
      "Epoch:0, Batch:519, Loss: 0.0012\n",
      "Epoch:0, Batch:520, Loss: 0.0012\n",
      "Epoch:0, Batch:521, Loss: 0.0012\n",
      "Epoch:0, Batch:522, Loss: 0.0013\n",
      "Epoch:0, Batch:523, Loss: 0.0011\n",
      "Epoch:0, Batch:524, Loss: 0.0013\n",
      "Epoch:0, Batch:525, Loss: 0.0012\n",
      "Epoch:0, Batch:526, Loss: 0.0016\n",
      "Epoch:0, Batch:527, Loss: 0.0013\n",
      "Epoch:0, Batch:528, Loss: 0.0013\n",
      "Epoch:0, Batch:529, Loss: 0.0015\n",
      "Epoch:0, Batch:530, Loss: 0.0013\n",
      "Epoch:0, Batch:531, Loss: 0.0011\n",
      "Epoch:0, Batch:532, Loss: 0.0012\n",
      "Epoch:0, Batch:533, Loss: 0.0010\n",
      "Epoch:0, Batch:534, Loss: 0.0014\n",
      "Epoch:0, Batch:535, Loss: 0.0012\n",
      "Epoch:0, Batch:536, Loss: 0.0011\n",
      "Epoch:0, Batch:537, Loss: 0.0011\n",
      "Epoch:0, Batch:538, Loss: 0.0012\n",
      "Epoch:0, Batch:539, Loss: 0.0014\n",
      "Epoch:0, Batch:540, Loss: 0.0015\n",
      "Epoch:0, Batch:541, Loss: 0.0012\n",
      "Epoch:0, Batch:542, Loss: 0.0014\n",
      "Epoch:0, Batch:543, Loss: 0.0012\n",
      "Epoch:0, Batch:544, Loss: 0.0010\n",
      "Epoch:0, Batch:545, Loss: 0.0012\n",
      "Epoch:0, Batch:546, Loss: 0.0015\n",
      "Epoch:0, Batch:547, Loss: 0.0009\n",
      "Epoch:0, Batch:548, Loss: 0.0013\n",
      "Epoch:0, Batch:549, Loss: 0.0009\n",
      "Epoch:0, Batch:550, Loss: 0.0011\n",
      "Epoch:0, Batch:551, Loss: 0.0018\n",
      "Epoch:0, Batch:552, Loss: 0.0018\n",
      "Epoch:0, Batch:553, Loss: 0.0015\n",
      "Epoch:0, Batch:554, Loss: 0.0012\n",
      "Epoch:0, Batch:555, Loss: 0.0009\n",
      "Epoch:0, Batch:556, Loss: 0.0017\n",
      "Epoch:0, Batch:557, Loss: 0.0026\n",
      "Epoch:0, Batch:558, Loss: 0.0017\n",
      "Epoch:0, Batch:559, Loss: 0.0017\n",
      "Epoch:0, Batch:560, Loss: 0.0010\n",
      "Epoch:0, Batch:561, Loss: 0.0016\n",
      "Epoch:0, Batch:562, Loss: 0.0011\n",
      "Epoch:0, Batch:563, Loss: 0.0013\n",
      "Epoch:0, Batch:564, Loss: 0.0020\n",
      "Epoch:0, Batch:565, Loss: 0.0059\n",
      "Epoch:0, Batch:566, Loss: 0.0017\n",
      "Epoch:0, Batch:567, Loss: 0.0031\n",
      "Epoch:0, Batch:568, Loss: 0.0083\n",
      "Epoch:0, Batch:569, Loss: 0.0030\n",
      "Epoch:0, Batch:570, Loss: 0.0023\n",
      "Epoch:0, Batch:571, Loss: 0.0026\n",
      "Epoch:0, Batch:572, Loss: 0.0039\n",
      "Epoch:0, Batch:573, Loss: 0.0057\n",
      "Epoch:0, Batch:574, Loss: 0.0039\n",
      "Epoch:0, Batch:575, Loss: 0.0021\n",
      "Epoch:0, Batch:576, Loss: 0.0046\n",
      "Epoch:0, Batch:577, Loss: 0.0081\n",
      "Epoch:0, Batch:578, Loss: 0.0060\n",
      "Epoch:0, Batch:579, Loss: 0.0009\n",
      "Epoch:0, Batch:580, Loss: 0.0020\n",
      "Epoch:0, Batch:581, Loss: 0.0062\n",
      "Epoch:0, Batch:582, Loss: 0.0132\n",
      "Epoch:0, Batch:583, Loss: 0.0051\n",
      "Epoch:0, Batch:584, Loss: 0.0014\n",
      "Epoch:0, Batch:585, Loss: 0.0096\n",
      "Epoch:0, Batch:586, Loss: 0.0110\n",
      "Epoch:0, Batch:587, Loss: 0.0017\n",
      "Epoch:0, Batch:588, Loss: 0.0050\n",
      "Epoch:0, Batch:589, Loss: 0.0071\n",
      "Epoch:0, Batch:590, Loss: 0.0050\n",
      "Epoch:0, Batch:591, Loss: 0.0034\n",
      "Epoch:0, Batch:592, Loss: 0.0023\n",
      "Epoch:0, Batch:593, Loss: 0.0027\n",
      "Epoch:0, Batch:594, Loss: 0.0049\n",
      "Epoch:0, Batch:595, Loss: 0.0030\n",
      "Epoch:0, Batch:596, Loss: 0.0013\n",
      "Epoch:0, Batch:597, Loss: 0.0049\n",
      "Epoch:0, Batch:598, Loss: 0.0020\n",
      "Epoch:0, Batch:599, Loss: 0.0011\n",
      "Epoch:0, Batch:600, Loss: 0.0053\n",
      "Epoch:0, Batch:601, Loss: 0.0063\n",
      "Epoch:0, Batch:602, Loss: 0.0018\n",
      "Epoch:0, Batch:603, Loss: 0.0018\n",
      "Epoch:0, Batch:604, Loss: 0.0043\n",
      "Epoch:0, Batch:605, Loss: 0.0061\n",
      "Epoch:0, Batch:606, Loss: 0.0027\n",
      "Epoch:0, Batch:607, Loss: 0.0011\n",
      "Epoch:0, Batch:608, Loss: 0.0030\n",
      "Epoch:0, Batch:609, Loss: 0.0050\n",
      "Epoch:0, Batch:610, Loss: 0.0041\n",
      "Epoch:0, Batch:611, Loss: 0.0016\n",
      "Epoch:0, Batch:612, Loss: 0.0016\n",
      "Epoch:0, Batch:613, Loss: 0.0034\n",
      "Epoch:0, Batch:614, Loss: 0.0040\n",
      "Epoch:0, Batch:615, Loss: 0.0023\n",
      "Epoch:0, Batch:616, Loss: 0.0012\n",
      "Epoch:0, Batch:617, Loss: 0.0021\n",
      "Epoch:0, Batch:618, Loss: 0.0029\n",
      "Epoch:0, Batch:619, Loss: 0.0030\n",
      "Epoch:0, Batch:620, Loss: 0.0014\n",
      "Epoch:0, Batch:621, Loss: 0.0018\n",
      "Epoch:0, Batch:622, Loss: 0.0019\n",
      "Epoch:0, Batch:623, Loss: 0.0025\n",
      "Epoch:0, Batch:624, Loss: 0.0016\n",
      "Epoch:0, Batch:625, Loss: 0.0016\n",
      "Epoch:0, Batch:626, Loss: 0.0012\n",
      "Epoch:0, Batch:627, Loss: 0.0026\n",
      "Epoch:0, Batch:628, Loss: 0.0021\n",
      "Epoch:0, Batch:629, Loss: 0.0015\n",
      "Epoch:0, Batch:630, Loss: 0.0015\n",
      "Epoch:0, Batch:631, Loss: 0.0022\n",
      "Epoch:0, Batch:632, Loss: 0.0018\n",
      "Epoch:0, Batch:633, Loss: 0.0014\n",
      "Epoch:0, Batch:634, Loss: 0.0016\n",
      "Epoch:0, Batch:635, Loss: 0.0015\n",
      "Epoch:0, Batch:636, Loss: 0.0014\n",
      "Epoch:0, Batch:637, Loss: 0.0019\n",
      "Epoch:0, Batch:638, Loss: 0.0015\n",
      "Epoch:0, Batch:639, Loss: 0.0011\n",
      "Epoch:0, Batch:640, Loss: 0.0019\n",
      "Epoch:0, Batch:641, Loss: 0.0019\n",
      "Epoch:0, Batch:642, Loss: 0.0015\n",
      "Epoch:0, Batch:643, Loss: 0.0013\n",
      "Epoch:0, Batch:644, Loss: 0.0027\n",
      "Epoch:0, Batch:645, Loss: 0.0013\n",
      "Epoch:0, Batch:646, Loss: 0.0009\n",
      "Epoch:0, Batch:647, Loss: 0.0017\n",
      "Epoch:0, Batch:648, Loss: 0.0014\n",
      "Epoch:0, Batch:649, Loss: 0.0011\n",
      "Epoch:0, Batch:650, Loss: 0.0010\n",
      "Epoch:0, Batch:651, Loss: 0.0016\n",
      "Epoch:0, Batch:652, Loss: 0.0013\n",
      "Epoch:0, Batch:653, Loss: 0.0010\n",
      "Epoch:0, Batch:654, Loss: 0.0022\n",
      "Epoch:0, Batch:655, Loss: 0.0014\n",
      "Epoch:0, Batch:656, Loss: 0.0013\n",
      "Epoch:0, Batch:657, Loss: 0.0013\n",
      "Epoch:0, Batch:658, Loss: 0.0015\n",
      "Epoch:0, Batch:659, Loss: 0.0013\n",
      "Epoch:0, Batch:660, Loss: 0.0011\n",
      "Epoch:0, Batch:661, Loss: 0.0011\n",
      "Epoch:0, Batch:662, Loss: 0.0013\n",
      "Epoch:0, Batch:663, Loss: 0.0014\n",
      "Epoch:0, Batch:664, Loss: 0.0011\n",
      "Epoch:0, Batch:665, Loss: 0.0014\n",
      "Epoch:0, Batch:666, Loss: 0.0012\n",
      "Epoch:0, Batch:667, Loss: 0.0011\n",
      "Epoch:0, Batch:668, Loss: 0.0010\n",
      "Epoch:0, Batch:669, Loss: 0.0010\n",
      "Epoch:0, Batch:670, Loss: 0.0014\n",
      "Epoch:0, Batch:671, Loss: 0.0016\n",
      "Epoch:0, Batch:672, Loss: 0.0020\n",
      "Epoch:0, Batch:673, Loss: 0.0032\n",
      "Epoch:0, Batch:674, Loss: 0.0016\n",
      "Epoch:0, Batch:675, Loss: 0.0014\n",
      "Epoch:0, Batch:676, Loss: 0.0011\n",
      "Epoch:0, Batch:677, Loss: 0.0047\n",
      "Epoch:0, Batch:678, Loss: 0.0019\n",
      "Epoch:0, Batch:679, Loss: 0.0030\n",
      "Epoch:0, Batch:680, Loss: 0.0031\n",
      "Epoch:0, Batch:681, Loss: 0.0016\n",
      "Epoch:0, Batch:682, Loss: 0.0018\n",
      "Epoch:0, Batch:683, Loss: 0.0028\n",
      "Epoch:0, Batch:684, Loss: 0.0030\n",
      "Epoch:0, Batch:685, Loss: 0.0032\n",
      "Epoch:0, Batch:686, Loss: 0.0017\n",
      "Epoch:0, Batch:687, Loss: 0.0035\n",
      "Epoch:0, Batch:688, Loss: 0.0045\n",
      "Epoch:0, Batch:689, Loss: 0.0026\n",
      "Epoch:0, Batch:690, Loss: 0.0013\n",
      "Epoch:0, Batch:691, Loss: 0.0027\n",
      "Epoch:0, Batch:692, Loss: 0.0022\n",
      "Epoch:0, Batch:693, Loss: 0.0025\n",
      "Epoch:0, Batch:694, Loss: 0.0015\n",
      "Epoch:0, Batch:695, Loss: 0.0013\n",
      "Epoch:0, Batch:696, Loss: 0.0020\n",
      "Epoch:0, Batch:697, Loss: 0.0025\n",
      "Epoch:0, Batch:698, Loss: 0.0014\n",
      "Epoch:0, Batch:699, Loss: 0.0016\n",
      "Epoch:0, Batch:700, Loss: 0.0025\n",
      "Epoch:0, Batch:701, Loss: 0.0020\n",
      "Epoch:0, Batch:702, Loss: 0.0015\n",
      "Epoch:0, Batch:703, Loss: 0.0015\n",
      "Epoch:0, Batch:704, Loss: 0.0025\n",
      "Epoch:0, Batch:705, Loss: 0.0030\n",
      "Epoch:0, Batch:706, Loss: 0.0027\n",
      "Epoch:0, Batch:707, Loss: 0.0014\n",
      "Epoch:0, Batch:708, Loss: 0.0014\n",
      "Epoch:0, Batch:709, Loss: 0.0029\n",
      "Epoch:0, Batch:710, Loss: 0.0031\n",
      "Epoch:0, Batch:711, Loss: 0.0022\n",
      "Epoch:0, Batch:712, Loss: 0.0011\n",
      "Epoch:0, Batch:713, Loss: 0.0021\n",
      "Epoch:0, Batch:714, Loss: 0.0024\n",
      "Epoch:0, Batch:715, Loss: 0.0016\n",
      "Epoch:0, Batch:716, Loss: 0.0013\n",
      "Epoch:0, Batch:717, Loss: 0.0016\n",
      "Epoch:0, Batch:718, Loss: 0.0021\n",
      "Epoch:0, Batch:719, Loss: 0.0018\n",
      "Epoch:0, Batch:720, Loss: 0.0016\n",
      "Epoch:0, Batch:721, Loss: 0.0015\n",
      "Epoch:0, Batch:722, Loss: 0.0016\n",
      "Epoch:0, Batch:723, Loss: 0.0016\n",
      "Epoch:0, Batch:724, Loss: 0.0010\n",
      "Epoch:0, Batch:725, Loss: 0.0013\n",
      "Epoch:0, Batch:726, Loss: 0.0019\n",
      "Epoch:0, Batch:727, Loss: 0.0015\n",
      "Epoch:0, Batch:728, Loss: 0.0012\n",
      "Epoch:0, Batch:729, Loss: 0.0013\n",
      "Epoch:0, Batch:730, Loss: 0.0016\n",
      "Epoch:0, Batch:731, Loss: 0.0012\n",
      "Epoch:0, Batch:732, Loss: 0.0012\n",
      "Epoch:0, Batch:733, Loss: 0.0012\n",
      "Epoch:0, Batch:734, Loss: 0.0013\n",
      "Epoch:0, Batch:735, Loss: 0.0013\n",
      "Epoch:0, Batch:736, Loss: 0.0013\n",
      "Epoch:0, Batch:737, Loss: 0.0013\n",
      "Epoch:0, Batch:738, Loss: 0.0012\n",
      "Epoch:0, Batch:739, Loss: 0.0012\n",
      "Epoch:0, Batch:740, Loss: 0.0013\n",
      "Epoch:0, Batch:741, Loss: 0.0013\n",
      "Epoch:0, Batch:742, Loss: 0.0011\n",
      "Epoch:0, Batch:743, Loss: 0.0012\n",
      "Epoch:0, Batch:744, Loss: 0.0012\n",
      "Epoch:0, Batch:745, Loss: 0.0012\n",
      "Epoch:0, Batch:746, Loss: 0.0008\n",
      "Epoch:0, Batch:747, Loss: 0.0010\n",
      "Epoch:0, Batch:748, Loss: 0.0021\n",
      "Epoch:0, Batch:749, Loss: 0.0024\n",
      "Epoch:0, Batch:750, Loss: 0.0012\n",
      "Epoch:0, Batch:751, Loss: 0.0013\n",
      "Epoch:0, Batch:752, Loss: 0.0024\n",
      "Epoch:0, Batch:753, Loss: 0.0015\n",
      "Epoch:0, Batch:754, Loss: 0.0016\n",
      "Epoch:0, Batch:755, Loss: 0.0016\n",
      "Epoch:0, Batch:756, Loss: 0.0020\n",
      "Epoch:0, Batch:757, Loss: 0.0013\n",
      "Epoch:0, Batch:758, Loss: 0.0022\n",
      "Epoch:0, Batch:759, Loss: 0.0013\n",
      "Epoch:0, Batch:760, Loss: 0.0011\n",
      "Epoch:0, Batch:761, Loss: 0.0013\n",
      "Epoch:0, Batch:762, Loss: 0.0014\n",
      "Epoch:0, Batch:763, Loss: 0.0015\n",
      "Epoch:0, Batch:764, Loss: 0.0012\n",
      "Epoch:0, Batch:765, Loss: 0.0012\n",
      "Epoch:0, Batch:766, Loss: 0.0013\n",
      "Epoch:0, Batch:767, Loss: 0.0011\n",
      "Epoch:0, Batch:768, Loss: 0.0013\n",
      "Epoch:0, Batch:769, Loss: 0.0011\n",
      "Epoch:0, Batch:770, Loss: 0.0012\n",
      "Epoch:0, Batch:771, Loss: 0.0012\n",
      "Epoch:0, Batch:772, Loss: 0.0016\n",
      "Epoch:0, Batch:773, Loss: 0.0011\n",
      "Epoch:0, Batch:774, Loss: 0.0012\n",
      "Epoch:0, Batch:775, Loss: 0.0012\n",
      "Epoch:0, Batch:776, Loss: 0.0015\n",
      "Epoch:0, Batch:777, Loss: 0.0015\n",
      "Epoch:0, Batch:778, Loss: 0.0013\n",
      "Epoch:0, Batch:779, Loss: 0.0014\n",
      "Epoch:0, Batch:780, Loss: 0.0013\n",
      "Epoch:0, Batch:781, Loss: 0.0012\n",
      "Epoch:0, Batch:782, Loss: 0.0015\n",
      "Epoch:0, Batch:783, Loss: 0.0014\n",
      "Epoch:0, Batch:784, Loss: 0.0011\n",
      "Epoch:0, Batch:785, Loss: 0.0013\n",
      "Epoch:0, Batch:786, Loss: 0.0014\n",
      "Epoch:0, Batch:787, Loss: 0.0014\n",
      "Epoch:0, Batch:788, Loss: 0.0012\n",
      "Epoch:0, Batch:789, Loss: 0.0014\n",
      "Epoch:0, Batch:790, Loss: 0.0015\n",
      "Epoch:0, Batch:791, Loss: 0.0012\n",
      "Epoch:0, Batch:792, Loss: 0.0012\n",
      "Epoch:0, Batch:793, Loss: 0.0012\n",
      "Epoch:0, Batch:794, Loss: 0.0011\n",
      "Epoch:0, Batch:795, Loss: 0.0014\n",
      "Epoch:0, Batch:796, Loss: 0.0015\n",
      "Epoch:0, Batch:797, Loss: 0.0017\n",
      "Epoch:0, Batch:798, Loss: 0.0017\n",
      "Epoch:0, Batch:799, Loss: 0.0015\n",
      "Epoch:0, Batch:800, Loss: 0.0015\n",
      "Epoch:0, Batch:801, Loss: 0.0024\n",
      "Epoch:0, Batch:802, Loss: 0.0004\n",
      "Epoch:0, Batch:803, Loss: 0.0029\n",
      "Epoch:0, Batch:804, Loss: 0.0015\n",
      "Epoch:0, Batch:805, Loss: 0.0011\n",
      "Epoch:0, Batch:806, Loss: 0.0013\n",
      "Epoch:0, Batch:807, Loss: 0.0021\n",
      "Epoch:0, Batch:808, Loss: 0.0017\n",
      "Epoch:0, Batch:809, Loss: 0.0011\n",
      "Epoch:0, Batch:810, Loss: 0.0018\n",
      "Epoch:0, Batch:811, Loss: 0.0021\n",
      "Epoch:0, Batch:812, Loss: 0.0016\n",
      "Epoch:0, Batch:813, Loss: 0.0021\n",
      "Epoch:0, Batch:814, Loss: 0.0010\n",
      "Epoch:0, Batch:815, Loss: 0.0010\n",
      "Epoch:0, Batch:816, Loss: 0.0012\n",
      "Epoch:0, Batch:817, Loss: 0.0017\n",
      "Epoch:0, Batch:818, Loss: 0.0015\n",
      "Epoch:0, Batch:819, Loss: 0.0010\n",
      "Epoch:0, Batch:820, Loss: 0.0020\n",
      "Epoch:0, Batch:821, Loss: 0.0013\n",
      "Epoch:0, Batch:822, Loss: 0.0023\n",
      "Epoch:0, Batch:823, Loss: 0.0013\n",
      "Epoch:0, Batch:824, Loss: 0.0012\n",
      "Epoch:0, Batch:825, Loss: 0.0015\n",
      "Epoch:0, Batch:826, Loss: 0.0018\n",
      "Epoch:0, Batch:827, Loss: 0.0018\n",
      "Epoch:0, Batch:828, Loss: 0.0016\n",
      "Epoch:0, Batch:829, Loss: 0.0013\n",
      "Epoch:0, Batch:830, Loss: 0.0014\n",
      "Epoch:0, Batch:831, Loss: 0.0010\n",
      "Epoch:0, Batch:832, Loss: 0.0012\n",
      "Epoch:0, Batch:833, Loss: 0.0019\n",
      "Epoch:0, Batch:834, Loss: 0.0013\n",
      "Epoch:0, Batch:835, Loss: 0.0014\n",
      "Epoch:0, Batch:836, Loss: 0.0012\n",
      "Epoch:0, Batch:837, Loss: 0.0016\n",
      "Epoch:0, Batch:838, Loss: 0.0014\n",
      "Epoch:0, Batch:839, Loss: 0.0013\n",
      "Epoch:0, Batch:840, Loss: 0.0012\n",
      "Epoch:0, Batch:841, Loss: 0.0013\n",
      "Epoch:0, Batch:842, Loss: 0.0013\n",
      "Epoch:0, Batch:843, Loss: 0.0014\n",
      "Epoch:0, Batch:844, Loss: 0.0011\n",
      "Epoch:0, Batch:845, Loss: 0.0014\n",
      "Epoch:0, Batch:846, Loss: 0.0011\n",
      "Epoch:0, Batch:847, Loss: 0.0012\n",
      "Epoch:0, Batch:848, Loss: 0.0014\n",
      "Epoch:0, Batch:849, Loss: 0.0012\n",
      "Epoch:0, Batch:850, Loss: 0.0013\n",
      "Epoch:0, Batch:851, Loss: 0.0012\n",
      "Epoch:0, Batch:852, Loss: 0.0012\n",
      "Epoch:0, Batch:853, Loss: 0.0012\n",
      "Epoch:0, Batch:854, Loss: 0.0012\n",
      "Epoch:0, Batch:855, Loss: 0.0017\n",
      "Epoch:0, Batch:856, Loss: 0.0013\n",
      "Epoch:0, Batch:857, Loss: 0.0013\n",
      "Epoch:0, Batch:858, Loss: 0.0011\n",
      "Epoch:0, Batch:859, Loss: 0.0012\n",
      "Epoch:0, Batch:860, Loss: 0.0016\n",
      "Epoch:0, Batch:861, Loss: 0.0018\n",
      "Epoch:0, Batch:862, Loss: 0.0011\n",
      "Epoch:0, Batch:863, Loss: 0.0018\n",
      "Epoch:0, Batch:864, Loss: 0.0016\n",
      "Epoch:0, Batch:865, Loss: 0.0013\n",
      "Epoch:0, Batch:866, Loss: 0.0010\n",
      "Epoch:0, Batch:867, Loss: 0.0017\n",
      "Epoch:0, Batch:868, Loss: 0.0019\n",
      "Epoch:0, Batch:869, Loss: 0.0015\n",
      "Epoch:0, Batch:870, Loss: 0.0015\n",
      "Epoch:0, Batch:871, Loss: 0.0022\n",
      "Epoch:0, Batch:872, Loss: 0.0018\n",
      "Epoch:0, Batch:873, Loss: 0.0011\n",
      "Epoch:0, Batch:874, Loss: 0.0031\n",
      "Epoch:0, Batch:875, Loss: 0.0020\n",
      "Epoch:0, Batch:876, Loss: 0.0015\n",
      "Epoch:0, Batch:877, Loss: 0.0025\n",
      "Epoch:0, Batch:878, Loss: 0.0010\n",
      "Epoch:0, Batch:879, Loss: 0.0013\n",
      "Epoch:0, Batch:880, Loss: 0.0015\n",
      "Epoch:0, Batch:881, Loss: 0.0020\n",
      "Epoch:0, Batch:882, Loss: 0.0032\n",
      "Epoch:0, Batch:883, Loss: 0.0011\n",
      "Epoch:0, Batch:884, Loss: 0.0005\n",
      "Epoch:0, Batch:885, Loss: 0.0024\n",
      "Epoch:0, Batch:886, Loss: 0.0027\n",
      "Epoch:0, Batch:887, Loss: 0.0013\n",
      "Epoch:0, Batch:888, Loss: 0.0012\n",
      "Epoch:0, Batch:889, Loss: 0.0021\n",
      "Epoch:0, Batch:890, Loss: 0.0018\n",
      "Epoch:0, Batch:891, Loss: 0.0015\n",
      "Epoch:0, Batch:892, Loss: 0.0016\n",
      "Epoch:0, Batch:893, Loss: 0.0036\n",
      "Epoch:0, Batch:894, Loss: 0.0018\n",
      "Epoch:0, Batch:895, Loss: 0.0015\n",
      "Epoch:0, Batch:896, Loss: 0.0034\n",
      "Epoch:0, Batch:897, Loss: 0.0023\n",
      "Epoch:0, Batch:898, Loss: 0.0011\n",
      "Epoch:0, Batch:899, Loss: 0.0027\n",
      "Epoch:0, Batch:900, Loss: 0.0030\n",
      "Epoch:0, Batch:901, Loss: 0.0017\n",
      "Epoch:0, Batch:902, Loss: 0.0017\n",
      "Epoch:0, Batch:903, Loss: 0.0024\n",
      "Epoch:0, Batch:904, Loss: 0.0024\n",
      "Epoch:0, Batch:905, Loss: 0.0012\n",
      "Epoch:0, Batch:906, Loss: 0.0015\n",
      "Epoch:0, Batch:907, Loss: 0.0025\n",
      "Epoch:0, Batch:908, Loss: 0.0025\n",
      "Epoch:0, Batch:909, Loss: 0.0025\n",
      "Epoch:0, Batch:910, Loss: 0.0016\n",
      "Epoch:0, Batch:911, Loss: 0.0019\n",
      "Epoch:0, Batch:912, Loss: 0.0026\n",
      "Epoch:0, Batch:913, Loss: 0.0020\n",
      "Epoch:0, Batch:914, Loss: 0.0014\n",
      "Epoch:0, Batch:915, Loss: 0.0015\n",
      "Epoch:0, Batch:916, Loss: 0.0023\n",
      "Epoch:0, Batch:917, Loss: 0.0026\n",
      "Epoch:0, Batch:918, Loss: 0.0010\n",
      "Epoch:0, Batch:919, Loss: 0.0015\n",
      "Epoch:0, Batch:920, Loss: 0.0021\n",
      "Epoch:0, Batch:921, Loss: 0.0019\n",
      "Epoch:0, Batch:922, Loss: 0.0012\n",
      "Epoch:0, Batch:923, Loss: 0.0014\n",
      "Epoch:0, Batch:924, Loss: 0.0019\n",
      "Epoch:0, Batch:925, Loss: 0.0013\n",
      "Epoch:0, Batch:926, Loss: 0.0013\n",
      "Epoch:0, Batch:927, Loss: 0.0019\n",
      "Epoch:0, Batch:928, Loss: 0.0017\n",
      "Epoch:0, Batch:929, Loss: 0.0009\n",
      "Epoch:0, Batch:930, Loss: 0.0011\n",
      "Epoch:1, Batch:1, Loss: 0.0035\n",
      "Epoch:1, Batch:2, Loss: 0.0017\n",
      "Epoch:1, Batch:3, Loss: 0.0014\n",
      "Epoch:1, Batch:4, Loss: 0.0025\n",
      "Epoch:1, Batch:5, Loss: 0.0026\n",
      "Epoch:1, Batch:6, Loss: 0.0015\n",
      "Epoch:1, Batch:7, Loss: 0.0013\n",
      "Epoch:1, Batch:8, Loss: 0.0018\n",
      "Epoch:1, Batch:9, Loss: 0.0024\n",
      "Epoch:1, Batch:10, Loss: 0.0014\n",
      "Epoch:1, Batch:11, Loss: 0.0012\n",
      "Epoch:1, Batch:12, Loss: 0.0019\n",
      "Epoch:1, Batch:13, Loss: 0.0021\n",
      "Epoch:1, Batch:14, Loss: 0.0018\n",
      "Epoch:1, Batch:15, Loss: 0.0015\n",
      "Epoch:1, Batch:16, Loss: 0.0016\n",
      "Epoch:1, Batch:17, Loss: 0.0016\n",
      "Epoch:1, Batch:18, Loss: 0.0013\n",
      "Epoch:1, Batch:19, Loss: 0.0012\n",
      "Epoch:1, Batch:20, Loss: 0.0014\n",
      "Epoch:1, Batch:21, Loss: 0.0012\n",
      "Epoch:1, Batch:22, Loss: 0.0013\n",
      "Epoch:1, Batch:23, Loss: 0.0012\n",
      "Epoch:1, Batch:24, Loss: 0.0013\n",
      "Epoch:1, Batch:25, Loss: 0.0014\n",
      "Epoch:1, Batch:26, Loss: 0.0013\n",
      "Epoch:1, Batch:27, Loss: 0.0012\n",
      "Epoch:1, Batch:28, Loss: 0.0013\n",
      "Epoch:1, Batch:29, Loss: 0.0015\n",
      "Epoch:1, Batch:30, Loss: 0.0011\n",
      "Epoch:1, Batch:31, Loss: 0.0012\n",
      "Epoch:1, Batch:32, Loss: 0.0013\n",
      "Epoch:1, Batch:33, Loss: 0.0013\n",
      "Epoch:1, Batch:34, Loss: 0.0012\n",
      "Epoch:1, Batch:35, Loss: 0.0014\n",
      "Epoch:1, Batch:36, Loss: 0.0014\n",
      "Epoch:1, Batch:37, Loss: 0.0013\n",
      "Epoch:1, Batch:38, Loss: 0.0013\n",
      "Epoch:1, Batch:39, Loss: 0.0013\n",
      "Epoch:1, Batch:40, Loss: 0.0013\n",
      "Epoch:1, Batch:41, Loss: 0.0012\n",
      "Epoch:1, Batch:42, Loss: 0.0014\n",
      "Epoch:1, Batch:43, Loss: 0.0012\n",
      "Epoch:1, Batch:44, Loss: 0.0011\n",
      "Epoch:1, Batch:45, Loss: 0.0012\n",
      "Epoch:1, Batch:46, Loss: 0.0013\n",
      "Epoch:1, Batch:47, Loss: 0.0012\n",
      "Epoch:1, Batch:48, Loss: 0.0012\n",
      "Epoch:1, Batch:49, Loss: 0.0013\n",
      "Epoch:1, Batch:50, Loss: 0.0012\n",
      "Epoch:1, Batch:51, Loss: 0.0011\n",
      "Epoch:1, Batch:52, Loss: 0.0013\n",
      "Epoch:1, Batch:53, Loss: 0.0012\n",
      "Epoch:1, Batch:54, Loss: 0.0013\n",
      "Epoch:1, Batch:55, Loss: 0.0014\n",
      "Epoch:1, Batch:56, Loss: 0.0014\n",
      "Epoch:1, Batch:57, Loss: 0.0014\n",
      "Epoch:1, Batch:58, Loss: 0.0012\n",
      "Epoch:1, Batch:59, Loss: 0.0013\n",
      "Epoch:1, Batch:60, Loss: 0.0012\n",
      "Epoch:1, Batch:61, Loss: 0.0012\n",
      "Epoch:1, Batch:62, Loss: 0.0012\n",
      "Epoch:1, Batch:63, Loss: 0.0014\n",
      "Epoch:1, Batch:64, Loss: 0.0011\n",
      "Epoch:1, Batch:65, Loss: 0.0013\n",
      "Epoch:1, Batch:66, Loss: 0.0014\n",
      "Epoch:1, Batch:67, Loss: 0.0010\n",
      "Epoch:1, Batch:68, Loss: 0.0011\n",
      "Epoch:1, Batch:69, Loss: 0.0015\n",
      "Epoch:1, Batch:70, Loss: 0.0012\n",
      "Epoch:1, Batch:71, Loss: 0.0012\n",
      "Epoch:1, Batch:72, Loss: 0.0014\n",
      "Epoch:1, Batch:73, Loss: 0.0013\n",
      "Epoch:1, Batch:74, Loss: 0.0012\n",
      "Epoch:1, Batch:75, Loss: 0.0013\n",
      "Epoch:1, Batch:76, Loss: 0.0012\n",
      "Epoch:1, Batch:77, Loss: 0.0012\n",
      "Epoch:1, Batch:78, Loss: 0.0014\n",
      "Epoch:1, Batch:79, Loss: 0.0012\n",
      "Epoch:1, Batch:80, Loss: 0.0013\n",
      "Epoch:1, Batch:81, Loss: 0.0012\n",
      "Epoch:1, Batch:82, Loss: 0.0012\n",
      "Epoch:1, Batch:83, Loss: 0.0012\n",
      "Epoch:1, Batch:84, Loss: 0.0012\n",
      "Epoch:1, Batch:85, Loss: 0.0012\n",
      "Epoch:1, Batch:86, Loss: 0.0013\n",
      "Epoch:1, Batch:87, Loss: 0.0012\n",
      "Epoch:1, Batch:88, Loss: 0.0012\n",
      "Epoch:1, Batch:89, Loss: 0.0012\n",
      "Epoch:1, Batch:90, Loss: 0.0012\n",
      "Epoch:1, Batch:91, Loss: 0.0012\n",
      "Epoch:1, Batch:92, Loss: 0.0015\n",
      "Epoch:1, Batch:93, Loss: 0.0013\n",
      "Epoch:1, Batch:94, Loss: 0.0012\n",
      "Epoch:1, Batch:95, Loss: 0.0014\n",
      "Epoch:1, Batch:96, Loss: 0.0013\n",
      "Epoch:1, Batch:97, Loss: 0.0013\n",
      "Epoch:1, Batch:98, Loss: 0.0013\n",
      "Epoch:1, Batch:99, Loss: 0.0012\n",
      "Epoch:1, Batch:100, Loss: 0.0012\n",
      "Epoch:1, Batch:101, Loss: 0.0012\n",
      "Epoch:1, Batch:102, Loss: 0.0014\n",
      "Epoch:1, Batch:103, Loss: 0.0014\n",
      "Epoch:1, Batch:104, Loss: 0.0012\n",
      "Epoch:1, Batch:105, Loss: 0.0014\n",
      "Epoch:1, Batch:106, Loss: 0.0012\n",
      "Epoch:1, Batch:107, Loss: 0.0015\n",
      "Epoch:1, Batch:108, Loss: 0.0014\n",
      "Epoch:1, Batch:109, Loss: 0.0012\n",
      "Epoch:1, Batch:110, Loss: 0.0013\n",
      "Epoch:1, Batch:111, Loss: 0.0013\n",
      "Epoch:1, Batch:112, Loss: 0.0015\n",
      "Epoch:1, Batch:113, Loss: 0.0012\n",
      "Epoch:1, Batch:114, Loss: 0.0015\n",
      "Epoch:1, Batch:115, Loss: 0.0013\n",
      "Epoch:1, Batch:116, Loss: 0.0011\n",
      "Epoch:1, Batch:117, Loss: 0.0012\n",
      "Epoch:1, Batch:118, Loss: 0.0012\n",
      "Epoch:1, Batch:119, Loss: 0.0012\n",
      "Epoch:1, Batch:120, Loss: 0.0013\n",
      "Epoch:1, Batch:121, Loss: 0.0014\n",
      "Epoch:1, Batch:122, Loss: 0.0012\n",
      "Epoch:1, Batch:123, Loss: 0.0012\n",
      "Epoch:1, Batch:124, Loss: 0.0012\n",
      "Epoch:1, Batch:125, Loss: 0.0016\n",
      "Epoch:1, Batch:126, Loss: 0.0013\n",
      "Epoch:1, Batch:127, Loss: 0.0012\n",
      "Epoch:1, Batch:128, Loss: 0.0012\n",
      "Epoch:1, Batch:129, Loss: 0.0013\n",
      "Epoch:1, Batch:130, Loss: 0.0013\n",
      "Epoch:1, Batch:131, Loss: 0.0013\n",
      "Epoch:1, Batch:132, Loss: 0.0014\n",
      "Epoch:1, Batch:133, Loss: 0.0012\n",
      "Epoch:1, Batch:134, Loss: 0.0013\n",
      "Epoch:1, Batch:135, Loss: 0.0013\n",
      "Epoch:1, Batch:136, Loss: 0.0013\n",
      "Epoch:1, Batch:137, Loss: 0.0013\n",
      "Epoch:1, Batch:138, Loss: 0.0012\n",
      "Epoch:1, Batch:139, Loss: 0.0012\n",
      "Epoch:1, Batch:140, Loss: 0.0013\n",
      "Epoch:1, Batch:141, Loss: 0.0012\n",
      "Epoch:1, Batch:142, Loss: 0.0013\n",
      "Epoch:1, Batch:143, Loss: 0.0011\n",
      "Epoch:1, Batch:144, Loss: 0.0012\n",
      "Epoch:1, Batch:145, Loss: 0.0011\n",
      "Epoch:1, Batch:146, Loss: 0.0012\n",
      "Epoch:1, Batch:147, Loss: 0.0013\n",
      "Epoch:1, Batch:148, Loss: 0.0013\n",
      "Epoch:1, Batch:149, Loss: 0.0013\n",
      "Epoch:1, Batch:150, Loss: 0.0011\n",
      "Epoch:1, Batch:151, Loss: 0.0011\n",
      "Epoch:1, Batch:152, Loss: 0.0012\n",
      "Epoch:1, Batch:153, Loss: 0.0013\n",
      "Epoch:1, Batch:154, Loss: 0.0013\n",
      "Epoch:1, Batch:155, Loss: 0.0011\n",
      "Epoch:1, Batch:156, Loss: 0.0013\n",
      "Epoch:1, Batch:157, Loss: 0.0012\n",
      "Epoch:1, Batch:158, Loss: 0.0012\n",
      "Epoch:1, Batch:159, Loss: 0.0013\n",
      "Epoch:1, Batch:160, Loss: 0.0013\n",
      "Epoch:1, Batch:161, Loss: 0.0012\n",
      "Epoch:1, Batch:162, Loss: 0.0012\n",
      "Epoch:1, Batch:163, Loss: 0.0012\n",
      "Epoch:1, Batch:164, Loss: 0.0012\n",
      "Epoch:1, Batch:165, Loss: 0.0012\n",
      "Epoch:1, Batch:166, Loss: 0.0014\n",
      "Epoch:1, Batch:167, Loss: 0.0011\n",
      "Epoch:1, Batch:168, Loss: 0.0012\n",
      "Epoch:1, Batch:169, Loss: 0.0012\n",
      "Epoch:1, Batch:170, Loss: 0.0012\n",
      "Epoch:1, Batch:171, Loss: 0.0012\n",
      "Epoch:1, Batch:172, Loss: 0.0013\n",
      "Epoch:1, Batch:173, Loss: 0.0014\n",
      "Epoch:1, Batch:174, Loss: 0.0011\n",
      "Epoch:1, Batch:175, Loss: 0.0011\n",
      "Epoch:1, Batch:176, Loss: 0.0013\n",
      "Epoch:1, Batch:177, Loss: 0.0013\n",
      "Epoch:1, Batch:178, Loss: 0.0013\n",
      "Epoch:1, Batch:179, Loss: 0.0012\n",
      "Epoch:1, Batch:180, Loss: 0.0014\n",
      "Epoch:1, Batch:181, Loss: 0.0011\n",
      "Epoch:1, Batch:182, Loss: 0.0012\n",
      "Epoch:1, Batch:183, Loss: 0.0014\n",
      "Epoch:1, Batch:184, Loss: 0.0011\n",
      "Epoch:1, Batch:185, Loss: 0.0014\n",
      "Epoch:1, Batch:186, Loss: 0.0013\n",
      "Epoch:1, Batch:187, Loss: 0.0014\n",
      "Epoch:1, Batch:188, Loss: 0.0015\n",
      "Epoch:1, Batch:189, Loss: 0.0012\n",
      "Epoch:1, Batch:190, Loss: 0.0013\n",
      "Epoch:1, Batch:191, Loss: 0.0012\n",
      "Epoch:1, Batch:192, Loss: 0.0013\n",
      "Epoch:1, Batch:193, Loss: 0.0013\n",
      "Epoch:1, Batch:194, Loss: 0.0014\n",
      "Epoch:1, Batch:195, Loss: 0.0012\n",
      "Epoch:1, Batch:196, Loss: 0.0013\n",
      "Epoch:1, Batch:197, Loss: 0.0012\n",
      "Epoch:1, Batch:198, Loss: 0.0013\n",
      "Epoch:1, Batch:199, Loss: 0.0012\n",
      "Epoch:1, Batch:200, Loss: 0.0012\n",
      "Epoch:1, Batch:201, Loss: 0.0012\n",
      "Epoch:1, Batch:202, Loss: 0.0012\n",
      "Epoch:1, Batch:203, Loss: 0.0013\n",
      "Epoch:1, Batch:204, Loss: 0.0012\n",
      "Epoch:1, Batch:205, Loss: 0.0014\n",
      "Epoch:1, Batch:206, Loss: 0.0012\n",
      "Epoch:1, Batch:207, Loss: 0.0013\n",
      "Epoch:1, Batch:208, Loss: 0.0013\n",
      "Epoch:1, Batch:209, Loss: 0.0012\n",
      "Epoch:1, Batch:210, Loss: 0.0012\n",
      "Epoch:1, Batch:211, Loss: 0.0012\n",
      "Epoch:1, Batch:212, Loss: 0.0013\n",
      "Epoch:1, Batch:213, Loss: 0.0014\n",
      "Epoch:1, Batch:214, Loss: 0.0012\n",
      "Epoch:1, Batch:215, Loss: 0.0013\n",
      "Epoch:1, Batch:216, Loss: 0.0013\n",
      "Epoch:1, Batch:217, Loss: 0.0014\n",
      "Epoch:1, Batch:218, Loss: 0.0013\n",
      "Epoch:1, Batch:219, Loss: 0.0011\n",
      "Epoch:1, Batch:220, Loss: 0.0011\n",
      "Epoch:1, Batch:221, Loss: 0.0014\n",
      "Epoch:1, Batch:222, Loss: 0.0012\n",
      "Epoch:1, Batch:223, Loss: 0.0014\n",
      "Epoch:1, Batch:224, Loss: 0.0012\n",
      "Epoch:1, Batch:225, Loss: 0.0013\n",
      "Epoch:1, Batch:226, Loss: 0.0013\n",
      "Epoch:1, Batch:227, Loss: 0.0013\n",
      "Epoch:1, Batch:228, Loss: 0.0013\n",
      "Epoch:1, Batch:229, Loss: 0.0012\n",
      "Epoch:1, Batch:230, Loss: 0.0013\n",
      "Epoch:1, Batch:231, Loss: 0.0012\n",
      "Epoch:1, Batch:232, Loss: 0.0012\n",
      "Epoch:1, Batch:233, Loss: 0.0012\n",
      "Epoch:1, Batch:234, Loss: 0.0012\n",
      "Epoch:1, Batch:235, Loss: 0.0013\n",
      "Epoch:1, Batch:236, Loss: 0.0013\n",
      "Epoch:1, Batch:237, Loss: 0.0012\n",
      "Epoch:1, Batch:238, Loss: 0.0011\n",
      "Epoch:1, Batch:239, Loss: 0.0012\n",
      "Epoch:1, Batch:240, Loss: 0.0012\n",
      "Epoch:1, Batch:241, Loss: 0.0013\n",
      "Epoch:1, Batch:242, Loss: 0.0013\n",
      "Epoch:1, Batch:243, Loss: 0.0013\n",
      "Epoch:1, Batch:244, Loss: 0.0013\n",
      "Epoch:1, Batch:245, Loss: 0.0013\n",
      "Epoch:1, Batch:246, Loss: 0.0012\n",
      "Epoch:1, Batch:247, Loss: 0.0012\n",
      "Epoch:1, Batch:248, Loss: 0.0013\n",
      "Epoch:1, Batch:249, Loss: 0.0012\n",
      "Epoch:1, Batch:250, Loss: 0.0013\n",
      "Epoch:1, Batch:251, Loss: 0.0011\n",
      "Epoch:1, Batch:252, Loss: 0.0013\n",
      "Epoch:1, Batch:253, Loss: 0.0012\n",
      "Epoch:1, Batch:254, Loss: 0.0012\n",
      "Epoch:1, Batch:255, Loss: 0.0013\n",
      "Epoch:1, Batch:256, Loss: 0.0013\n",
      "Epoch:1, Batch:257, Loss: 0.0012\n",
      "Epoch:1, Batch:258, Loss: 0.0012\n",
      "Epoch:1, Batch:259, Loss: 0.0012\n",
      "Epoch:1, Batch:260, Loss: 0.0011\n",
      "Epoch:1, Batch:261, Loss: 0.0013\n",
      "Epoch:1, Batch:262, Loss: 0.0012\n",
      "Epoch:1, Batch:263, Loss: 0.0012\n",
      "Epoch:1, Batch:264, Loss: 0.0013\n",
      "Epoch:1, Batch:265, Loss: 0.0014\n",
      "Epoch:1, Batch:266, Loss: 0.0014\n",
      "Epoch:1, Batch:267, Loss: 0.0013\n",
      "Epoch:1, Batch:268, Loss: 0.0013\n",
      "Epoch:1, Batch:269, Loss: 0.0013\n",
      "Epoch:1, Batch:270, Loss: 0.0012\n",
      "Epoch:1, Batch:271, Loss: 0.0012\n",
      "Epoch:1, Batch:272, Loss: 0.0012\n",
      "Epoch:1, Batch:273, Loss: 0.0013\n",
      "Epoch:1, Batch:274, Loss: 0.0015\n",
      "Epoch:1, Batch:275, Loss: 0.0014\n",
      "Epoch:1, Batch:276, Loss: 0.0012\n",
      "Epoch:1, Batch:277, Loss: 0.0013\n",
      "Epoch:1, Batch:278, Loss: 0.0013\n",
      "Epoch:1, Batch:279, Loss: 0.0013\n",
      "Epoch:1, Batch:280, Loss: 0.0012\n",
      "Epoch:1, Batch:281, Loss: 0.0012\n",
      "Epoch:1, Batch:282, Loss: 0.0013\n",
      "Epoch:1, Batch:283, Loss: 0.0012\n",
      "Epoch:1, Batch:284, Loss: 0.0012\n",
      "Epoch:1, Batch:285, Loss: 0.0012\n",
      "Epoch:1, Batch:286, Loss: 0.0011\n",
      "Epoch:1, Batch:287, Loss: 0.0012\n",
      "Epoch:1, Batch:288, Loss: 0.0011\n",
      "Epoch:1, Batch:289, Loss: 0.0012\n",
      "Epoch:1, Batch:290, Loss: 0.0013\n",
      "Epoch:1, Batch:291, Loss: 0.0012\n",
      "Epoch:1, Batch:292, Loss: 0.0012\n",
      "Epoch:1, Batch:293, Loss: 0.0013\n",
      "Epoch:1, Batch:294, Loss: 0.0013\n",
      "Epoch:1, Batch:295, Loss: 0.0013\n",
      "Epoch:1, Batch:296, Loss: 0.0012\n",
      "Epoch:1, Batch:297, Loss: 0.0013\n",
      "Epoch:1, Batch:298, Loss: 0.0012\n",
      "Epoch:1, Batch:299, Loss: 0.0015\n",
      "Epoch:1, Batch:300, Loss: 0.0012\n",
      "Epoch:1, Batch:301, Loss: 0.0014\n",
      "Epoch:1, Batch:302, Loss: 0.0014\n",
      "Epoch:1, Batch:303, Loss: 0.0013\n",
      "Epoch:1, Batch:304, Loss: 0.0012\n",
      "Epoch:1, Batch:305, Loss: 0.0011\n",
      "Epoch:1, Batch:306, Loss: 0.0012\n",
      "Epoch:1, Batch:307, Loss: 0.0012\n",
      "Epoch:1, Batch:308, Loss: 0.0011\n",
      "Epoch:1, Batch:309, Loss: 0.0013\n",
      "Epoch:1, Batch:310, Loss: 0.0012\n",
      "Epoch:1, Batch:311, Loss: 0.0013\n",
      "Epoch:1, Batch:312, Loss: 0.0012\n",
      "Epoch:1, Batch:313, Loss: 0.0012\n",
      "Epoch:1, Batch:314, Loss: 0.0012\n",
      "Epoch:1, Batch:315, Loss: 0.0012\n",
      "Epoch:1, Batch:316, Loss: 0.0012\n",
      "Epoch:1, Batch:317, Loss: 0.0013\n",
      "Epoch:1, Batch:318, Loss: 0.0013\n",
      "Epoch:1, Batch:319, Loss: 0.0014\n",
      "Epoch:1, Batch:320, Loss: 0.0013\n",
      "Epoch:1, Batch:321, Loss: 0.0014\n",
      "Epoch:1, Batch:322, Loss: 0.0013\n",
      "Epoch:1, Batch:323, Loss: 0.0012\n",
      "Epoch:1, Batch:324, Loss: 0.0013\n",
      "Epoch:1, Batch:325, Loss: 0.0015\n",
      "Epoch:1, Batch:326, Loss: 0.0013\n",
      "Epoch:1, Batch:327, Loss: 0.0014\n",
      "Epoch:1, Batch:328, Loss: 0.0012\n",
      "Epoch:1, Batch:329, Loss: 0.0012\n",
      "Epoch:1, Batch:330, Loss: 0.0012\n",
      "Epoch:1, Batch:331, Loss: 0.0013\n",
      "Epoch:1, Batch:332, Loss: 0.0012\n",
      "Epoch:1, Batch:333, Loss: 0.0013\n",
      "Epoch:1, Batch:334, Loss: 0.0012\n",
      "Epoch:1, Batch:335, Loss: 0.0013\n",
      "Epoch:1, Batch:336, Loss: 0.0012\n",
      "Epoch:1, Batch:337, Loss: 0.0011\n",
      "Epoch:1, Batch:338, Loss: 0.0012\n",
      "Epoch:1, Batch:339, Loss: 0.0012\n",
      "Epoch:1, Batch:340, Loss: 0.0012\n",
      "Epoch:1, Batch:341, Loss: 0.0012\n",
      "Epoch:1, Batch:342, Loss: 0.0012\n",
      "Epoch:1, Batch:343, Loss: 0.0012\n",
      "Epoch:1, Batch:344, Loss: 0.0016\n",
      "Epoch:1, Batch:345, Loss: 0.0011\n",
      "Epoch:1, Batch:346, Loss: 0.0012\n",
      "Epoch:1, Batch:347, Loss: 0.0013\n",
      "Epoch:1, Batch:348, Loss: 0.0014\n",
      "Epoch:1, Batch:349, Loss: 0.0013\n",
      "Epoch:1, Batch:350, Loss: 0.0013\n",
      "Epoch:1, Batch:351, Loss: 0.0010\n",
      "Epoch:1, Batch:352, Loss: 0.0010\n",
      "Epoch:1, Batch:353, Loss: 0.0012\n",
      "Epoch:1, Batch:354, Loss: 0.0009\n",
      "Epoch:1, Batch:355, Loss: 0.0012\n",
      "Epoch:1, Batch:356, Loss: 0.0015\n",
      "Epoch:1, Batch:357, Loss: 0.0011\n",
      "Epoch:1, Batch:358, Loss: 0.0011\n",
      "Epoch:1, Batch:359, Loss: 0.0014\n",
      "Epoch:1, Batch:360, Loss: 0.0015\n",
      "Epoch:1, Batch:361, Loss: 0.0014\n",
      "Epoch:1, Batch:362, Loss: 0.0014\n",
      "Epoch:1, Batch:363, Loss: 0.0018\n",
      "Epoch:1, Batch:364, Loss: 0.0015\n",
      "Epoch:1, Batch:365, Loss: 0.0014\n",
      "Epoch:1, Batch:366, Loss: 0.0011\n",
      "Epoch:1, Batch:367, Loss: 0.0012\n",
      "Epoch:1, Batch:368, Loss: 0.0012\n",
      "Epoch:1, Batch:369, Loss: 0.0012\n",
      "Epoch:1, Batch:370, Loss: 0.0012\n",
      "Epoch:1, Batch:371, Loss: 0.0012\n",
      "Epoch:1, Batch:372, Loss: 0.0012\n",
      "Epoch:1, Batch:373, Loss: 0.0012\n",
      "Epoch:1, Batch:374, Loss: 0.0013\n",
      "Epoch:1, Batch:375, Loss: 0.0015\n",
      "Epoch:1, Batch:376, Loss: 0.0012\n",
      "Epoch:1, Batch:377, Loss: 0.0012\n",
      "Epoch:1, Batch:378, Loss: 0.0013\n",
      "Epoch:1, Batch:379, Loss: 0.0010\n",
      "Epoch:1, Batch:380, Loss: 0.0011\n",
      "Epoch:1, Batch:381, Loss: 0.0012\n",
      "Epoch:1, Batch:382, Loss: 0.0013\n",
      "Epoch:1, Batch:383, Loss: 0.0013\n",
      "Epoch:1, Batch:384, Loss: 0.0015\n",
      "Epoch:1, Batch:385, Loss: 0.0013\n",
      "Epoch:1, Batch:386, Loss: 0.0013\n",
      "Epoch:1, Batch:387, Loss: 0.0014\n",
      "Epoch:1, Batch:388, Loss: 0.0011\n",
      "Epoch:1, Batch:389, Loss: 0.0012\n",
      "Epoch:1, Batch:390, Loss: 0.0013\n",
      "Epoch:1, Batch:391, Loss: 0.0011\n",
      "Epoch:1, Batch:392, Loss: 0.0012\n",
      "Epoch:1, Batch:393, Loss: 0.0011\n",
      "Epoch:1, Batch:394, Loss: 0.0012\n",
      "Epoch:1, Batch:395, Loss: 0.0012\n",
      "Epoch:1, Batch:396, Loss: 0.0013\n",
      "Epoch:1, Batch:397, Loss: 0.0013\n",
      "Epoch:1, Batch:398, Loss: 0.0011\n",
      "Epoch:1, Batch:399, Loss: 0.0014\n",
      "Epoch:1, Batch:400, Loss: 0.0013\n",
      "Epoch:1, Batch:401, Loss: 0.0014\n",
      "Epoch:1, Batch:402, Loss: 0.0013\n",
      "Epoch:1, Batch:403, Loss: 0.0012\n",
      "Epoch:1, Batch:404, Loss: 0.0013\n",
      "Epoch:1, Batch:405, Loss: 0.0014\n",
      "Epoch:1, Batch:406, Loss: 0.0010\n",
      "Epoch:1, Batch:407, Loss: 0.0014\n",
      "Epoch:1, Batch:408, Loss: 0.0013\n",
      "Epoch:1, Batch:409, Loss: 0.0014\n",
      "Epoch:1, Batch:410, Loss: 0.0013\n",
      "Epoch:1, Batch:411, Loss: 0.0014\n",
      "Epoch:1, Batch:412, Loss: 0.0014\n",
      "Epoch:1, Batch:413, Loss: 0.0015\n",
      "Epoch:1, Batch:414, Loss: 0.0013\n",
      "Epoch:1, Batch:415, Loss: 0.0014\n",
      "Epoch:1, Batch:416, Loss: 0.0012\n",
      "Epoch:1, Batch:417, Loss: 0.0013\n",
      "Epoch:1, Batch:418, Loss: 0.0013\n",
      "Epoch:1, Batch:419, Loss: 0.0014\n",
      "Epoch:1, Batch:420, Loss: 0.0012\n",
      "Epoch:1, Batch:421, Loss: 0.0017\n",
      "Epoch:1, Batch:422, Loss: 0.0013\n",
      "Epoch:1, Batch:423, Loss: 0.0012\n",
      "Epoch:1, Batch:424, Loss: 0.0014\n",
      "Epoch:1, Batch:425, Loss: 0.0013\n",
      "Epoch:1, Batch:426, Loss: 0.0012\n",
      "Epoch:1, Batch:427, Loss: 0.0012\n",
      "Epoch:1, Batch:428, Loss: 0.0012\n",
      "Epoch:1, Batch:429, Loss: 0.0012\n",
      "Epoch:1, Batch:430, Loss: 0.0013\n",
      "Epoch:1, Batch:431, Loss: 0.0012\n",
      "Epoch:1, Batch:432, Loss: 0.0013\n",
      "Epoch:1, Batch:433, Loss: 0.0012\n",
      "Epoch:1, Batch:434, Loss: 0.0012\n",
      "Epoch:1, Batch:435, Loss: 0.0012\n",
      "Epoch:1, Batch:436, Loss: 0.0012\n",
      "Epoch:1, Batch:437, Loss: 0.0013\n",
      "Epoch:1, Batch:438, Loss: 0.0012\n",
      "Epoch:1, Batch:439, Loss: 0.0013\n",
      "Epoch:1, Batch:440, Loss: 0.0013\n",
      "Epoch:1, Batch:441, Loss: 0.0012\n",
      "Epoch:1, Batch:442, Loss: 0.0013\n",
      "Epoch:1, Batch:443, Loss: 0.0012\n",
      "Epoch:1, Batch:444, Loss: 0.0013\n",
      "Epoch:1, Batch:445, Loss: 0.0012\n",
      "Epoch:1, Batch:446, Loss: 0.0012\n",
      "Epoch:1, Batch:447, Loss: 0.0012\n",
      "Epoch:1, Batch:448, Loss: 0.0013\n",
      "Epoch:1, Batch:449, Loss: 0.0012\n",
      "Epoch:1, Batch:450, Loss: 0.0013\n",
      "Epoch:1, Batch:451, Loss: 0.0013\n",
      "Epoch:1, Batch:452, Loss: 0.0012\n",
      "Epoch:1, Batch:453, Loss: 0.0014\n",
      "Epoch:1, Batch:454, Loss: 0.0012\n",
      "Epoch:1, Batch:455, Loss: 0.0012\n",
      "Epoch:1, Batch:456, Loss: 0.0013\n",
      "Epoch:1, Batch:457, Loss: 0.0013\n",
      "Epoch:1, Batch:458, Loss: 0.0013\n",
      "Epoch:1, Batch:459, Loss: 0.0012\n",
      "Epoch:1, Batch:460, Loss: 0.0013\n",
      "Epoch:1, Batch:461, Loss: 0.0013\n",
      "Epoch:1, Batch:462, Loss: 0.0013\n",
      "Epoch:1, Batch:463, Loss: 0.0013\n",
      "Epoch:1, Batch:464, Loss: 0.0013\n",
      "Epoch:1, Batch:465, Loss: 0.0012\n",
      "Epoch:1, Batch:466, Loss: 0.0013\n",
      "Epoch:1, Batch:467, Loss: 0.0013\n",
      "Epoch:1, Batch:468, Loss: 0.0013\n",
      "Epoch:1, Batch:469, Loss: 0.0013\n",
      "Epoch:1, Batch:470, Loss: 0.0012\n",
      "Epoch:1, Batch:471, Loss: 0.0013\n",
      "Epoch:1, Batch:472, Loss: 0.0013\n",
      "Epoch:1, Batch:473, Loss: 0.0014\n",
      "Epoch:1, Batch:474, Loss: 0.0014\n",
      "Epoch:1, Batch:475, Loss: 0.0013\n",
      "Epoch:1, Batch:476, Loss: 0.0012\n",
      "Epoch:1, Batch:477, Loss: 0.0013\n",
      "Epoch:1, Batch:478, Loss: 0.0013\n",
      "Epoch:1, Batch:479, Loss: 0.0013\n",
      "Epoch:1, Batch:480, Loss: 0.0013\n",
      "Epoch:1, Batch:481, Loss: 0.0014\n",
      "Epoch:1, Batch:482, Loss: 0.0013\n",
      "Epoch:1, Batch:483, Loss: 0.0012\n",
      "Epoch:1, Batch:484, Loss: 0.0012\n",
      "Epoch:1, Batch:485, Loss: 0.0013\n",
      "Epoch:1, Batch:486, Loss: 0.0013\n",
      "Epoch:1, Batch:487, Loss: 0.0012\n",
      "Epoch:1, Batch:488, Loss: 0.0013\n",
      "Epoch:1, Batch:489, Loss: 0.0012\n",
      "Epoch:1, Batch:490, Loss: 0.0012\n",
      "Epoch:1, Batch:491, Loss: 0.0013\n",
      "Epoch:1, Batch:492, Loss: 0.0012\n",
      "Epoch:1, Batch:493, Loss: 0.0012\n",
      "Epoch:1, Batch:494, Loss: 0.0013\n",
      "Epoch:1, Batch:495, Loss: 0.0011\n",
      "Epoch:1, Batch:496, Loss: 0.0012\n",
      "Epoch:1, Batch:497, Loss: 0.0013\n",
      "Epoch:1, Batch:498, Loss: 0.0013\n",
      "Epoch:1, Batch:499, Loss: 0.0013\n",
      "Epoch:1, Batch:500, Loss: 0.0013\n",
      "Epoch:1, Batch:501, Loss: 0.0012\n",
      "Epoch:1, Batch:502, Loss: 0.0013\n",
      "Epoch:1, Batch:503, Loss: 0.0012\n",
      "Epoch:1, Batch:504, Loss: 0.0013\n",
      "Epoch:1, Batch:505, Loss: 0.0012\n",
      "Epoch:1, Batch:506, Loss: 0.0013\n",
      "Epoch:1, Batch:507, Loss: 0.0012\n",
      "Epoch:1, Batch:508, Loss: 0.0013\n",
      "Epoch:1, Batch:509, Loss: 0.0013\n",
      "Epoch:1, Batch:510, Loss: 0.0013\n",
      "Epoch:1, Batch:511, Loss: 0.0014\n",
      "Epoch:1, Batch:512, Loss: 0.0012\n",
      "Epoch:1, Batch:513, Loss: 0.0012\n",
      "Epoch:1, Batch:514, Loss: 0.0012\n",
      "Epoch:1, Batch:515, Loss: 0.0013\n",
      "Epoch:1, Batch:516, Loss: 0.0012\n",
      "Epoch:1, Batch:517, Loss: 0.0013\n",
      "Epoch:1, Batch:518, Loss: 0.0013\n",
      "Epoch:1, Batch:519, Loss: 0.0012\n",
      "Epoch:1, Batch:520, Loss: 0.0013\n",
      "Epoch:1, Batch:521, Loss: 0.0012\n",
      "Epoch:1, Batch:522, Loss: 0.0012\n",
      "Epoch:1, Batch:523, Loss: 0.0012\n",
      "Epoch:1, Batch:524, Loss: 0.0013\n",
      "Epoch:1, Batch:525, Loss: 0.0011\n",
      "Epoch:1, Batch:526, Loss: 0.0012\n",
      "Epoch:1, Batch:527, Loss: 0.0012\n",
      "Epoch:1, Batch:528, Loss: 0.0012\n",
      "Epoch:1, Batch:529, Loss: 0.0012\n",
      "Epoch:1, Batch:530, Loss: 0.0012\n",
      "Epoch:1, Batch:531, Loss: 0.0012\n",
      "Epoch:1, Batch:532, Loss: 0.0013\n",
      "Epoch:1, Batch:533, Loss: 0.0012\n",
      "Epoch:1, Batch:534, Loss: 0.0012\n",
      "Epoch:1, Batch:535, Loss: 0.0012\n",
      "Epoch:1, Batch:536, Loss: 0.0012\n",
      "Epoch:1, Batch:537, Loss: 0.0012\n",
      "Epoch:1, Batch:538, Loss: 0.0013\n",
      "Epoch:1, Batch:539, Loss: 0.0012\n",
      "Epoch:1, Batch:540, Loss: 0.0012\n",
      "Epoch:1, Batch:541, Loss: 0.0014\n",
      "Epoch:1, Batch:542, Loss: 0.0012\n",
      "Epoch:1, Batch:543, Loss: 0.0012\n",
      "Epoch:1, Batch:544, Loss: 0.0012\n",
      "Epoch:1, Batch:545, Loss: 0.0012\n",
      "Epoch:1, Batch:546, Loss: 0.0013\n",
      "Epoch:1, Batch:547, Loss: 0.0013\n",
      "Epoch:1, Batch:548, Loss: 0.0013\n",
      "Epoch:1, Batch:549, Loss: 0.0011\n",
      "Epoch:1, Batch:550, Loss: 0.0013\n",
      "Epoch:1, Batch:551, Loss: 0.0013\n",
      "Epoch:1, Batch:552, Loss: 0.0013\n",
      "Epoch:1, Batch:553, Loss: 0.0012\n",
      "Epoch:1, Batch:554, Loss: 0.0011\n",
      "Epoch:1, Batch:555, Loss: 0.0012\n",
      "Epoch:1, Batch:556, Loss: 0.0013\n",
      "Epoch:1, Batch:557, Loss: 0.0013\n",
      "Epoch:1, Batch:558, Loss: 0.0013\n",
      "Epoch:1, Batch:559, Loss: 0.0012\n",
      "Epoch:1, Batch:560, Loss: 0.0013\n",
      "Epoch:1, Batch:561, Loss: 0.0013\n",
      "Epoch:1, Batch:562, Loss: 0.0012\n",
      "Epoch:1, Batch:563, Loss: 0.0012\n",
      "Epoch:1, Batch:564, Loss: 0.0012\n",
      "Epoch:1, Batch:565, Loss: 0.0012\n",
      "Epoch:1, Batch:566, Loss: 0.0012\n",
      "Epoch:1, Batch:567, Loss: 0.0012\n",
      "Epoch:1, Batch:568, Loss: 0.0012\n",
      "Epoch:1, Batch:569, Loss: 0.0011\n",
      "Epoch:1, Batch:570, Loss: 0.0013\n",
      "Epoch:1, Batch:571, Loss: 0.0012\n",
      "Epoch:1, Batch:572, Loss: 0.0012\n",
      "Epoch:1, Batch:573, Loss: 0.0013\n",
      "Epoch:1, Batch:574, Loss: 0.0012\n",
      "Epoch:1, Batch:575, Loss: 0.0013\n",
      "Epoch:1, Batch:576, Loss: 0.0013\n",
      "Epoch:1, Batch:577, Loss: 0.0012\n",
      "Epoch:1, Batch:578, Loss: 0.0013\n",
      "Epoch:1, Batch:579, Loss: 0.0013\n",
      "Epoch:1, Batch:580, Loss: 0.0013\n",
      "Epoch:1, Batch:581, Loss: 0.0012\n",
      "Epoch:1, Batch:582, Loss: 0.0013\n",
      "Epoch:1, Batch:583, Loss: 0.0013\n",
      "Epoch:1, Batch:584, Loss: 0.0012\n",
      "Epoch:1, Batch:585, Loss: 0.0013\n",
      "Epoch:1, Batch:586, Loss: 0.0011\n",
      "Epoch:1, Batch:587, Loss: 0.0013\n",
      "Epoch:1, Batch:588, Loss: 0.0011\n",
      "Epoch:1, Batch:589, Loss: 0.0012\n",
      "Epoch:1, Batch:590, Loss: 0.0013\n",
      "Epoch:1, Batch:591, Loss: 0.0012\n",
      "Epoch:1, Batch:592, Loss: 0.0012\n",
      "Epoch:1, Batch:593, Loss: 0.0013\n",
      "Epoch:1, Batch:594, Loss: 0.0013\n",
      "Epoch:1, Batch:595, Loss: 0.0011\n",
      "Epoch:1, Batch:596, Loss: 0.0013\n",
      "Epoch:1, Batch:597, Loss: 0.0011\n",
      "Epoch:1, Batch:598, Loss: 0.0011\n",
      "Epoch:1, Batch:599, Loss: 0.0012\n",
      "Epoch:1, Batch:600, Loss: 0.0017\n",
      "Epoch:1, Batch:601, Loss: 0.0012\n",
      "Epoch:1, Batch:602, Loss: 0.0010\n",
      "Epoch:1, Batch:603, Loss: 0.0013\n",
      "Epoch:1, Batch:604, Loss: 0.0011\n",
      "Epoch:1, Batch:605, Loss: 0.0012\n",
      "Epoch:1, Batch:606, Loss: 0.0012\n",
      "Epoch:1, Batch:607, Loss: 0.0013\n",
      "Epoch:1, Batch:608, Loss: 0.0015\n",
      "Epoch:1, Batch:609, Loss: 0.0015\n",
      "Epoch:1, Batch:610, Loss: 0.0013\n",
      "Epoch:1, Batch:611, Loss: 0.0011\n",
      "Epoch:1, Batch:612, Loss: 0.0013\n",
      "Epoch:1, Batch:613, Loss: 0.0013\n",
      "Epoch:1, Batch:614, Loss: 0.0012\n",
      "Epoch:1, Batch:615, Loss: 0.0013\n",
      "Epoch:1, Batch:616, Loss: 0.0011\n",
      "Epoch:1, Batch:617, Loss: 0.0013\n",
      "Epoch:1, Batch:618, Loss: 0.0012\n",
      "Epoch:1, Batch:619, Loss: 0.0012\n",
      "Epoch:1, Batch:620, Loss: 0.0014\n",
      "Epoch:1, Batch:621, Loss: 0.0013\n",
      "Epoch:1, Batch:622, Loss: 0.0012\n",
      "Epoch:1, Batch:623, Loss: 0.0012\n",
      "Epoch:1, Batch:624, Loss: 0.0015\n",
      "Epoch:1, Batch:625, Loss: 0.0013\n",
      "Epoch:1, Batch:626, Loss: 0.0014\n",
      "Epoch:1, Batch:627, Loss: 0.0014\n",
      "Epoch:1, Batch:628, Loss: 0.0013\n",
      "Epoch:1, Batch:629, Loss: 0.0012\n",
      "Epoch:1, Batch:630, Loss: 0.0013\n",
      "Epoch:1, Batch:631, Loss: 0.0012\n",
      "Epoch:1, Batch:632, Loss: 0.0012\n",
      "Epoch:1, Batch:633, Loss: 0.0013\n",
      "Epoch:1, Batch:634, Loss: 0.0014\n",
      "Epoch:1, Batch:635, Loss: 0.0014\n",
      "Epoch:1, Batch:636, Loss: 0.0013\n",
      "Epoch:1, Batch:637, Loss: 0.0012\n",
      "Epoch:1, Batch:638, Loss: 0.0013\n",
      "Epoch:1, Batch:639, Loss: 0.0012\n",
      "Epoch:1, Batch:640, Loss: 0.0013\n",
      "Epoch:1, Batch:641, Loss: 0.0014\n",
      "Epoch:1, Batch:642, Loss: 0.0012\n",
      "Epoch:1, Batch:643, Loss: 0.0011\n",
      "Epoch:1, Batch:644, Loss: 0.0013\n",
      "Epoch:1, Batch:645, Loss: 0.0013\n",
      "Epoch:1, Batch:646, Loss: 0.0014\n",
      "Epoch:1, Batch:647, Loss: 0.0013\n",
      "Epoch:1, Batch:648, Loss: 0.0012\n",
      "Epoch:1, Batch:649, Loss: 0.0012\n",
      "Epoch:1, Batch:650, Loss: 0.0011\n",
      "Epoch:1, Batch:651, Loss: 0.0012\n",
      "Epoch:1, Batch:652, Loss: 0.0013\n",
      "Epoch:1, Batch:653, Loss: 0.0013\n",
      "Epoch:1, Batch:654, Loss: 0.0014\n",
      "Epoch:1, Batch:655, Loss: 0.0011\n",
      "Epoch:1, Batch:656, Loss: 0.0012\n",
      "Epoch:1, Batch:657, Loss: 0.0013\n",
      "Epoch:1, Batch:658, Loss: 0.0012\n",
      "Epoch:1, Batch:659, Loss: 0.0012\n",
      "Epoch:1, Batch:660, Loss: 0.0011\n",
      "Epoch:1, Batch:661, Loss: 0.0013\n",
      "Epoch:1, Batch:662, Loss: 0.0012\n",
      "Epoch:1, Batch:663, Loss: 0.0012\n",
      "Epoch:1, Batch:664, Loss: 0.0013\n",
      "Epoch:1, Batch:665, Loss: 0.0012\n",
      "Epoch:1, Batch:666, Loss: 0.0013\n",
      "Epoch:1, Batch:667, Loss: 0.0012\n",
      "Epoch:1, Batch:668, Loss: 0.0010\n",
      "Epoch:1, Batch:669, Loss: 0.0012\n",
      "Epoch:1, Batch:670, Loss: 0.0012\n",
      "Epoch:1, Batch:671, Loss: 0.0014\n",
      "Epoch:1, Batch:672, Loss: 0.0013\n",
      "Epoch:1, Batch:673, Loss: 0.0013\n",
      "Epoch:1, Batch:674, Loss: 0.0012\n",
      "Epoch:1, Batch:675, Loss: 0.0010\n",
      "Epoch:1, Batch:676, Loss: 0.0013\n",
      "Epoch:1, Batch:677, Loss: 0.0014\n",
      "Epoch:1, Batch:678, Loss: 0.0012\n",
      "Epoch:1, Batch:679, Loss: 0.0014\n",
      "Epoch:1, Batch:680, Loss: 0.0012\n",
      "Epoch:1, Batch:681, Loss: 0.0013\n",
      "Epoch:1, Batch:682, Loss: 0.0011\n",
      "Epoch:1, Batch:683, Loss: 0.0013\n",
      "Epoch:1, Batch:684, Loss: 0.0012\n",
      "Epoch:1, Batch:685, Loss: 0.0012\n",
      "Epoch:1, Batch:686, Loss: 0.0013\n",
      "Epoch:1, Batch:687, Loss: 0.0013\n",
      "Epoch:1, Batch:688, Loss: 0.0012\n",
      "Epoch:1, Batch:689, Loss: 0.0012\n",
      "Epoch:1, Batch:690, Loss: 0.0010\n",
      "Epoch:1, Batch:691, Loss: 0.0012\n",
      "Epoch:1, Batch:692, Loss: 0.0013\n",
      "Epoch:1, Batch:693, Loss: 0.0013\n",
      "Epoch:1, Batch:694, Loss: 0.0011\n",
      "Epoch:1, Batch:695, Loss: 0.0018\n",
      "Epoch:1, Batch:696, Loss: 0.0013\n",
      "Epoch:1, Batch:697, Loss: 0.0011\n",
      "Epoch:1, Batch:698, Loss: 0.0016\n",
      "Epoch:1, Batch:699, Loss: 0.0012\n",
      "Epoch:1, Batch:700, Loss: 0.0013\n",
      "Epoch:1, Batch:701, Loss: 0.0013\n",
      "Epoch:1, Batch:702, Loss: 0.0014\n",
      "Epoch:1, Batch:703, Loss: 0.0012\n",
      "Epoch:1, Batch:704, Loss: 0.0011\n",
      "Epoch:1, Batch:705, Loss: 0.0014\n",
      "Epoch:1, Batch:706, Loss: 0.0013\n",
      "Epoch:1, Batch:707, Loss: 0.0011\n",
      "Epoch:1, Batch:708, Loss: 0.0014\n",
      "Epoch:1, Batch:709, Loss: 0.0011\n",
      "Epoch:1, Batch:710, Loss: 0.0013\n",
      "Epoch:1, Batch:711, Loss: 0.0014\n",
      "Epoch:1, Batch:712, Loss: 0.0010\n",
      "Epoch:1, Batch:713, Loss: 0.0011\n",
      "Epoch:1, Batch:714, Loss: 0.0014\n",
      "Epoch:1, Batch:715, Loss: 0.0015\n",
      "Epoch:1, Batch:716, Loss: 0.0011\n",
      "Epoch:1, Batch:717, Loss: 0.0014\n",
      "Epoch:1, Batch:718, Loss: 0.0018\n",
      "Epoch:1, Batch:719, Loss: 0.0015\n",
      "Epoch:1, Batch:720, Loss: 0.0014\n",
      "Epoch:1, Batch:721, Loss: 0.0011\n",
      "Epoch:1, Batch:722, Loss: 0.0016\n",
      "Epoch:1, Batch:723, Loss: 0.0011\n",
      "Epoch:1, Batch:724, Loss: 0.0013\n",
      "Epoch:1, Batch:725, Loss: 0.0012\n",
      "Epoch:1, Batch:726, Loss: 0.0013\n",
      "Epoch:1, Batch:727, Loss: 0.0013\n",
      "Epoch:1, Batch:728, Loss: 0.0020\n",
      "Epoch:1, Batch:729, Loss: 0.0016\n",
      "Epoch:1, Batch:730, Loss: 0.0015\n",
      "Epoch:1, Batch:731, Loss: 0.0019\n",
      "Epoch:1, Batch:732, Loss: 0.0018\n",
      "Epoch:1, Batch:733, Loss: 0.0013\n",
      "Epoch:1, Batch:734, Loss: 0.0016\n",
      "Epoch:1, Batch:735, Loss: 0.0013\n",
      "Epoch:1, Batch:736, Loss: 0.0013\n",
      "Epoch:1, Batch:737, Loss: 0.0013\n",
      "Epoch:1, Batch:738, Loss: 0.0013\n",
      "Epoch:1, Batch:739, Loss: 0.0013\n",
      "Epoch:1, Batch:740, Loss: 0.0012\n",
      "Epoch:1, Batch:741, Loss: 0.0014\n",
      "Epoch:1, Batch:742, Loss: 0.0011\n",
      "Epoch:1, Batch:743, Loss: 0.0014\n",
      "Epoch:1, Batch:744, Loss: 0.0013\n",
      "Epoch:1, Batch:745, Loss: 0.0013\n",
      "Epoch:1, Batch:746, Loss: 0.0014\n",
      "Epoch:1, Batch:747, Loss: 0.0010\n",
      "Epoch:1, Batch:748, Loss: 0.0012\n",
      "Epoch:1, Batch:749, Loss: 0.0015\n",
      "Epoch:1, Batch:750, Loss: 0.0012\n",
      "Epoch:1, Batch:751, Loss: 0.0013\n",
      "Epoch:1, Batch:752, Loss: 0.0012\n",
      "Epoch:1, Batch:753, Loss: 0.0013\n",
      "Epoch:1, Batch:754, Loss: 0.0011\n",
      "Epoch:1, Batch:755, Loss: 0.0015\n",
      "Epoch:1, Batch:756, Loss: 0.0011\n",
      "Epoch:1, Batch:757, Loss: 0.0012\n",
      "Epoch:1, Batch:758, Loss: 0.0013\n",
      "Epoch:1, Batch:759, Loss: 0.0012\n",
      "Epoch:1, Batch:760, Loss: 0.0015\n",
      "Epoch:1, Batch:761, Loss: 0.0013\n",
      "Epoch:1, Batch:762, Loss: 0.0011\n",
      "Epoch:1, Batch:763, Loss: 0.0014\n",
      "Epoch:1, Batch:764, Loss: 0.0013\n",
      "Epoch:1, Batch:765, Loss: 0.0011\n",
      "Epoch:1, Batch:766, Loss: 0.0013\n",
      "Epoch:1, Batch:767, Loss: 0.0016\n",
      "Epoch:1, Batch:768, Loss: 0.0014\n",
      "Epoch:1, Batch:769, Loss: 0.0014\n",
      "Epoch:1, Batch:770, Loss: 0.0009\n",
      "Epoch:1, Batch:771, Loss: 0.0012\n",
      "Epoch:1, Batch:772, Loss: 0.0014\n",
      "Epoch:1, Batch:773, Loss: 0.0014\n",
      "Epoch:1, Batch:774, Loss: 0.0012\n",
      "Epoch:1, Batch:775, Loss: 0.0015\n",
      "Epoch:1, Batch:776, Loss: 0.0013\n",
      "Epoch:1, Batch:777, Loss: 0.0014\n",
      "Epoch:1, Batch:778, Loss: 0.0011\n",
      "Epoch:1, Batch:779, Loss: 0.0012\n",
      "Epoch:1, Batch:780, Loss: 0.0012\n",
      "Epoch:1, Batch:781, Loss: 0.0012\n",
      "Epoch:1, Batch:782, Loss: 0.0013\n",
      "Epoch:1, Batch:783, Loss: 0.0010\n",
      "Epoch:1, Batch:784, Loss: 0.0014\n",
      "Epoch:1, Batch:785, Loss: 0.0014\n",
      "Epoch:1, Batch:786, Loss: 0.0012\n",
      "Epoch:1, Batch:787, Loss: 0.0013\n",
      "Epoch:1, Batch:788, Loss: 0.0012\n",
      "Epoch:1, Batch:789, Loss: 0.0016\n",
      "Epoch:1, Batch:790, Loss: 0.0012\n",
      "Epoch:1, Batch:791, Loss: 0.0012\n",
      "Epoch:1, Batch:792, Loss: 0.0014\n",
      "Epoch:1, Batch:793, Loss: 0.0013\n",
      "Epoch:1, Batch:794, Loss: 0.0015\n",
      "Epoch:1, Batch:795, Loss: 0.0009\n",
      "Epoch:1, Batch:796, Loss: 0.0016\n",
      "Epoch:1, Batch:797, Loss: 0.0012\n",
      "Epoch:1, Batch:798, Loss: 0.0014\n",
      "Epoch:1, Batch:799, Loss: 0.0017\n",
      "Epoch:1, Batch:800, Loss: 0.0013\n",
      "Epoch:1, Batch:801, Loss: 0.0014\n",
      "Epoch:1, Batch:802, Loss: 0.0008\n",
      "Epoch:1, Batch:803, Loss: 0.0017\n",
      "Epoch:1, Batch:804, Loss: 0.0013\n",
      "Epoch:1, Batch:805, Loss: 0.0013\n",
      "Epoch:1, Batch:806, Loss: 0.0015\n",
      "Epoch:1, Batch:807, Loss: 0.0014\n",
      "Epoch:1, Batch:808, Loss: 0.0013\n",
      "Epoch:1, Batch:809, Loss: 0.0014\n",
      "Epoch:1, Batch:810, Loss: 0.0013\n",
      "Epoch:1, Batch:811, Loss: 0.0012\n",
      "Epoch:1, Batch:812, Loss: 0.0012\n",
      "Epoch:1, Batch:813, Loss: 0.0011\n",
      "Epoch:1, Batch:814, Loss: 0.0013\n",
      "Epoch:1, Batch:815, Loss: 0.0011\n",
      "Epoch:1, Batch:816, Loss: 0.0015\n",
      "Epoch:1, Batch:817, Loss: 0.0013\n",
      "Epoch:1, Batch:818, Loss: 0.0014\n",
      "Epoch:1, Batch:819, Loss: 0.0014\n",
      "Epoch:1, Batch:820, Loss: 0.0013\n",
      "Epoch:1, Batch:821, Loss: 0.0012\n",
      "Epoch:1, Batch:822, Loss: 0.0012\n",
      "Epoch:1, Batch:823, Loss: 0.0014\n",
      "Epoch:1, Batch:824, Loss: 0.0013\n",
      "Epoch:1, Batch:825, Loss: 0.0011\n",
      "Epoch:1, Batch:826, Loss: 0.0014\n",
      "Epoch:1, Batch:827, Loss: 0.0014\n",
      "Epoch:1, Batch:828, Loss: 0.0013\n",
      "Epoch:1, Batch:829, Loss: 0.0013\n",
      "Epoch:1, Batch:830, Loss: 0.0012\n",
      "Epoch:1, Batch:831, Loss: 0.0012\n",
      "Epoch:1, Batch:832, Loss: 0.0012\n",
      "Epoch:1, Batch:833, Loss: 0.0013\n",
      "Epoch:1, Batch:834, Loss: 0.0012\n",
      "Epoch:1, Batch:835, Loss: 0.0012\n",
      "Epoch:1, Batch:836, Loss: 0.0012\n",
      "Epoch:1, Batch:837, Loss: 0.0013\n",
      "Epoch:1, Batch:838, Loss: 0.0012\n",
      "Epoch:1, Batch:839, Loss: 0.0013\n",
      "Epoch:1, Batch:840, Loss: 0.0015\n",
      "Epoch:1, Batch:841, Loss: 0.0012\n",
      "Epoch:1, Batch:842, Loss: 0.0013\n",
      "Epoch:1, Batch:843, Loss: 0.0012\n",
      "Epoch:1, Batch:844, Loss: 0.0013\n",
      "Epoch:1, Batch:845, Loss: 0.0013\n",
      "Epoch:1, Batch:846, Loss: 0.0013\n",
      "Epoch:1, Batch:847, Loss: 0.0012\n",
      "Epoch:1, Batch:848, Loss: 0.0012\n",
      "Epoch:1, Batch:849, Loss: 0.0012\n",
      "Epoch:1, Batch:850, Loss: 0.0010\n",
      "Epoch:1, Batch:851, Loss: 0.0012\n",
      "Epoch:1, Batch:852, Loss: 0.0014\n",
      "Epoch:1, Batch:853, Loss: 0.0010\n",
      "Epoch:1, Batch:854, Loss: 0.0013\n",
      "Epoch:1, Batch:855, Loss: 0.0016\n",
      "Epoch:1, Batch:856, Loss: 0.0012\n",
      "Epoch:1, Batch:857, Loss: 0.0015\n",
      "Epoch:1, Batch:858, Loss: 0.0013\n",
      "Epoch:1, Batch:859, Loss: 0.0012\n",
      "Epoch:1, Batch:860, Loss: 0.0013\n",
      "Epoch:1, Batch:861, Loss: 0.0014\n",
      "Epoch:1, Batch:862, Loss: 0.0014\n",
      "Epoch:1, Batch:863, Loss: 0.0016\n",
      "Epoch:1, Batch:864, Loss: 0.0011\n",
      "Epoch:1, Batch:865, Loss: 0.0016\n",
      "Epoch:1, Batch:866, Loss: 0.0021\n",
      "Epoch:1, Batch:867, Loss: 0.0018\n",
      "Epoch:1, Batch:868, Loss: 0.0007\n",
      "Epoch:1, Batch:869, Loss: 0.0019\n",
      "Epoch:1, Batch:870, Loss: 0.0012\n",
      "Epoch:1, Batch:871, Loss: 0.0019\n",
      "Epoch:1, Batch:872, Loss: 0.0013\n",
      "Epoch:1, Batch:873, Loss: 0.0015\n",
      "Epoch:1, Batch:874, Loss: 0.0006\n",
      "Epoch:1, Batch:875, Loss: 0.0013\n",
      "Epoch:1, Batch:876, Loss: 0.0011\n",
      "Epoch:1, Batch:877, Loss: 0.0012\n",
      "Epoch:1, Batch:878, Loss: 0.0013\n",
      "Epoch:1, Batch:879, Loss: 0.0021\n",
      "Epoch:1, Batch:880, Loss: 0.0018\n",
      "Epoch:1, Batch:881, Loss: 0.0007\n",
      "Epoch:1, Batch:882, Loss: 0.0022\n",
      "Epoch:1, Batch:883, Loss: 0.0015\n",
      "Epoch:1, Batch:884, Loss: 0.0012\n",
      "Epoch:1, Batch:885, Loss: 0.0016\n",
      "Epoch:1, Batch:886, Loss: 0.0014\n",
      "Epoch:1, Batch:887, Loss: 0.0013\n",
      "Epoch:1, Batch:888, Loss: 0.0014\n",
      "Epoch:1, Batch:889, Loss: 0.0016\n",
      "Epoch:1, Batch:890, Loss: 0.0012\n",
      "Epoch:1, Batch:891, Loss: 0.0016\n",
      "Epoch:1, Batch:892, Loss: 0.0011\n",
      "Epoch:1, Batch:893, Loss: 0.0020\n",
      "Epoch:1, Batch:894, Loss: 0.0018\n",
      "Epoch:1, Batch:895, Loss: 0.0013\n",
      "Epoch:1, Batch:896, Loss: 0.0012\n",
      "Epoch:1, Batch:897, Loss: 0.0013\n",
      "Epoch:1, Batch:898, Loss: 0.0013\n",
      "Epoch:1, Batch:899, Loss: 0.0019\n",
      "Epoch:1, Batch:900, Loss: 0.0011\n",
      "Epoch:1, Batch:901, Loss: 0.0014\n",
      "Epoch:1, Batch:902, Loss: 0.0013\n",
      "Epoch:1, Batch:903, Loss: 0.0014\n",
      "Epoch:1, Batch:904, Loss: 0.0015\n",
      "Epoch:1, Batch:905, Loss: 0.0015\n",
      "Epoch:1, Batch:906, Loss: 0.0013\n",
      "Epoch:1, Batch:907, Loss: 0.0013\n",
      "Epoch:1, Batch:908, Loss: 0.0015\n",
      "Epoch:1, Batch:909, Loss: 0.0012\n",
      "Epoch:1, Batch:910, Loss: 0.0014\n",
      "Epoch:1, Batch:911, Loss: 0.0014\n",
      "Epoch:1, Batch:912, Loss: 0.0014\n",
      "Epoch:1, Batch:913, Loss: 0.0012\n",
      "Epoch:1, Batch:914, Loss: 0.0014\n",
      "Epoch:1, Batch:915, Loss: 0.0012\n",
      "Epoch:1, Batch:916, Loss: 0.0013\n",
      "Epoch:1, Batch:917, Loss: 0.0011\n",
      "Epoch:1, Batch:918, Loss: 0.0017\n",
      "Epoch:1, Batch:919, Loss: 0.0012\n",
      "Epoch:1, Batch:920, Loss: 0.0014\n",
      "Epoch:1, Batch:921, Loss: 0.0015\n",
      "Epoch:1, Batch:922, Loss: 0.0013\n",
      "Epoch:1, Batch:923, Loss: 0.0013\n",
      "Epoch:1, Batch:924, Loss: 0.0020\n",
      "Epoch:1, Batch:925, Loss: 0.0016\n",
      "Epoch:1, Batch:926, Loss: 0.0013\n",
      "Epoch:1, Batch:927, Loss: 0.0014\n",
      "Epoch:1, Batch:928, Loss: 0.0011\n",
      "Epoch:1, Batch:929, Loss: 0.0019\n",
      "Epoch:1, Batch:930, Loss: 0.0016\n",
      "Training time: 36.045011043548584\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:22:41.755422Z",
     "start_time": "2025-05-14T15:22:40.539433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in x_test_loader:\n",
    "        #train_predict = model(x_train)\n",
    "        batch = batch.reshape(batch_size, 1, input_dim)\n",
    "        test_predict = model(batch)\n",
    "        print(test_predict)"
   ],
   "id": "1801c4056f6112fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4564],\n",
      "        [-0.4563],\n",
      "        [-0.4558],\n",
      "        [-0.4559],\n",
      "        [-0.4557]])\n",
      "tensor([[-0.4556],\n",
      "        [-0.4557],\n",
      "        [-0.4563],\n",
      "        [-0.4563],\n",
      "        [-0.4565]])\n",
      "tensor([[-0.4566],\n",
      "        [-0.4565],\n",
      "        [-0.4565],\n",
      "        [-0.4566],\n",
      "        [-0.4566]])\n",
      "tensor([[-0.4567],\n",
      "        [-0.4562],\n",
      "        [-0.4560],\n",
      "        [-0.4557],\n",
      "        [-0.4555]])\n",
      "tensor([[-0.4556],\n",
      "        [-0.4559],\n",
      "        [-0.4561],\n",
      "        [-0.4567],\n",
      "        [-0.4565]])\n",
      "tensor([[-0.4564],\n",
      "        [-0.4561],\n",
      "        [-0.4563],\n",
      "        [-0.4562],\n",
      "        [-0.4557]])\n",
      "tensor([[-0.4551],\n",
      "        [-0.4550],\n",
      "        [-0.4549],\n",
      "        [-0.4553],\n",
      "        [-0.4554]])\n",
      "tensor([[-0.4552],\n",
      "        [-0.4551],\n",
      "        [-0.4546],\n",
      "        [-0.4544],\n",
      "        [-0.4549]])\n",
      "tensor([[-0.4545],\n",
      "        [-0.4550],\n",
      "        [-0.4533],\n",
      "        [-0.4533],\n",
      "        [-0.4539]])\n",
      "tensor([[-0.4541],\n",
      "        [-0.4537],\n",
      "        [-0.4536],\n",
      "        [-0.4536],\n",
      "        [-0.4538]])\n",
      "tensor([[-0.4535],\n",
      "        [-0.4533],\n",
      "        [-0.4531],\n",
      "        [-0.4525],\n",
      "        [-0.4526]])\n",
      "tensor([[-0.4526],\n",
      "        [-0.4530],\n",
      "        [-0.4528],\n",
      "        [-0.4525],\n",
      "        [-0.4523]])\n",
      "tensor([[-0.4521],\n",
      "        [-0.4522],\n",
      "        [-0.4521],\n",
      "        [-0.4520],\n",
      "        [-0.4518]])\n",
      "tensor([[-0.4520],\n",
      "        [-0.4522],\n",
      "        [-0.4520],\n",
      "        [-0.4521],\n",
      "        [-0.4527]])\n",
      "tensor([[-0.4527],\n",
      "        [-0.4528],\n",
      "        [-0.4530],\n",
      "        [-0.4527],\n",
      "        [-0.4528]])\n",
      "tensor([[-0.4528],\n",
      "        [-0.4528],\n",
      "        [-0.4527],\n",
      "        [-0.4528],\n",
      "        [-0.4529]])\n",
      "tensor([[-0.4527],\n",
      "        [-0.4512],\n",
      "        [-0.4514],\n",
      "        [-0.4517],\n",
      "        [-0.4515]])\n",
      "tensor([[-0.4514],\n",
      "        [-0.4516],\n",
      "        [-0.4513],\n",
      "        [-0.4512],\n",
      "        [-0.4511]])\n",
      "tensor([[-0.4513],\n",
      "        [-0.4510],\n",
      "        [-0.4512],\n",
      "        [-0.4513],\n",
      "        [-0.4511]])\n",
      "tensor([[-0.4509],\n",
      "        [-0.4512],\n",
      "        [-0.4513],\n",
      "        [-0.4506],\n",
      "        [-0.4509]])\n",
      "tensor([[-0.4505],\n",
      "        [-0.4502],\n",
      "        [-0.4504],\n",
      "        [-0.4499],\n",
      "        [-0.4500]])\n",
      "tensor([[-0.4510],\n",
      "        [-0.4511],\n",
      "        [-0.4508],\n",
      "        [-0.4506],\n",
      "        [-0.4504]])\n",
      "tensor([[-0.4508],\n",
      "        [-0.4498],\n",
      "        [-0.4493],\n",
      "        [-0.4493],\n",
      "        [-0.4486]])\n",
      "tensor([[-0.4491],\n",
      "        [-0.4489],\n",
      "        [-0.4485],\n",
      "        [-0.4482],\n",
      "        [-0.4477]])\n",
      "tensor([[-0.4483],\n",
      "        [-0.4479],\n",
      "        [-0.4472],\n",
      "        [-0.4474],\n",
      "        [-0.4471]])\n",
      "tensor([[-0.4473],\n",
      "        [-0.4473],\n",
      "        [-0.4474],\n",
      "        [-0.4476],\n",
      "        [-0.4471]])\n",
      "tensor([[-0.4470],\n",
      "        [-0.4472],\n",
      "        [-0.4479],\n",
      "        [-0.4485],\n",
      "        [-0.4485]])\n",
      "tensor([[-0.4481],\n",
      "        [-0.4487],\n",
      "        [-0.4489],\n",
      "        [-0.4477],\n",
      "        [-0.4478]])\n",
      "tensor([[-0.4478],\n",
      "        [-0.4479],\n",
      "        [-0.4480],\n",
      "        [-0.4480],\n",
      "        [-0.4481]])\n",
      "tensor([[-0.4482],\n",
      "        [-0.4478],\n",
      "        [-0.4482],\n",
      "        [-0.4476],\n",
      "        [-0.4475]])\n",
      "tensor([[-0.4472],\n",
      "        [-0.4472],\n",
      "        [-0.4478],\n",
      "        [-0.4472],\n",
      "        [-0.4478]])\n",
      "tensor([[-0.4480],\n",
      "        [-0.4476],\n",
      "        [-0.4485],\n",
      "        [-0.4483],\n",
      "        [-0.4479]])\n",
      "tensor([[-0.4483],\n",
      "        [-0.4489],\n",
      "        [-0.4483],\n",
      "        [-0.4484],\n",
      "        [-0.4485]])\n",
      "tensor([[-0.4485],\n",
      "        [-0.4490],\n",
      "        [-0.4489],\n",
      "        [-0.4490],\n",
      "        [-0.4492]])\n",
      "tensor([[-0.4489],\n",
      "        [-0.4488],\n",
      "        [-0.4483],\n",
      "        [-0.4483],\n",
      "        [-0.4483]])\n",
      "tensor([[-0.4481],\n",
      "        [-0.4482],\n",
      "        [-0.4486],\n",
      "        [-0.4483],\n",
      "        [-0.4481]])\n",
      "tensor([[-0.4479],\n",
      "        [-0.4473],\n",
      "        [-0.4474],\n",
      "        [-0.4473],\n",
      "        [-0.4476]])\n",
      "tensor([[-0.4480],\n",
      "        [-0.4474],\n",
      "        [-0.4469],\n",
      "        [-0.4472],\n",
      "        [-0.4469]])\n",
      "tensor([[-0.4472],\n",
      "        [-0.4471],\n",
      "        [-0.4469],\n",
      "        [-0.4479],\n",
      "        [-0.4482]])\n",
      "tensor([[-0.4479],\n",
      "        [-0.4485],\n",
      "        [-0.4482],\n",
      "        [-0.4481],\n",
      "        [-0.4484]])\n",
      "tensor([[-0.4490],\n",
      "        [-0.4487],\n",
      "        [-0.4482],\n",
      "        [-0.4479],\n",
      "        [-0.4482]])\n",
      "tensor([[-0.4483],\n",
      "        [-0.4487],\n",
      "        [-0.4489],\n",
      "        [-0.4483],\n",
      "        [-0.4486]])\n",
      "tensor([[-0.4491],\n",
      "        [-0.4492],\n",
      "        [-0.4502],\n",
      "        [-0.4504],\n",
      "        [-0.4503]])\n",
      "tensor([[-0.4504],\n",
      "        [-0.4474],\n",
      "        [-0.4478],\n",
      "        [-0.4473],\n",
      "        [-0.4470]])\n",
      "tensor([[-0.4466],\n",
      "        [-0.4458],\n",
      "        [-0.4461],\n",
      "        [-0.4456],\n",
      "        [-0.4454]])\n",
      "tensor([[-0.4451],\n",
      "        [-0.4444],\n",
      "        [-0.4439],\n",
      "        [-0.4435],\n",
      "        [-0.4439]])\n",
      "tensor([[-0.4433],\n",
      "        [-0.4430],\n",
      "        [-0.4431],\n",
      "        [-0.4435],\n",
      "        [-0.4437]])\n",
      "tensor([[-0.4440],\n",
      "        [-0.4440],\n",
      "        [-0.4450],\n",
      "        [-0.4446],\n",
      "        [-0.4440]])\n",
      "tensor([[-0.4436],\n",
      "        [-0.4433],\n",
      "        [-0.4438],\n",
      "        [-0.4437],\n",
      "        [-0.4441]])\n",
      "tensor([[-0.4437],\n",
      "        [-0.4439],\n",
      "        [-0.4435],\n",
      "        [-0.4437],\n",
      "        [-0.4437]])\n",
      "tensor([[-0.4436],\n",
      "        [-0.4435],\n",
      "        [-0.4441],\n",
      "        [-0.4437],\n",
      "        [-0.4436]])\n",
      "tensor([[-0.4432],\n",
      "        [-0.4437],\n",
      "        [-0.4440],\n",
      "        [-0.4444],\n",
      "        [-0.4446]])\n",
      "tensor([[-0.4438],\n",
      "        [-0.4427],\n",
      "        [-0.4423],\n",
      "        [-0.4420],\n",
      "        [-0.4428]])\n",
      "tensor([[-0.4429],\n",
      "        [-0.4435],\n",
      "        [-0.4426],\n",
      "        [-0.4426],\n",
      "        [-0.4423]])\n",
      "tensor([[-0.4430],\n",
      "        [-0.4425],\n",
      "        [-0.4429],\n",
      "        [-0.4426],\n",
      "        [-0.4430]])\n",
      "tensor([[-0.4431],\n",
      "        [-0.4422],\n",
      "        [-0.4420],\n",
      "        [-0.4418],\n",
      "        [-0.4417]])\n",
      "tensor([[-0.4421],\n",
      "        [-0.4432],\n",
      "        [-0.4439],\n",
      "        [-0.4445],\n",
      "        [-0.4450]])\n",
      "tensor([[-0.4450],\n",
      "        [-0.4447],\n",
      "        [-0.4449],\n",
      "        [-0.4447],\n",
      "        [-0.4457]])\n",
      "tensor([[-0.4460],\n",
      "        [-0.4458],\n",
      "        [-0.4452],\n",
      "        [-0.4452],\n",
      "        [-0.4455]])\n",
      "tensor([[-0.4457],\n",
      "        [-0.4447],\n",
      "        [-0.4452],\n",
      "        [-0.4452],\n",
      "        [-0.4452]])\n",
      "tensor([[-0.4449],\n",
      "        [-0.4459],\n",
      "        [-0.4456],\n",
      "        [-0.4459],\n",
      "        [-0.4459]])\n",
      "tensor([[-0.4459],\n",
      "        [-0.4460],\n",
      "        [-0.4458],\n",
      "        [-0.4460],\n",
      "        [-0.4460]])\n",
      "tensor([[-0.4449],\n",
      "        [-0.4451],\n",
      "        [-0.4453],\n",
      "        [-0.4448],\n",
      "        [-0.4440]])\n",
      "tensor([[-0.4440],\n",
      "        [-0.4440],\n",
      "        [-0.4444],\n",
      "        [-0.4445],\n",
      "        [-0.4452]])\n",
      "tensor([[-0.4454],\n",
      "        [-0.4452],\n",
      "        [-0.4461],\n",
      "        [-0.4449],\n",
      "        [-0.4453]])\n",
      "tensor([[-0.4454],\n",
      "        [-0.4448],\n",
      "        [-0.4449],\n",
      "        [-0.4451],\n",
      "        [-0.4449]])\n",
      "tensor([[-0.4453],\n",
      "        [-0.4452],\n",
      "        [-0.4449],\n",
      "        [-0.4460],\n",
      "        [-0.4458]])\n",
      "tensor([[-0.4464],\n",
      "        [-0.4461],\n",
      "        [-0.4463],\n",
      "        [-0.4460],\n",
      "        [-0.4457]])\n",
      "tensor([[-0.4456],\n",
      "        [-0.4463],\n",
      "        [-0.4463],\n",
      "        [-0.4466],\n",
      "        [-0.4473]])\n",
      "tensor([[-0.4469],\n",
      "        [-0.4470],\n",
      "        [-0.4465],\n",
      "        [-0.4481],\n",
      "        [-0.4468]])\n",
      "tensor([[-0.4469],\n",
      "        [-0.4476],\n",
      "        [-0.4482],\n",
      "        [-0.4473],\n",
      "        [-0.4471]])\n",
      "tensor([[-0.4472],\n",
      "        [-0.4471],\n",
      "        [-0.4451],\n",
      "        [-0.4473],\n",
      "        [-0.4478]])\n",
      "tensor([[-0.4474],\n",
      "        [-0.4464],\n",
      "        [-0.4460],\n",
      "        [-0.4463],\n",
      "        [-0.4462]])\n",
      "tensor([[-0.4464],\n",
      "        [-0.4469],\n",
      "        [-0.4463],\n",
      "        [-0.4459],\n",
      "        [-0.4463]])\n",
      "tensor([[-0.4472],\n",
      "        [-0.4466],\n",
      "        [-0.4471],\n",
      "        [-0.4479],\n",
      "        [-0.4489]])\n",
      "tensor([[-0.4482],\n",
      "        [-0.4474],\n",
      "        [-0.4475],\n",
      "        [-0.4475],\n",
      "        [-0.4468]])\n",
      "tensor([[-0.4469],\n",
      "        [-0.4461],\n",
      "        [-0.4461],\n",
      "        [-0.4459],\n",
      "        [-0.4460]])\n",
      "tensor([[-0.4456],\n",
      "        [-0.4464],\n",
      "        [-0.4458],\n",
      "        [-0.4456],\n",
      "        [-0.4461]])\n",
      "tensor([[-0.4455],\n",
      "        [-0.4459],\n",
      "        [-0.4466],\n",
      "        [-0.4465],\n",
      "        [-0.4465]])\n",
      "tensor([[-0.4466],\n",
      "        [-0.4466],\n",
      "        [-0.4468],\n",
      "        [-0.4468],\n",
      "        [-0.4468]])\n",
      "tensor([[-0.4471],\n",
      "        [-0.4468],\n",
      "        [-0.4466],\n",
      "        [-0.4465],\n",
      "        [-0.4459]])\n",
      "tensor([[-0.4450],\n",
      "        [-0.4456],\n",
      "        [-0.4461],\n",
      "        [-0.4465],\n",
      "        [-0.4468]])\n",
      "tensor([[-0.4467],\n",
      "        [-0.4473],\n",
      "        [-0.4480],\n",
      "        [-0.4475],\n",
      "        [-0.4471]])\n",
      "tensor([[-0.4464],\n",
      "        [-0.4473],\n",
      "        [-0.4479],\n",
      "        [-0.4483],\n",
      "        [-0.4487]])\n",
      "tensor([[-0.4492],\n",
      "        [-0.4490],\n",
      "        [-0.4486],\n",
      "        [-0.4488],\n",
      "        [-0.4478]])\n",
      "tensor([[-0.4479],\n",
      "        [-0.4481],\n",
      "        [-0.4484],\n",
      "        [-0.4481],\n",
      "        [-0.4491]])\n",
      "tensor([[-0.4482],\n",
      "        [-0.4476],\n",
      "        [-0.4474],\n",
      "        [-0.4475],\n",
      "        [-0.4479]])\n",
      "tensor([[-0.4477],\n",
      "        [-0.4482],\n",
      "        [-0.4480],\n",
      "        [-0.4478],\n",
      "        [-0.4482]])\n",
      "tensor([[-0.4484],\n",
      "        [-0.4496],\n",
      "        [-0.4501],\n",
      "        [-0.4503],\n",
      "        [-0.4502]])\n",
      "tensor([[-0.4514],\n",
      "        [-0.4510],\n",
      "        [-0.4509],\n",
      "        [-0.4509],\n",
      "        [-0.4512]])\n",
      "tensor([[-0.4507],\n",
      "        [-0.4506],\n",
      "        [-0.4508],\n",
      "        [-0.4510],\n",
      "        [-0.4511]])\n",
      "tensor([[-0.4511],\n",
      "        [-0.4511],\n",
      "        [-0.4512],\n",
      "        [-0.4508],\n",
      "        [-0.4507]])\n",
      "tensor([[-0.4509],\n",
      "        [-0.4508],\n",
      "        [-0.4508],\n",
      "        [-0.4510],\n",
      "        [-0.4504]])\n",
      "tensor([[-0.4501],\n",
      "        [-0.4495],\n",
      "        [-0.4493],\n",
      "        [-0.4495],\n",
      "        [-0.4493]])\n",
      "tensor([[-0.4495],\n",
      "        [-0.4497],\n",
      "        [-0.4494],\n",
      "        [-0.4494],\n",
      "        [-0.4494]])\n",
      "tensor([[-0.4496],\n",
      "        [-0.4500],\n",
      "        [-0.4496],\n",
      "        [-0.4497],\n",
      "        [-0.4500]])\n",
      "tensor([[-0.4497],\n",
      "        [-0.4500],\n",
      "        [-0.4495],\n",
      "        [-0.4492],\n",
      "        [-0.4491]])\n",
      "tensor([[-0.4493],\n",
      "        [-0.4494],\n",
      "        [-0.4498],\n",
      "        [-0.4498],\n",
      "        [-0.4497]])\n",
      "tensor([[-0.4506],\n",
      "        [-0.4503],\n",
      "        [-0.4505],\n",
      "        [-0.4500],\n",
      "        [-0.4502]])\n",
      "tensor([[-0.4502],\n",
      "        [-0.4501],\n",
      "        [-0.4505],\n",
      "        [-0.4507],\n",
      "        [-0.4507]])\n",
      "tensor([[-0.4505],\n",
      "        [-0.4501],\n",
      "        [-0.4500],\n",
      "        [-0.4496],\n",
      "        [-0.4494]])\n",
      "tensor([[-0.4502],\n",
      "        [-0.4505],\n",
      "        [-0.4502],\n",
      "        [-0.4505],\n",
      "        [-0.4501]])\n",
      "tensor([[-0.4506],\n",
      "        [-0.4510],\n",
      "        [-0.4513],\n",
      "        [-0.4515],\n",
      "        [-0.4517]])\n",
      "tensor([[-0.4517],\n",
      "        [-0.4512],\n",
      "        [-0.4515],\n",
      "        [-0.4514],\n",
      "        [-0.4509]])\n",
      "tensor([[-0.4502],\n",
      "        [-0.4501],\n",
      "        [-0.4503],\n",
      "        [-0.4507],\n",
      "        [-0.4509]])\n",
      "tensor([[-0.4508],\n",
      "        [-0.4508],\n",
      "        [-0.4500],\n",
      "        [-0.4499],\n",
      "        [-0.4494]])\n",
      "tensor([[-0.4489],\n",
      "        [-0.4496],\n",
      "        [-0.4502],\n",
      "        [-0.4498],\n",
      "        [-0.4492]])\n",
      "tensor([[-0.4487],\n",
      "        [-0.4486],\n",
      "        [-0.4490],\n",
      "        [-0.4485],\n",
      "        [-0.4487]])\n",
      "tensor([[-0.4487],\n",
      "        [-0.4495],\n",
      "        [-0.4496],\n",
      "        [-0.4489],\n",
      "        [-0.4488]])\n",
      "tensor([[-0.4481],\n",
      "        [-0.4484],\n",
      "        [-0.4476],\n",
      "        [-0.4470],\n",
      "        [-0.4470]])\n",
      "tensor([[-0.4468],\n",
      "        [-0.4471],\n",
      "        [-0.4469],\n",
      "        [-0.4467],\n",
      "        [-0.4471]])\n",
      "tensor([[-0.4465],\n",
      "        [-0.4463],\n",
      "        [-0.4460],\n",
      "        [-0.4465],\n",
      "        [-0.4462]])\n",
      "tensor([[-0.4464],\n",
      "        [-0.4464],\n",
      "        [-0.4468],\n",
      "        [-0.4471],\n",
      "        [-0.4473]])\n",
      "tensor([[-0.4478],\n",
      "        [-0.4479],\n",
      "        [-0.4479],\n",
      "        [-0.4475],\n",
      "        [-0.4471]])\n",
      "tensor([[-0.4473],\n",
      "        [-0.4478],\n",
      "        [-0.4481],\n",
      "        [-0.4483],\n",
      "        [-0.4481]])\n",
      "tensor([[-0.4478],\n",
      "        [-0.4480],\n",
      "        [-0.4476],\n",
      "        [-0.4475],\n",
      "        [-0.4480]])\n",
      "tensor([[-0.4474],\n",
      "        [-0.4475],\n",
      "        [-0.4471],\n",
      "        [-0.4464],\n",
      "        [-0.4463]])\n",
      "tensor([[-0.4458],\n",
      "        [-0.4457],\n",
      "        [-0.4454],\n",
      "        [-0.4452],\n",
      "        [-0.4450]])\n",
      "tensor([[-0.4450],\n",
      "        [-0.4454],\n",
      "        [-0.4453],\n",
      "        [-0.4458],\n",
      "        [-0.4447]])\n",
      "tensor([[-0.4446],\n",
      "        [-0.4446],\n",
      "        [-0.4447],\n",
      "        [-0.4449],\n",
      "        [-0.4450]])\n",
      "tensor([[-0.4450],\n",
      "        [-0.4445],\n",
      "        [-0.4443],\n",
      "        [-0.4429],\n",
      "        [-0.4427]])\n",
      "tensor([[-0.4428],\n",
      "        [-0.4428],\n",
      "        [-0.4433],\n",
      "        [-0.4440],\n",
      "        [-0.4443]])\n",
      "tensor([[-0.4439],\n",
      "        [-0.4438],\n",
      "        [-0.4438],\n",
      "        [-0.4441],\n",
      "        [-0.4441]])\n",
      "tensor([[-0.4450],\n",
      "        [-0.4449],\n",
      "        [-0.4448],\n",
      "        [-0.4449],\n",
      "        [-0.4450]])\n",
      "tensor([[-0.4451],\n",
      "        [-0.4449],\n",
      "        [-0.4450],\n",
      "        [-0.4448],\n",
      "        [-0.4451]])\n",
      "tensor([[-0.4456],\n",
      "        [-0.4457],\n",
      "        [-0.4465],\n",
      "        [-0.4472],\n",
      "        [-0.4480]])\n",
      "tensor([[-0.4473],\n",
      "        [-0.4482],\n",
      "        [-0.4477],\n",
      "        [-0.4485],\n",
      "        [-0.4476]])\n",
      "tensor([[-0.4469],\n",
      "        [-0.4480],\n",
      "        [-0.4476],\n",
      "        [-0.4477],\n",
      "        [-0.4474]])\n",
      "tensor([[-0.4474],\n",
      "        [-0.4473],\n",
      "        [-0.4464],\n",
      "        [-0.4455],\n",
      "        [-0.4450]])\n",
      "tensor([[-0.4456],\n",
      "        [-0.4462],\n",
      "        [-0.4462],\n",
      "        [-0.4459],\n",
      "        [-0.4456]])\n",
      "tensor([[-0.4455],\n",
      "        [-0.4451],\n",
      "        [-0.4451],\n",
      "        [-0.4448],\n",
      "        [-0.4445]])\n",
      "tensor([[-0.4453],\n",
      "        [-0.4448],\n",
      "        [-0.4445],\n",
      "        [-0.4448],\n",
      "        [-0.4451]])\n",
      "tensor([[-0.4458],\n",
      "        [-0.4465],\n",
      "        [-0.4466],\n",
      "        [-0.4464],\n",
      "        [-0.4467]])\n",
      "tensor([[-0.4473],\n",
      "        [-0.4480],\n",
      "        [-0.4469],\n",
      "        [-0.4466],\n",
      "        [-0.4469]])\n",
      "tensor([[-0.4470],\n",
      "        [-0.4469],\n",
      "        [-0.4464],\n",
      "        [-0.4466],\n",
      "        [-0.4467]])\n",
      "tensor([[-0.4457],\n",
      "        [-0.4447],\n",
      "        [-0.4447],\n",
      "        [-0.4446],\n",
      "        [-0.4452]])\n",
      "tensor([[-0.4460],\n",
      "        [-0.4462],\n",
      "        [-0.4459],\n",
      "        [-0.4453],\n",
      "        [-0.4466]])\n",
      "tensor([[-0.4459],\n",
      "        [-0.4445],\n",
      "        [-0.4451],\n",
      "        [-0.4442],\n",
      "        [-0.4428]])\n",
      "tensor([[-0.4429],\n",
      "        [-0.4432],\n",
      "        [-0.4430],\n",
      "        [-0.4424],\n",
      "        [-0.4427]])\n",
      "tensor([[-0.4422],\n",
      "        [-0.4425],\n",
      "        [-0.4430],\n",
      "        [-0.4429],\n",
      "        [-0.4434]])\n",
      "tensor([[-0.4439],\n",
      "        [-0.4432],\n",
      "        [-0.4424],\n",
      "        [-0.4427],\n",
      "        [-0.4427]])\n",
      "tensor([[-0.4422],\n",
      "        [-0.4421],\n",
      "        [-0.4429],\n",
      "        [-0.4432],\n",
      "        [-0.4430]])\n",
      "tensor([[-0.4426],\n",
      "        [-0.4413],\n",
      "        [-0.4414],\n",
      "        [-0.4410],\n",
      "        [-0.4413]])\n",
      "tensor([[-0.4404],\n",
      "        [-0.4400],\n",
      "        [-0.4428],\n",
      "        [-0.4437],\n",
      "        [-0.4471]])\n",
      "tensor([[-0.4476],\n",
      "        [-0.4479],\n",
      "        [-0.4474],\n",
      "        [-0.4469],\n",
      "        [-0.4475]])\n",
      "tensor([[-0.4466],\n",
      "        [-0.4467],\n",
      "        [-0.4472],\n",
      "        [-0.4476],\n",
      "        [-0.4469]])\n",
      "tensor([[-0.4468],\n",
      "        [-0.4471],\n",
      "        [-0.4474],\n",
      "        [-0.4476],\n",
      "        [-0.4477]])\n",
      "tensor([[-0.4474],\n",
      "        [-0.4475],\n",
      "        [-0.4477],\n",
      "        [-0.4478],\n",
      "        [-0.4474]])\n",
      "tensor([[-0.4474],\n",
      "        [-0.4474],\n",
      "        [-0.4474],\n",
      "        [-0.4477],\n",
      "        [-0.4477]])\n",
      "tensor([[-0.4475],\n",
      "        [-0.4472],\n",
      "        [-0.4470],\n",
      "        [-0.4471],\n",
      "        [-0.4467]])\n",
      "tensor([[-0.4475],\n",
      "        [-0.4474],\n",
      "        [-0.4478],\n",
      "        [-0.4478],\n",
      "        [-0.4479]])\n",
      "tensor([[-0.4478],\n",
      "        [-0.4488],\n",
      "        [-0.4486],\n",
      "        [-0.4489],\n",
      "        [-0.4490]])\n",
      "tensor([[-0.4491],\n",
      "        [-0.4495],\n",
      "        [-0.4498],\n",
      "        [-0.4499],\n",
      "        [-0.4496]])\n",
      "tensor([[-0.4503],\n",
      "        [-0.4500],\n",
      "        [-0.4496],\n",
      "        [-0.4498],\n",
      "        [-0.4499]])\n",
      "tensor([[-0.4501],\n",
      "        [-0.4498],\n",
      "        [-0.4493],\n",
      "        [-0.4485],\n",
      "        [-0.4485]])\n",
      "tensor([[-0.4483],\n",
      "        [-0.4480],\n",
      "        [-0.4480],\n",
      "        [-0.4484],\n",
      "        [-0.4478]])\n",
      "tensor([[-0.4476],\n",
      "        [-0.4487],\n",
      "        [-0.4491],\n",
      "        [-0.4496],\n",
      "        [-0.4495]])\n",
      "tensor([[-0.4498],\n",
      "        [-0.4501],\n",
      "        [-0.4500],\n",
      "        [-0.4503],\n",
      "        [-0.4501]])\n",
      "tensor([[-0.4499],\n",
      "        [-0.4498],\n",
      "        [-0.4491],\n",
      "        [-0.4484],\n",
      "        [-0.4486]])\n",
      "tensor([[-0.4489],\n",
      "        [-0.4490],\n",
      "        [-0.4494],\n",
      "        [-0.4489],\n",
      "        [-0.4492]])\n",
      "tensor([[-0.4481],\n",
      "        [-0.4479],\n",
      "        [-0.4482],\n",
      "        [-0.4478],\n",
      "        [-0.4479]])\n",
      "tensor([[-0.4481],\n",
      "        [-0.4481],\n",
      "        [-0.4480],\n",
      "        [-0.4481],\n",
      "        [-0.4483]])\n",
      "tensor([[-0.4477],\n",
      "        [-0.4475],\n",
      "        [-0.4469],\n",
      "        [-0.4469],\n",
      "        [-0.4472]])\n",
      "tensor([[-0.4472],\n",
      "        [-0.4473],\n",
      "        [-0.4471],\n",
      "        [-0.4467],\n",
      "        [-0.4471]])\n",
      "tensor([[-0.4469],\n",
      "        [-0.4454],\n",
      "        [-0.4454],\n",
      "        [-0.4456],\n",
      "        [-0.4453]])\n",
      "tensor([[-0.4454],\n",
      "        [-0.4452],\n",
      "        [-0.4454],\n",
      "        [-0.4452],\n",
      "        [-0.4454]])\n",
      "tensor([[-0.4454],\n",
      "        [-0.4457],\n",
      "        [-0.4459],\n",
      "        [-0.4466],\n",
      "        [-0.4466]])\n",
      "tensor([[-0.4464],\n",
      "        [-0.4459],\n",
      "        [-0.4463],\n",
      "        [-0.4463],\n",
      "        [-0.4462]])\n",
      "tensor([[-0.4466],\n",
      "        [-0.4466],\n",
      "        [-0.4470],\n",
      "        [-0.4466],\n",
      "        [-0.4466]])\n",
      "tensor([[-0.4464],\n",
      "        [-0.4467],\n",
      "        [-0.4466],\n",
      "        [-0.4463],\n",
      "        [-0.4464]])\n",
      "tensor([[-0.4462],\n",
      "        [-0.4460],\n",
      "        [-0.4460],\n",
      "        [-0.4462],\n",
      "        [-0.4463]])\n",
      "tensor([[-0.4460],\n",
      "        [-0.4461],\n",
      "        [-0.4465],\n",
      "        [-0.4478],\n",
      "        [-0.4481]])\n",
      "tensor([[-0.4472],\n",
      "        [-0.4473],\n",
      "        [-0.4470],\n",
      "        [-0.4464],\n",
      "        [-0.4466]])\n",
      "tensor([[-0.4467],\n",
      "        [-0.4469],\n",
      "        [-0.4469],\n",
      "        [-0.4469],\n",
      "        [-0.4468]])\n",
      "tensor([[-0.4467],\n",
      "        [-0.4471],\n",
      "        [-0.4472],\n",
      "        [-0.4472],\n",
      "        [-0.4463]])\n",
      "tensor([[-0.4463],\n",
      "        [-0.4464],\n",
      "        [-0.4454],\n",
      "        [-0.4459],\n",
      "        [-0.4456]])\n",
      "tensor([[-0.4457],\n",
      "        [-0.4452],\n",
      "        [-0.4456],\n",
      "        [-0.4458],\n",
      "        [-0.4459]])\n",
      "tensor([[-0.4461],\n",
      "        [-0.4455],\n",
      "        [-0.4454],\n",
      "        [-0.4458],\n",
      "        [-0.4457]])\n",
      "tensor([[-0.4453],\n",
      "        [-0.4446],\n",
      "        [-0.4447],\n",
      "        [-0.4448],\n",
      "        [-0.4452]])\n",
      "tensor([[-0.4449],\n",
      "        [-0.4452],\n",
      "        [-0.4455],\n",
      "        [-0.4453],\n",
      "        [-0.4450]])\n",
      "tensor([[-0.4457],\n",
      "        [-0.4457],\n",
      "        [-0.4461],\n",
      "        [-0.4461],\n",
      "        [-0.4463]])\n",
      "tensor([[-0.4464],\n",
      "        [-0.4473],\n",
      "        [-0.4471],\n",
      "        [-0.4471],\n",
      "        [-0.4475]])\n",
      "tensor([[-0.4472],\n",
      "        [-0.4474],\n",
      "        [-0.4472],\n",
      "        [-0.4474],\n",
      "        [-0.4482]])\n",
      "tensor([[-0.4477],\n",
      "        [-0.4477],\n",
      "        [-0.4469],\n",
      "        [-0.4469],\n",
      "        [-0.4469]])\n",
      "tensor([[-0.4468],\n",
      "        [-0.4464],\n",
      "        [-0.4466],\n",
      "        [-0.4467],\n",
      "        [-0.4465]])\n",
      "tensor([[-0.4462],\n",
      "        [-0.4466],\n",
      "        [-0.4463],\n",
      "        [-0.4460],\n",
      "        [-0.4464]])\n",
      "tensor([[-0.4461],\n",
      "        [-0.4464],\n",
      "        [-0.4460],\n",
      "        [-0.4464],\n",
      "        [-0.4470]])\n",
      "tensor([[-0.4468],\n",
      "        [-0.4462],\n",
      "        [-0.4463],\n",
      "        [-0.4465],\n",
      "        [-0.4465]])\n",
      "tensor([[-0.4468],\n",
      "        [-0.4468],\n",
      "        [-0.4468],\n",
      "        [-0.4471],\n",
      "        [-0.4467]])\n",
      "tensor([[-0.4469],\n",
      "        [-0.4474],\n",
      "        [-0.4470],\n",
      "        [-0.4469],\n",
      "        [-0.4470]])\n",
      "tensor([[-0.4471],\n",
      "        [-0.4472],\n",
      "        [-0.4474],\n",
      "        [-0.4474],\n",
      "        [-0.4475]])\n",
      "tensor([[-0.4472],\n",
      "        [-0.4479],\n",
      "        [-0.4477],\n",
      "        [-0.4478],\n",
      "        [-0.4481]])\n",
      "tensor([[-0.4478],\n",
      "        [-0.4475],\n",
      "        [-0.4473],\n",
      "        [-0.4466],\n",
      "        [-0.4462]])\n",
      "tensor([[-0.4459],\n",
      "        [-0.4452],\n",
      "        [-0.4457],\n",
      "        [-0.4460],\n",
      "        [-0.4461]])\n",
      "tensor([[-0.4459],\n",
      "        [-0.4454],\n",
      "        [-0.4458],\n",
      "        [-0.4454],\n",
      "        [-0.4448]])\n",
      "tensor([[-0.4449],\n",
      "        [-0.4443],\n",
      "        [-0.4445],\n",
      "        [-0.4454],\n",
      "        [-0.4465]])\n",
      "tensor([[-0.4471],\n",
      "        [-0.4465],\n",
      "        [-0.4473],\n",
      "        [-0.4468],\n",
      "        [-0.4467]])\n",
      "tensor([[-0.4468],\n",
      "        [-0.4467],\n",
      "        [-0.4469],\n",
      "        [-0.4462],\n",
      "        [-0.4462]])\n",
      "tensor([[-0.4460],\n",
      "        [-0.4462],\n",
      "        [-0.4458],\n",
      "        [-0.4458],\n",
      "        [-0.4452]])\n",
      "tensor([[-0.4452],\n",
      "        [-0.4451],\n",
      "        [-0.4452],\n",
      "        [-0.4449],\n",
      "        [-0.4449]])\n",
      "tensor([[-0.4456],\n",
      "        [-0.4467],\n",
      "        [-0.4470],\n",
      "        [-0.4474],\n",
      "        [-0.4472]])\n",
      "tensor([[-0.4471],\n",
      "        [-0.4469],\n",
      "        [-0.4472],\n",
      "        [-0.4470],\n",
      "        [-0.4469]])\n",
      "tensor([[-0.4469],\n",
      "        [-0.4469],\n",
      "        [-0.4463],\n",
      "        [-0.4464],\n",
      "        [-0.4464]])\n",
      "tensor([[-0.4464],\n",
      "        [-0.4471],\n",
      "        [-0.4464],\n",
      "        [-0.4463],\n",
      "        [-0.4462]])\n",
      "tensor([[-0.4463],\n",
      "        [-0.4461],\n",
      "        [-0.4461],\n",
      "        [-0.4462],\n",
      "        [-0.4462]])\n",
      "tensor([[-0.4461],\n",
      "        [-0.4460],\n",
      "        [-0.4460],\n",
      "        [-0.4454],\n",
      "        [-0.4451]])\n",
      "tensor([[-0.4454],\n",
      "        [-0.4451],\n",
      "        [-0.4439],\n",
      "        [-0.4441],\n",
      "        [-0.4441]])\n",
      "tensor([[-0.4455],\n",
      "        [-0.4452],\n",
      "        [-0.4453],\n",
      "        [-0.4451],\n",
      "        [-0.4446]])\n",
      "tensor([[-0.4446],\n",
      "        [-0.4449],\n",
      "        [-0.4449],\n",
      "        [-0.4449],\n",
      "        [-0.4449]])\n",
      "tensor([[-0.4448],\n",
      "        [-0.4428],\n",
      "        [-0.4430],\n",
      "        [-0.4429],\n",
      "        [-0.4429]])\n",
      "tensor([[-0.4438],\n",
      "        [-0.4436],\n",
      "        [-0.4437],\n",
      "        [-0.4451],\n",
      "        [-0.4446]])\n",
      "tensor([[-0.4447],\n",
      "        [-0.4441],\n",
      "        [-0.4435],\n",
      "        [-0.4430],\n",
      "        [-0.4422]])\n",
      "tensor([[-0.4424],\n",
      "        [-0.4426],\n",
      "        [-0.4423],\n",
      "        [-0.4428],\n",
      "        [-0.4428]])\n",
      "tensor([[-0.4429],\n",
      "        [-0.4429],\n",
      "        [-0.4430],\n",
      "        [-0.4423],\n",
      "        [-0.4425]])\n",
      "tensor([[-0.4430],\n",
      "        [-0.4431],\n",
      "        [-0.4429],\n",
      "        [-0.4430],\n",
      "        [-0.4436]])\n",
      "tensor([[-0.4441],\n",
      "        [-0.4443],\n",
      "        [-0.4438],\n",
      "        [-0.4440],\n",
      "        [-0.4441]])\n",
      "tensor([[-0.4439],\n",
      "        [-0.4444],\n",
      "        [-0.4447],\n",
      "        [-0.4447],\n",
      "        [-0.4449]])\n",
      "tensor([[-0.4448],\n",
      "        [-0.4447],\n",
      "        [-0.4448],\n",
      "        [-0.4448],\n",
      "        [-0.4453]])\n",
      "tensor([[-0.4447],\n",
      "        [-0.4445],\n",
      "        [-0.4437],\n",
      "        [-0.4441],\n",
      "        [-0.4439]])\n",
      "tensor([[-0.4432],\n",
      "        [-0.4432],\n",
      "        [-0.4432],\n",
      "        [-0.4432],\n",
      "        [-0.4427]])\n",
      "tensor([[-0.4428],\n",
      "        [-0.4429],\n",
      "        [-0.4428],\n",
      "        [-0.4431],\n",
      "        [-0.4431]])\n",
      "tensor([[-0.4429],\n",
      "        [-0.4429],\n",
      "        [-0.4437],\n",
      "        [-0.4442],\n",
      "        [-0.4443]])\n",
      "tensor([[-0.4446],\n",
      "        [-0.4450],\n",
      "        [-0.4447],\n",
      "        [-0.4448],\n",
      "        [-0.4444]])\n",
      "tensor([[-0.4446],\n",
      "        [-0.4447],\n",
      "        [-0.4456],\n",
      "        [-0.4453],\n",
      "        [-0.4450]])\n",
      "tensor([[-0.4448],\n",
      "        [-0.4454],\n",
      "        [-0.4450],\n",
      "        [-0.4460],\n",
      "        [-0.4460]])\n",
      "tensor([[-0.4457],\n",
      "        [-0.4457],\n",
      "        [-0.4451],\n",
      "        [-0.4454],\n",
      "        [-0.4461]])\n",
      "tensor([[-0.4465],\n",
      "        [-0.4466],\n",
      "        [-0.4460],\n",
      "        [-0.4462],\n",
      "        [-0.4460]])\n",
      "tensor([[-0.4457],\n",
      "        [-0.4460],\n",
      "        [-0.4460],\n",
      "        [-0.4461],\n",
      "        [-0.4460]])\n",
      "tensor([[-0.4463],\n",
      "        [-0.4467],\n",
      "        [-0.4470],\n",
      "        [-0.4463],\n",
      "        [-0.4468]])\n",
      "tensor([[-0.4464],\n",
      "        [-0.4481],\n",
      "        [-0.4492],\n",
      "        [-0.4495],\n",
      "        [-0.4499]])\n",
      "tensor([[-0.4480],\n",
      "        [-0.4492],\n",
      "        [-0.4485],\n",
      "        [-0.4483],\n",
      "        [-0.4480]])\n",
      "tensor([[-0.4492],\n",
      "        [-0.4499],\n",
      "        [-0.4506],\n",
      "        [-0.4500],\n",
      "        [-0.4497]])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:22:42.633786Z",
     "start_time": "2025-05-14T15:22:41.890012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(y_test[0:5], label='Original Data')\n",
    "# plt.plot(np.concatenate((train_predict.numpy().flatten(), test_predict.numpy().flatten())), label='LSTM Predictions')\n",
    "plt.plot(test_predict.numpy().flatten(), label='LSTM Predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "6b8aeb38f86e6d7c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:22:42.644489Z",
     "start_time": "2025-05-14T15:22:42.642518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "id": "74c467305e209496",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:22:53.794127Z",
     "start_time": "2025-05-14T15:22:42.927152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_columns = [column for column in dfrm_training.columns if column not in [tgt_field_column_name] and 'close' in column]\n",
    "Y_train = dfrm_training[tgt_field_column_name]\n",
    "X_train = dfrm_training[x_columns]\n",
    "# null_indices = X_train[X_train.isnull().any(axis=1)].index\n",
    "# X_train = X_train.drop(null_indices)\n",
    "# Y_train = Y_train.drop(null_indices)\n",
    "\n",
    "Y_test = dfrm_test[tgt_field_column_name]\n",
    "X_test = dfrm_test[x_columns]\n",
    "\n",
    "# Cap the numeric values within (-1, +1) range\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "transformed_Y_train = scaler.fit_transform(Y_train.values.reshape(-1,1))\n",
    "transformed_Y_test = scaler.fit_transform(Y_test.values.reshape(-1,1))\n",
    "\n",
    "transformed_X_train = X_train.copy()\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "for column_name in X_train.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    transformed_X_train[column_name] = scaler.fit_transform(X_train[[column_name]])\n",
    "\n",
    "transformed_X_test = X_test.copy()\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "for column_name in X_test.select_dtypes(include=['int64', 'float64']).columns:\n",
    "    transformed_X_test[column_name] = scaler.fit_transform(X_test[[column_name]])\n",
    "\n",
    "x_train = torch.from_numpy(transformed_X_train.to_numpy()).to(torch.float32)\n",
    "x_test = torch.from_numpy(transformed_X_test.to_numpy()).to(torch.float32)\n",
    "y_train = torch.from_numpy(transformed_Y_train).to(torch.float32)\n",
    "y_test = torch.from_numpy(transformed_Y_test).to(torch.float32)\n",
    "\n",
    "x_train_loader = DataLoader(x_train, batch_size=5, shuffle=False) # Keep shuffle false to preserve the direction\n",
    "y_train_loader = DataLoader(y_train, batch_size=5, shuffle=False) # Keep shuffle false to preserve the direction\n",
    "x_test_loader = DataLoader(x_test, batch_size=5, shuffle=False) # Keep shuffle false to preserve the direction\n",
    "y_test_loader = DataLoader(y_test, batch_size=5, shuffle=False) # Keep shuffle false to preserve the direction\n",
    "\n",
    "input_dim = len(X_train.columns) # Number of features\n",
    "#hidden_dim = int(len(X_train)) # Number of samples or units\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "num_epochs = 2 # Intentionally keeping low for initial debugging\n",
    "seq_len = 10\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure x has batch dimension\n",
    "        #print(\"Input size:\", x.size(0))\n",
    "        # print(f'X:{x}')\n",
    "        x = self.dropout(x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, num_layers= num_layers, output_dim=output_dim, )\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "model.lstm\n",
    "\n",
    "\n",
    "hist = np.zeros(num_epochs)\n",
    "start_time = time.time()\n",
    "lstm = []\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    counter = 1\n",
    "    for batch in x_train_loader:\n",
    "        batch = batch.reshape(5, 1, input_dim)\n",
    "        y_train_pred = model(batch)\n",
    "        y_train_observed = next(iter(y_train_loader))\n",
    "        loss = criterion(y_train_pred, y_train_observed)\n",
    "        print(f'Epoch:{t:}, Batch:{counter}, Loss: {loss.item():.4f}')\n",
    "        # hist[t] = loss.item()\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        counter += 1\n",
    "\n",
    "training_time = time.time()-start_time\n",
    "print(\"Training time: {}\".format(training_time))\n"
   ],
   "id": "dd3f8567ef3ec9c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, Batch:1, Loss: 1.0982\n",
      "Epoch:0, Batch:2, Loss: 0.0445\n",
      "Epoch:0, Batch:3, Loss: 5.5324\n",
      "Epoch:0, Batch:4, Loss: 0.0089\n",
      "Epoch:0, Batch:5, Loss: 1.1203\n",
      "Epoch:0, Batch:6, Loss: 1.2313\n",
      "Epoch:0, Batch:7, Loss: 0.7164\n",
      "Epoch:0, Batch:8, Loss: 0.1862\n",
      "Epoch:0, Batch:9, Loss: 0.0141\n",
      "Epoch:0, Batch:10, Loss: 0.3704\n",
      "Epoch:0, Batch:11, Loss: 0.2928\n",
      "Epoch:0, Batch:12, Loss: 0.0033\n",
      "Epoch:0, Batch:13, Loss: 0.1650\n",
      "Epoch:0, Batch:14, Loss: 0.3532\n",
      "Epoch:0, Batch:15, Loss: 0.3669\n",
      "Epoch:0, Batch:16, Loss: 0.1684\n",
      "Epoch:0, Batch:17, Loss: 0.0039\n",
      "Epoch:0, Batch:18, Loss: 0.1132\n",
      "Epoch:0, Batch:19, Loss: 0.1968\n",
      "Epoch:0, Batch:20, Loss: 0.0799\n",
      "Epoch:0, Batch:21, Loss: 0.0058\n",
      "Epoch:0, Batch:22, Loss: 0.0288\n",
      "Epoch:0, Batch:23, Loss: 0.0880\n",
      "Epoch:0, Batch:24, Loss: 0.0999\n",
      "Epoch:0, Batch:25, Loss: 0.0687\n",
      "Epoch:0, Batch:26, Loss: 0.0177\n",
      "Epoch:0, Batch:27, Loss: 0.0020\n",
      "Epoch:0, Batch:28, Loss: 0.0261\n",
      "Epoch:0, Batch:29, Loss: 0.0522\n",
      "Epoch:0, Batch:30, Loss: 0.0479\n",
      "Epoch:0, Batch:31, Loss: 0.0181\n",
      "Epoch:0, Batch:32, Loss: 0.0021\n",
      "Epoch:0, Batch:33, Loss: 0.0102\n",
      "Epoch:0, Batch:34, Loss: 0.0305\n",
      "Epoch:0, Batch:35, Loss: 0.0305\n",
      "Epoch:0, Batch:36, Loss: 0.0265\n",
      "Epoch:0, Batch:37, Loss: 0.0120\n",
      "Epoch:0, Batch:38, Loss: 0.0010\n",
      "Epoch:0, Batch:39, Loss: 0.0048\n",
      "Epoch:0, Batch:40, Loss: 0.0190\n",
      "Epoch:0, Batch:41, Loss: 0.0228\n",
      "Epoch:0, Batch:42, Loss: 0.0147\n",
      "Epoch:0, Batch:43, Loss: 0.0027\n",
      "Epoch:0, Batch:44, Loss: 0.0010\n",
      "Epoch:0, Batch:45, Loss: 0.0046\n",
      "Epoch:0, Batch:46, Loss: 0.0091\n",
      "Epoch:0, Batch:47, Loss: 0.0131\n",
      "Epoch:0, Batch:48, Loss: 0.0089\n",
      "Epoch:0, Batch:49, Loss: 0.0047\n",
      "Epoch:0, Batch:50, Loss: 0.0015\n",
      "Epoch:0, Batch:51, Loss: 0.0033\n",
      "Epoch:0, Batch:52, Loss: 0.0069\n",
      "Epoch:0, Batch:53, Loss: 0.0064\n",
      "Epoch:0, Batch:54, Loss: 0.0032\n",
      "Epoch:0, Batch:55, Loss: 0.0021\n",
      "Epoch:0, Batch:56, Loss: 0.0018\n",
      "Epoch:0, Batch:57, Loss: 0.0034\n",
      "Epoch:0, Batch:58, Loss: 0.0027\n",
      "Epoch:0, Batch:59, Loss: 0.0037\n",
      "Epoch:0, Batch:60, Loss: 0.0036\n",
      "Epoch:0, Batch:61, Loss: 0.0032\n",
      "Epoch:0, Batch:62, Loss: 0.0020\n",
      "Epoch:0, Batch:63, Loss: 0.0015\n",
      "Epoch:0, Batch:64, Loss: 0.0036\n",
      "Epoch:0, Batch:65, Loss: 0.0029\n",
      "Epoch:0, Batch:66, Loss: 0.0033\n",
      "Epoch:0, Batch:67, Loss: 0.0023\n",
      "Epoch:0, Batch:68, Loss: 0.0013\n",
      "Epoch:0, Batch:69, Loss: 0.0018\n",
      "Epoch:0, Batch:70, Loss: 0.0021\n",
      "Epoch:0, Batch:71, Loss: 0.0045\n",
      "Epoch:0, Batch:72, Loss: 0.0018\n",
      "Epoch:0, Batch:73, Loss: 0.0013\n",
      "Epoch:0, Batch:74, Loss: 0.0015\n",
      "Epoch:0, Batch:75, Loss: 0.0024\n",
      "Epoch:0, Batch:76, Loss: 0.0026\n",
      "Epoch:0, Batch:77, Loss: 0.0016\n",
      "Epoch:0, Batch:78, Loss: 0.0013\n",
      "Epoch:0, Batch:79, Loss: 0.0011\n",
      "Epoch:0, Batch:80, Loss: 0.0012\n",
      "Epoch:0, Batch:81, Loss: 0.0025\n",
      "Epoch:0, Batch:82, Loss: 0.0016\n",
      "Epoch:0, Batch:83, Loss: 0.0013\n",
      "Epoch:0, Batch:84, Loss: 0.0012\n",
      "Epoch:0, Batch:85, Loss: 0.0012\n",
      "Epoch:0, Batch:86, Loss: 0.0016\n",
      "Epoch:0, Batch:87, Loss: 0.0014\n",
      "Epoch:0, Batch:88, Loss: 0.0024\n",
      "Epoch:0, Batch:89, Loss: 0.0011\n",
      "Epoch:0, Batch:90, Loss: 0.0014\n",
      "Epoch:0, Batch:91, Loss: 0.0019\n",
      "Epoch:0, Batch:92, Loss: 0.0013\n",
      "Epoch:0, Batch:93, Loss: 0.0015\n",
      "Epoch:0, Batch:94, Loss: 0.0014\n",
      "Epoch:0, Batch:95, Loss: 0.0016\n",
      "Epoch:0, Batch:96, Loss: 0.0014\n",
      "Epoch:0, Batch:97, Loss: 0.0018\n",
      "Epoch:0, Batch:98, Loss: 0.0020\n",
      "Epoch:0, Batch:99, Loss: 0.0013\n",
      "Epoch:0, Batch:100, Loss: 0.0017\n",
      "Epoch:0, Batch:101, Loss: 0.0022\n",
      "Epoch:0, Batch:102, Loss: 0.0026\n",
      "Epoch:0, Batch:103, Loss: 0.0008\n",
      "Epoch:0, Batch:104, Loss: 0.0010\n",
      "Epoch:0, Batch:105, Loss: 0.0011\n",
      "Epoch:0, Batch:106, Loss: 0.0018\n",
      "Epoch:0, Batch:107, Loss: 0.0013\n",
      "Epoch:0, Batch:108, Loss: 0.0013\n",
      "Epoch:0, Batch:109, Loss: 0.0013\n",
      "Epoch:0, Batch:110, Loss: 0.0009\n",
      "Epoch:0, Batch:111, Loss: 0.0014\n",
      "Epoch:0, Batch:112, Loss: 0.0013\n",
      "Epoch:0, Batch:113, Loss: 0.0014\n",
      "Epoch:0, Batch:114, Loss: 0.0012\n",
      "Epoch:0, Batch:115, Loss: 0.0014\n",
      "Epoch:0, Batch:116, Loss: 0.0014\n",
      "Epoch:0, Batch:117, Loss: 0.0011\n",
      "Epoch:0, Batch:118, Loss: 0.0016\n",
      "Epoch:0, Batch:119, Loss: 0.0015\n",
      "Epoch:0, Batch:120, Loss: 0.0009\n",
      "Epoch:0, Batch:121, Loss: 0.0014\n",
      "Epoch:0, Batch:122, Loss: 0.0014\n",
      "Epoch:0, Batch:123, Loss: 0.0011\n",
      "Epoch:0, Batch:124, Loss: 0.0014\n",
      "Epoch:0, Batch:125, Loss: 0.0012\n",
      "Epoch:0, Batch:126, Loss: 0.0008\n",
      "Epoch:0, Batch:127, Loss: 0.0018\n",
      "Epoch:0, Batch:128, Loss: 0.0014\n",
      "Epoch:0, Batch:129, Loss: 0.0015\n",
      "Epoch:0, Batch:130, Loss: 0.0014\n",
      "Epoch:0, Batch:131, Loss: 0.0013\n",
      "Epoch:0, Batch:132, Loss: 0.0011\n",
      "Epoch:0, Batch:133, Loss: 0.0014\n",
      "Epoch:0, Batch:134, Loss: 0.0010\n",
      "Epoch:0, Batch:135, Loss: 0.0014\n",
      "Epoch:0, Batch:136, Loss: 0.0011\n",
      "Epoch:0, Batch:137, Loss: 0.0017\n",
      "Epoch:0, Batch:138, Loss: 0.0018\n",
      "Epoch:0, Batch:139, Loss: 0.0010\n",
      "Epoch:0, Batch:140, Loss: 0.0016\n",
      "Epoch:0, Batch:141, Loss: 0.0013\n",
      "Epoch:0, Batch:142, Loss: 0.0015\n",
      "Epoch:0, Batch:143, Loss: 0.0013\n",
      "Epoch:0, Batch:144, Loss: 0.0013\n",
      "Epoch:0, Batch:145, Loss: 0.0018\n",
      "Epoch:0, Batch:146, Loss: 0.0018\n",
      "Epoch:0, Batch:147, Loss: 0.0013\n",
      "Epoch:0, Batch:148, Loss: 0.0019\n",
      "Epoch:0, Batch:149, Loss: 0.0013\n",
      "Epoch:0, Batch:150, Loss: 0.0010\n",
      "Epoch:0, Batch:151, Loss: 0.0011\n",
      "Epoch:0, Batch:152, Loss: 0.0016\n",
      "Epoch:0, Batch:153, Loss: 0.0011\n",
      "Epoch:0, Batch:154, Loss: 0.0013\n",
      "Epoch:0, Batch:155, Loss: 0.0017\n",
      "Epoch:0, Batch:156, Loss: 0.0012\n",
      "Epoch:0, Batch:157, Loss: 0.0012\n",
      "Epoch:0, Batch:158, Loss: 0.0017\n",
      "Epoch:0, Batch:159, Loss: 0.0016\n",
      "Epoch:0, Batch:160, Loss: 0.0013\n",
      "Epoch:0, Batch:161, Loss: 0.0014\n",
      "Epoch:0, Batch:162, Loss: 0.0012\n",
      "Epoch:0, Batch:163, Loss: 0.0014\n",
      "Epoch:0, Batch:164, Loss: 0.0014\n",
      "Epoch:0, Batch:165, Loss: 0.0013\n",
      "Epoch:0, Batch:166, Loss: 0.0010\n",
      "Epoch:0, Batch:167, Loss: 0.0014\n",
      "Epoch:0, Batch:168, Loss: 0.0016\n",
      "Epoch:0, Batch:169, Loss: 0.0018\n",
      "Epoch:0, Batch:170, Loss: 0.0018\n",
      "Epoch:0, Batch:171, Loss: 0.0010\n",
      "Epoch:0, Batch:172, Loss: 0.0011\n",
      "Epoch:0, Batch:173, Loss: 0.0012\n",
      "Epoch:0, Batch:174, Loss: 0.0015\n",
      "Epoch:0, Batch:175, Loss: 0.0019\n",
      "Epoch:0, Batch:176, Loss: 0.0013\n",
      "Epoch:0, Batch:177, Loss: 0.0016\n",
      "Epoch:0, Batch:178, Loss: 0.0015\n",
      "Epoch:0, Batch:179, Loss: 0.0013\n",
      "Epoch:0, Batch:180, Loss: 0.0013\n",
      "Epoch:0, Batch:181, Loss: 0.0014\n",
      "Epoch:0, Batch:182, Loss: 0.0012\n",
      "Epoch:0, Batch:183, Loss: 0.0013\n",
      "Epoch:0, Batch:184, Loss: 0.0013\n",
      "Epoch:0, Batch:185, Loss: 0.0011\n",
      "Epoch:0, Batch:186, Loss: 0.0014\n",
      "Epoch:0, Batch:187, Loss: 0.0020\n",
      "Epoch:0, Batch:188, Loss: 0.0010\n",
      "Epoch:0, Batch:189, Loss: 0.0010\n",
      "Epoch:0, Batch:190, Loss: 0.0010\n",
      "Epoch:0, Batch:191, Loss: 0.0036\n",
      "Epoch:0, Batch:192, Loss: 0.0010\n",
      "Epoch:0, Batch:193, Loss: 0.0013\n",
      "Epoch:0, Batch:194, Loss: 0.0013\n",
      "Epoch:0, Batch:195, Loss: 0.0008\n",
      "Epoch:0, Batch:196, Loss: 0.0009\n",
      "Epoch:0, Batch:197, Loss: 0.0012\n",
      "Epoch:0, Batch:198, Loss: 0.0015\n",
      "Epoch:0, Batch:199, Loss: 0.0013\n",
      "Epoch:0, Batch:200, Loss: 0.0012\n",
      "Epoch:0, Batch:201, Loss: 0.0014\n",
      "Epoch:0, Batch:202, Loss: 0.0007\n",
      "Epoch:0, Batch:203, Loss: 0.0014\n",
      "Epoch:0, Batch:204, Loss: 0.0013\n",
      "Epoch:0, Batch:205, Loss: 0.0011\n",
      "Epoch:0, Batch:206, Loss: 0.0023\n",
      "Epoch:0, Batch:207, Loss: 0.0009\n",
      "Epoch:0, Batch:208, Loss: 0.0011\n",
      "Epoch:0, Batch:209, Loss: 0.0011\n",
      "Epoch:0, Batch:210, Loss: 0.0011\n",
      "Epoch:0, Batch:211, Loss: 0.0016\n",
      "Epoch:0, Batch:212, Loss: 0.0010\n",
      "Epoch:0, Batch:213, Loss: 0.0013\n",
      "Epoch:0, Batch:214, Loss: 0.0012\n",
      "Epoch:0, Batch:215, Loss: 0.0014\n",
      "Epoch:0, Batch:216, Loss: 0.0017\n",
      "Epoch:0, Batch:217, Loss: 0.0012\n",
      "Epoch:0, Batch:218, Loss: 0.0009\n",
      "Epoch:0, Batch:219, Loss: 0.0013\n",
      "Epoch:0, Batch:220, Loss: 0.0017\n",
      "Epoch:0, Batch:221, Loss: 0.0017\n",
      "Epoch:0, Batch:222, Loss: 0.0014\n",
      "Epoch:0, Batch:223, Loss: 0.0009\n",
      "Epoch:0, Batch:224, Loss: 0.0011\n",
      "Epoch:0, Batch:225, Loss: 0.0009\n",
      "Epoch:0, Batch:226, Loss: 0.0011\n",
      "Epoch:0, Batch:227, Loss: 0.0014\n",
      "Epoch:0, Batch:228, Loss: 0.0008\n",
      "Epoch:0, Batch:229, Loss: 0.0011\n",
      "Epoch:0, Batch:230, Loss: 0.0010\n",
      "Epoch:0, Batch:231, Loss: 0.0012\n",
      "Epoch:0, Batch:232, Loss: 0.0014\n",
      "Epoch:0, Batch:233, Loss: 0.0012\n",
      "Epoch:0, Batch:234, Loss: 0.0022\n",
      "Epoch:0, Batch:235, Loss: 0.0013\n",
      "Epoch:0, Batch:236, Loss: 0.0014\n",
      "Epoch:0, Batch:237, Loss: 0.0012\n",
      "Epoch:0, Batch:238, Loss: 0.0011\n",
      "Epoch:0, Batch:239, Loss: 0.0014\n",
      "Epoch:0, Batch:240, Loss: 0.0017\n",
      "Epoch:0, Batch:241, Loss: 0.0014\n",
      "Epoch:0, Batch:242, Loss: 0.0009\n",
      "Epoch:0, Batch:243, Loss: 0.0016\n",
      "Epoch:0, Batch:244, Loss: 0.0011\n",
      "Epoch:0, Batch:245, Loss: 0.0012\n",
      "Epoch:0, Batch:246, Loss: 0.0015\n",
      "Epoch:0, Batch:247, Loss: 0.0010\n",
      "Epoch:0, Batch:248, Loss: 0.0012\n",
      "Epoch:0, Batch:249, Loss: 0.0026\n",
      "Epoch:0, Batch:250, Loss: 0.0016\n",
      "Epoch:0, Batch:251, Loss: 0.0012\n",
      "Epoch:0, Batch:252, Loss: 0.0016\n",
      "Epoch:0, Batch:253, Loss: 0.0016\n",
      "Epoch:0, Batch:254, Loss: 0.0014\n",
      "Epoch:0, Batch:255, Loss: 0.0011\n",
      "Epoch:0, Batch:256, Loss: 0.0011\n",
      "Epoch:0, Batch:257, Loss: 0.0018\n",
      "Epoch:0, Batch:258, Loss: 0.0013\n",
      "Epoch:0, Batch:259, Loss: 0.0012\n",
      "Epoch:0, Batch:260, Loss: 0.0016\n",
      "Epoch:0, Batch:261, Loss: 0.0015\n",
      "Epoch:0, Batch:262, Loss: 0.0011\n",
      "Epoch:0, Batch:263, Loss: 0.0018\n",
      "Epoch:0, Batch:264, Loss: 0.0015\n",
      "Epoch:0, Batch:265, Loss: 0.0013\n",
      "Epoch:0, Batch:266, Loss: 0.0020\n",
      "Epoch:0, Batch:267, Loss: 0.0010\n",
      "Epoch:0, Batch:268, Loss: 0.0009\n",
      "Epoch:0, Batch:269, Loss: 0.0011\n",
      "Epoch:0, Batch:270, Loss: 0.0014\n",
      "Epoch:0, Batch:271, Loss: 0.0021\n",
      "Epoch:0, Batch:272, Loss: 0.0016\n",
      "Epoch:0, Batch:273, Loss: 0.0012\n",
      "Epoch:0, Batch:274, Loss: 0.0016\n",
      "Epoch:0, Batch:275, Loss: 0.0018\n",
      "Epoch:0, Batch:276, Loss: 0.0012\n",
      "Epoch:0, Batch:277, Loss: 0.0012\n",
      "Epoch:0, Batch:278, Loss: 0.0012\n",
      "Epoch:0, Batch:279, Loss: 0.0016\n",
      "Epoch:0, Batch:280, Loss: 0.0012\n",
      "Epoch:0, Batch:281, Loss: 0.0016\n",
      "Epoch:0, Batch:282, Loss: 0.0014\n",
      "Epoch:0, Batch:283, Loss: 0.0014\n",
      "Epoch:0, Batch:284, Loss: 0.0015\n",
      "Epoch:0, Batch:285, Loss: 0.0015\n",
      "Epoch:0, Batch:286, Loss: 0.0014\n",
      "Epoch:0, Batch:287, Loss: 0.0011\n",
      "Epoch:0, Batch:288, Loss: 0.0012\n",
      "Epoch:0, Batch:289, Loss: 0.0012\n",
      "Epoch:0, Batch:290, Loss: 0.0013\n",
      "Epoch:0, Batch:291, Loss: 0.0018\n",
      "Epoch:0, Batch:292, Loss: 0.0011\n",
      "Epoch:0, Batch:293, Loss: 0.0017\n",
      "Epoch:0, Batch:294, Loss: 0.0012\n",
      "Epoch:0, Batch:295, Loss: 0.0012\n",
      "Epoch:0, Batch:296, Loss: 0.0012\n",
      "Epoch:0, Batch:297, Loss: 0.0012\n",
      "Epoch:0, Batch:298, Loss: 0.0013\n",
      "Epoch:0, Batch:299, Loss: 0.0011\n",
      "Epoch:0, Batch:300, Loss: 0.0013\n",
      "Epoch:0, Batch:301, Loss: 0.0010\n",
      "Epoch:0, Batch:302, Loss: 0.0014\n",
      "Epoch:0, Batch:303, Loss: 0.0012\n",
      "Epoch:0, Batch:304, Loss: 0.0015\n",
      "Epoch:0, Batch:305, Loss: 0.0014\n",
      "Epoch:0, Batch:306, Loss: 0.0011\n",
      "Epoch:0, Batch:307, Loss: 0.0013\n",
      "Epoch:0, Batch:308, Loss: 0.0015\n",
      "Epoch:0, Batch:309, Loss: 0.0015\n",
      "Epoch:0, Batch:310, Loss: 0.0014\n",
      "Epoch:0, Batch:311, Loss: 0.0011\n",
      "Epoch:0, Batch:312, Loss: 0.0011\n",
      "Epoch:0, Batch:313, Loss: 0.0013\n",
      "Epoch:0, Batch:314, Loss: 0.0014\n",
      "Epoch:0, Batch:315, Loss: 0.0012\n",
      "Epoch:0, Batch:316, Loss: 0.0018\n",
      "Epoch:0, Batch:317, Loss: 0.0013\n",
      "Epoch:0, Batch:318, Loss: 0.0012\n",
      "Epoch:0, Batch:319, Loss: 0.0012\n",
      "Epoch:0, Batch:320, Loss: 0.0012\n",
      "Epoch:0, Batch:321, Loss: 0.0011\n",
      "Epoch:0, Batch:322, Loss: 0.0011\n",
      "Epoch:0, Batch:323, Loss: 0.0022\n",
      "Epoch:0, Batch:324, Loss: 0.0018\n",
      "Epoch:0, Batch:325, Loss: 0.0012\n",
      "Epoch:0, Batch:326, Loss: 0.0013\n",
      "Epoch:0, Batch:327, Loss: 0.0013\n",
      "Epoch:0, Batch:328, Loss: 0.0011\n",
      "Epoch:0, Batch:329, Loss: 0.0012\n",
      "Epoch:0, Batch:330, Loss: 0.0013\n",
      "Epoch:0, Batch:331, Loss: 0.0013\n",
      "Epoch:0, Batch:332, Loss: 0.0011\n",
      "Epoch:0, Batch:333, Loss: 0.0014\n",
      "Epoch:0, Batch:334, Loss: 0.0015\n",
      "Epoch:0, Batch:335, Loss: 0.0016\n",
      "Epoch:0, Batch:336, Loss: 0.0024\n",
      "Epoch:0, Batch:337, Loss: 0.0012\n",
      "Epoch:0, Batch:338, Loss: 0.0008\n",
      "Epoch:0, Batch:339, Loss: 0.0020\n",
      "Epoch:0, Batch:340, Loss: 0.0016\n",
      "Epoch:0, Batch:341, Loss: 0.0013\n",
      "Epoch:0, Batch:342, Loss: 0.0014\n",
      "Epoch:0, Batch:343, Loss: 0.0009\n",
      "Epoch:0, Batch:344, Loss: 0.0014\n",
      "Epoch:0, Batch:345, Loss: 0.0019\n",
      "Epoch:0, Batch:346, Loss: 0.0009\n",
      "Epoch:0, Batch:347, Loss: 0.0016\n",
      "Epoch:0, Batch:348, Loss: 0.0015\n",
      "Epoch:0, Batch:349, Loss: 0.0011\n",
      "Epoch:0, Batch:350, Loss: 0.0017\n",
      "Epoch:0, Batch:351, Loss: 0.0009\n",
      "Epoch:0, Batch:352, Loss: 0.0042\n",
      "Epoch:0, Batch:353, Loss: 0.0023\n",
      "Epoch:0, Batch:354, Loss: 0.0023\n",
      "Epoch:0, Batch:355, Loss: 0.0023\n",
      "Epoch:0, Batch:356, Loss: 0.0028\n",
      "Epoch:0, Batch:357, Loss: 0.0016\n",
      "Epoch:0, Batch:358, Loss: 0.0009\n",
      "Epoch:0, Batch:359, Loss: 0.0017\n",
      "Epoch:0, Batch:360, Loss: 0.0031\n",
      "Epoch:0, Batch:361, Loss: 0.0010\n",
      "Epoch:0, Batch:362, Loss: 0.0013\n",
      "Epoch:0, Batch:363, Loss: 0.0018\n",
      "Epoch:0, Batch:364, Loss: 0.0012\n",
      "Epoch:0, Batch:365, Loss: 0.0020\n",
      "Epoch:0, Batch:366, Loss: 0.0014\n",
      "Epoch:0, Batch:367, Loss: 0.0015\n",
      "Epoch:0, Batch:368, Loss: 0.0012\n",
      "Epoch:0, Batch:369, Loss: 0.0013\n",
      "Epoch:0, Batch:370, Loss: 0.0018\n",
      "Epoch:0, Batch:371, Loss: 0.0023\n",
      "Epoch:0, Batch:372, Loss: 0.0015\n",
      "Epoch:0, Batch:373, Loss: 0.0013\n",
      "Epoch:0, Batch:374, Loss: 0.0012\n",
      "Epoch:0, Batch:375, Loss: 0.0018\n",
      "Epoch:0, Batch:376, Loss: 0.0014\n",
      "Epoch:0, Batch:377, Loss: 0.0013\n",
      "Epoch:0, Batch:378, Loss: 0.0014\n",
      "Epoch:0, Batch:379, Loss: 0.0018\n",
      "Epoch:0, Batch:380, Loss: 0.0012\n",
      "Epoch:0, Batch:381, Loss: 0.0012\n",
      "Epoch:0, Batch:382, Loss: 0.0015\n",
      "Epoch:0, Batch:383, Loss: 0.0015\n",
      "Epoch:0, Batch:384, Loss: 0.0013\n",
      "Epoch:0, Batch:385, Loss: 0.0012\n",
      "Epoch:0, Batch:386, Loss: 0.0014\n",
      "Epoch:0, Batch:387, Loss: 0.0013\n",
      "Epoch:0, Batch:388, Loss: 0.0013\n",
      "Epoch:0, Batch:389, Loss: 0.0011\n",
      "Epoch:0, Batch:390, Loss: 0.0014\n",
      "Epoch:0, Batch:391, Loss: 0.0013\n",
      "Epoch:0, Batch:392, Loss: 0.0013\n",
      "Epoch:0, Batch:393, Loss: 0.0012\n",
      "Epoch:0, Batch:394, Loss: 0.0013\n",
      "Epoch:0, Batch:395, Loss: 0.0014\n",
      "Epoch:0, Batch:396, Loss: 0.0013\n",
      "Epoch:0, Batch:397, Loss: 0.0012\n",
      "Epoch:0, Batch:398, Loss: 0.0013\n",
      "Epoch:0, Batch:399, Loss: 0.0012\n",
      "Epoch:0, Batch:400, Loss: 0.0013\n",
      "Epoch:0, Batch:401, Loss: 0.0013\n",
      "Epoch:0, Batch:402, Loss: 0.0012\n",
      "Epoch:0, Batch:403, Loss: 0.0013\n",
      "Epoch:0, Batch:404, Loss: 0.0011\n",
      "Epoch:0, Batch:405, Loss: 0.0013\n",
      "Epoch:0, Batch:406, Loss: 0.0010\n",
      "Epoch:0, Batch:407, Loss: 0.0011\n",
      "Epoch:0, Batch:408, Loss: 0.0013\n",
      "Epoch:0, Batch:409, Loss: 0.0010\n",
      "Epoch:0, Batch:410, Loss: 0.0010\n",
      "Epoch:0, Batch:411, Loss: 0.0014\n",
      "Epoch:0, Batch:412, Loss: 0.0018\n",
      "Epoch:0, Batch:413, Loss: 0.0013\n",
      "Epoch:0, Batch:414, Loss: 0.0009\n",
      "Epoch:0, Batch:415, Loss: 0.0014\n",
      "Epoch:0, Batch:416, Loss: 0.0013\n",
      "Epoch:0, Batch:417, Loss: 0.0014\n",
      "Epoch:0, Batch:418, Loss: 0.0013\n",
      "Epoch:0, Batch:419, Loss: 0.0013\n",
      "Epoch:0, Batch:420, Loss: 0.0013\n",
      "Epoch:0, Batch:421, Loss: 0.0013\n",
      "Epoch:0, Batch:422, Loss: 0.0013\n",
      "Epoch:0, Batch:423, Loss: 0.0013\n",
      "Epoch:0, Batch:424, Loss: 0.0013\n",
      "Epoch:0, Batch:425, Loss: 0.0012\n",
      "Epoch:0, Batch:426, Loss: 0.0012\n",
      "Epoch:0, Batch:427, Loss: 0.0014\n",
      "Epoch:0, Batch:428, Loss: 0.0013\n",
      "Epoch:0, Batch:429, Loss: 0.0013\n",
      "Epoch:0, Batch:430, Loss: 0.0012\n",
      "Epoch:0, Batch:431, Loss: 0.0012\n",
      "Epoch:0, Batch:432, Loss: 0.0013\n",
      "Epoch:0, Batch:433, Loss: 0.0015\n",
      "Epoch:0, Batch:434, Loss: 0.0012\n",
      "Epoch:0, Batch:435, Loss: 0.0013\n",
      "Epoch:0, Batch:436, Loss: 0.0014\n",
      "Epoch:0, Batch:437, Loss: 0.0015\n",
      "Epoch:0, Batch:438, Loss: 0.0013\n",
      "Epoch:0, Batch:439, Loss: 0.0012\n",
      "Epoch:0, Batch:440, Loss: 0.0013\n",
      "Epoch:0, Batch:441, Loss: 0.0013\n",
      "Epoch:0, Batch:442, Loss: 0.0016\n",
      "Epoch:0, Batch:443, Loss: 0.0009\n",
      "Epoch:0, Batch:444, Loss: 0.0014\n",
      "Epoch:0, Batch:445, Loss: 0.0012\n",
      "Epoch:0, Batch:446, Loss: 0.0014\n",
      "Epoch:0, Batch:447, Loss: 0.0015\n",
      "Epoch:0, Batch:448, Loss: 0.0012\n",
      "Epoch:0, Batch:449, Loss: 0.0012\n",
      "Epoch:0, Batch:450, Loss: 0.0016\n",
      "Epoch:0, Batch:451, Loss: 0.0015\n",
      "Epoch:0, Batch:452, Loss: 0.0014\n",
      "Epoch:0, Batch:453, Loss: 0.0016\n",
      "Epoch:0, Batch:454, Loss: 0.0013\n",
      "Epoch:0, Batch:455, Loss: 0.0012\n",
      "Epoch:0, Batch:456, Loss: 0.0013\n",
      "Epoch:0, Batch:457, Loss: 0.0014\n",
      "Epoch:0, Batch:458, Loss: 0.0013\n",
      "Epoch:0, Batch:459, Loss: 0.0011\n",
      "Epoch:0, Batch:460, Loss: 0.0013\n",
      "Epoch:0, Batch:461, Loss: 0.0015\n",
      "Epoch:0, Batch:462, Loss: 0.0014\n",
      "Epoch:0, Batch:463, Loss: 0.0014\n",
      "Epoch:0, Batch:464, Loss: 0.0017\n",
      "Epoch:0, Batch:465, Loss: 0.0013\n",
      "Epoch:0, Batch:466, Loss: 0.0011\n",
      "Epoch:0, Batch:467, Loss: 0.0012\n",
      "Epoch:0, Batch:468, Loss: 0.0013\n",
      "Epoch:0, Batch:469, Loss: 0.0012\n",
      "Epoch:0, Batch:470, Loss: 0.0010\n",
      "Epoch:0, Batch:471, Loss: 0.0012\n",
      "Epoch:0, Batch:472, Loss: 0.0013\n",
      "Epoch:0, Batch:473, Loss: 0.0013\n",
      "Epoch:0, Batch:474, Loss: 0.0011\n",
      "Epoch:0, Batch:475, Loss: 0.0015\n",
      "Epoch:0, Batch:476, Loss: 0.0014\n",
      "Epoch:0, Batch:477, Loss: 0.0015\n",
      "Epoch:0, Batch:478, Loss: 0.0018\n",
      "Epoch:0, Batch:479, Loss: 0.0012\n",
      "Epoch:0, Batch:480, Loss: 0.0016\n",
      "Epoch:0, Batch:481, Loss: 0.0011\n",
      "Epoch:0, Batch:482, Loss: 0.0015\n",
      "Epoch:0, Batch:483, Loss: 0.0007\n",
      "Epoch:0, Batch:484, Loss: 0.0009\n",
      "Epoch:0, Batch:485, Loss: 0.0013\n",
      "Epoch:0, Batch:486, Loss: 0.0013\n",
      "Epoch:0, Batch:487, Loss: 0.0018\n",
      "Epoch:0, Batch:488, Loss: 0.0016\n",
      "Epoch:0, Batch:489, Loss: 0.0011\n",
      "Epoch:0, Batch:490, Loss: 0.0015\n",
      "Epoch:0, Batch:491, Loss: 0.0014\n",
      "Epoch:0, Batch:492, Loss: 0.0010\n",
      "Epoch:0, Batch:493, Loss: 0.0014\n",
      "Epoch:0, Batch:494, Loss: 0.0011\n",
      "Epoch:0, Batch:495, Loss: 0.0015\n",
      "Epoch:0, Batch:496, Loss: 0.0011\n",
      "Epoch:0, Batch:497, Loss: 0.0013\n",
      "Epoch:0, Batch:498, Loss: 0.0020\n",
      "Epoch:0, Batch:499, Loss: 0.0013\n",
      "Epoch:0, Batch:500, Loss: 0.0012\n",
      "Epoch:0, Batch:501, Loss: 0.0011\n",
      "Epoch:0, Batch:502, Loss: 0.0013\n",
      "Epoch:0, Batch:503, Loss: 0.0013\n",
      "Epoch:0, Batch:504, Loss: 0.0013\n",
      "Epoch:0, Batch:505, Loss: 0.0011\n",
      "Epoch:0, Batch:506, Loss: 0.0013\n",
      "Epoch:0, Batch:507, Loss: 0.0013\n",
      "Epoch:0, Batch:508, Loss: 0.0014\n",
      "Epoch:0, Batch:509, Loss: 0.0013\n",
      "Epoch:0, Batch:510, Loss: 0.0017\n",
      "Epoch:0, Batch:511, Loss: 0.0012\n",
      "Epoch:0, Batch:512, Loss: 0.0016\n",
      "Epoch:0, Batch:513, Loss: 0.0012\n",
      "Epoch:0, Batch:514, Loss: 0.0010\n",
      "Epoch:0, Batch:515, Loss: 0.0011\n",
      "Epoch:0, Batch:516, Loss: 0.0016\n",
      "Epoch:0, Batch:517, Loss: 0.0013\n",
      "Epoch:0, Batch:518, Loss: 0.0012\n",
      "Epoch:0, Batch:519, Loss: 0.0012\n",
      "Epoch:0, Batch:520, Loss: 0.0015\n",
      "Epoch:0, Batch:521, Loss: 0.0014\n",
      "Epoch:0, Batch:522, Loss: 0.0012\n",
      "Epoch:0, Batch:523, Loss: 0.0013\n",
      "Epoch:0, Batch:524, Loss: 0.0013\n",
      "Epoch:0, Batch:525, Loss: 0.0007\n",
      "Epoch:0, Batch:526, Loss: 0.0011\n",
      "Epoch:0, Batch:527, Loss: 0.0012\n",
      "Epoch:0, Batch:528, Loss: 0.0012\n",
      "Epoch:0, Batch:529, Loss: 0.0013\n",
      "Epoch:0, Batch:530, Loss: 0.0010\n",
      "Epoch:0, Batch:531, Loss: 0.0013\n",
      "Epoch:0, Batch:532, Loss: 0.0010\n",
      "Epoch:0, Batch:533, Loss: 0.0018\n",
      "Epoch:0, Batch:534, Loss: 0.0011\n",
      "Epoch:0, Batch:535, Loss: 0.0016\n",
      "Epoch:0, Batch:536, Loss: 0.0010\n",
      "Epoch:0, Batch:537, Loss: 0.0014\n",
      "Epoch:0, Batch:538, Loss: 0.0011\n",
      "Epoch:0, Batch:539, Loss: 0.0016\n",
      "Epoch:0, Batch:540, Loss: 0.0021\n",
      "Epoch:0, Batch:541, Loss: 0.0011\n",
      "Epoch:0, Batch:542, Loss: 0.0014\n",
      "Epoch:0, Batch:543, Loss: 0.0016\n",
      "Epoch:0, Batch:544, Loss: 0.0013\n",
      "Epoch:0, Batch:545, Loss: 0.0012\n",
      "Epoch:0, Batch:546, Loss: 0.0014\n",
      "Epoch:0, Batch:547, Loss: 0.0013\n",
      "Epoch:0, Batch:548, Loss: 0.0014\n",
      "Epoch:0, Batch:549, Loss: 0.0011\n",
      "Epoch:0, Batch:550, Loss: 0.0012\n",
      "Epoch:0, Batch:551, Loss: 0.0014\n",
      "Epoch:0, Batch:552, Loss: 0.0014\n",
      "Epoch:0, Batch:553, Loss: 0.0016\n",
      "Epoch:0, Batch:554, Loss: 0.0012\n",
      "Epoch:0, Batch:555, Loss: 0.0013\n",
      "Epoch:0, Batch:556, Loss: 0.0018\n",
      "Epoch:0, Batch:557, Loss: 0.0018\n",
      "Epoch:0, Batch:558, Loss: 0.0016\n",
      "Epoch:0, Batch:559, Loss: 0.0021\n",
      "Epoch:0, Batch:560, Loss: 0.0010\n",
      "Epoch:0, Batch:561, Loss: 0.0023\n",
      "Epoch:0, Batch:562, Loss: 0.0010\n",
      "Epoch:0, Batch:563, Loss: 0.0015\n",
      "Epoch:0, Batch:564, Loss: 0.0017\n",
      "Epoch:0, Batch:565, Loss: 0.0011\n",
      "Epoch:0, Batch:566, Loss: 0.0029\n",
      "Epoch:0, Batch:567, Loss: 0.0033\n",
      "Epoch:0, Batch:568, Loss: 0.0031\n",
      "Epoch:0, Batch:569, Loss: 0.0010\n",
      "Epoch:0, Batch:570, Loss: 0.0035\n",
      "Epoch:0, Batch:571, Loss: 0.0013\n",
      "Epoch:0, Batch:572, Loss: 0.0019\n",
      "Epoch:0, Batch:573, Loss: 0.0023\n",
      "Epoch:0, Batch:574, Loss: 0.0019\n",
      "Epoch:0, Batch:575, Loss: 0.0034\n",
      "Epoch:0, Batch:576, Loss: 0.0021\n",
      "Epoch:0, Batch:577, Loss: 0.0009\n",
      "Epoch:0, Batch:578, Loss: 0.0016\n",
      "Epoch:0, Batch:579, Loss: 0.0022\n",
      "Epoch:0, Batch:580, Loss: 0.0012\n",
      "Epoch:0, Batch:581, Loss: 0.0020\n",
      "Epoch:0, Batch:582, Loss: 0.0010\n",
      "Epoch:0, Batch:583, Loss: 0.0019\n",
      "Epoch:0, Batch:584, Loss: 0.0036\n",
      "Epoch:0, Batch:585, Loss: 0.0027\n",
      "Epoch:0, Batch:586, Loss: 0.0011\n",
      "Epoch:0, Batch:587, Loss: 0.0026\n",
      "Epoch:0, Batch:588, Loss: 0.0053\n",
      "Epoch:0, Batch:589, Loss: 0.0019\n",
      "Epoch:0, Batch:590, Loss: 0.0020\n",
      "Epoch:0, Batch:591, Loss: 0.0040\n",
      "Epoch:0, Batch:592, Loss: 0.0029\n",
      "Epoch:0, Batch:593, Loss: 0.0033\n",
      "Epoch:0, Batch:594, Loss: 0.0025\n",
      "Epoch:0, Batch:595, Loss: 0.0007\n",
      "Epoch:0, Batch:596, Loss: 0.0019\n",
      "Epoch:0, Batch:597, Loss: 0.0011\n",
      "Epoch:0, Batch:598, Loss: 0.0007\n",
      "Epoch:0, Batch:599, Loss: 0.0054\n",
      "Epoch:0, Batch:600, Loss: 0.0031\n",
      "Epoch:0, Batch:601, Loss: 0.0037\n",
      "Epoch:0, Batch:602, Loss: 0.0016\n",
      "Epoch:0, Batch:603, Loss: 0.0046\n",
      "Epoch:0, Batch:604, Loss: 0.0023\n",
      "Epoch:0, Batch:605, Loss: 0.0010\n",
      "Epoch:0, Batch:606, Loss: 0.0046\n",
      "Epoch:0, Batch:607, Loss: 0.0056\n",
      "Epoch:0, Batch:608, Loss: 0.0014\n",
      "Epoch:0, Batch:609, Loss: 0.0030\n",
      "Epoch:0, Batch:610, Loss: 0.0055\n",
      "Epoch:0, Batch:611, Loss: 0.0022\n",
      "Epoch:0, Batch:612, Loss: 0.0013\n",
      "Epoch:0, Batch:613, Loss: 0.0021\n",
      "Epoch:0, Batch:614, Loss: 0.0011\n",
      "Epoch:0, Batch:615, Loss: 0.0014\n",
      "Epoch:0, Batch:616, Loss: 0.0014\n",
      "Epoch:0, Batch:617, Loss: 0.0025\n",
      "Epoch:0, Batch:618, Loss: 0.0011\n",
      "Epoch:0, Batch:619, Loss: 0.0017\n",
      "Epoch:0, Batch:620, Loss: 0.0036\n",
      "Epoch:0, Batch:621, Loss: 0.0012\n",
      "Epoch:0, Batch:622, Loss: 0.0025\n",
      "Epoch:0, Batch:623, Loss: 0.0021\n",
      "Epoch:0, Batch:624, Loss: 0.0018\n",
      "Epoch:0, Batch:625, Loss: 0.0012\n",
      "Epoch:0, Batch:626, Loss: 0.0043\n",
      "Epoch:0, Batch:627, Loss: 0.0023\n",
      "Epoch:0, Batch:628, Loss: 0.0011\n",
      "Epoch:0, Batch:629, Loss: 0.0039\n",
      "Epoch:0, Batch:630, Loss: 0.0027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 89\u001B[0m\n\u001B[1;32m     87\u001B[0m         optimiser\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     88\u001B[0m         loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 89\u001B[0m         \u001B[43moptimiser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m         counter \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     92\u001B[0m training_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\u001B[38;5;241m-\u001B[39mstart_time\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llm_venv/lib/python3.9/site-packages/torch/optim/optimizer.py:385\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    381\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    382\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    383\u001B[0m             )\n\u001B[0;32m--> 385\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    388\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llm_venv/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llm_venv/lib/python3.9/site-packages/torch/optim/adam.py:166\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    155\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    157\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    158\u001B[0m         group,\n\u001B[1;32m    159\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    163\u001B[0m         max_exp_avg_sqs,\n\u001B[1;32m    164\u001B[0m         state_steps)\n\u001B[0;32m--> 166\u001B[0m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llm_venv/lib/python3.9/site-packages/torch/optim/adam.py:316\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    314\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[0;32m--> 316\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    319\u001B[0m \u001B[43m     \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    320\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    321\u001B[0m \u001B[43m     \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    322\u001B[0m \u001B[43m     \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    323\u001B[0m \u001B[43m     \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    324\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[43m     \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    326\u001B[0m \u001B[43m     \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    327\u001B[0m \u001B[43m     \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    328\u001B[0m \u001B[43m     \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    329\u001B[0m \u001B[43m     \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    330\u001B[0m \u001B[43m     \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    331\u001B[0m \u001B[43m     \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    332\u001B[0m \u001B[43m     \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    333\u001B[0m \u001B[43m     \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llm_venv/lib/python3.9/site-packages/torch/optim/adam.py:439\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[1;32m    437\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (max_exp_avg_sqs[i]\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 439\u001B[0m         denom \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbias_correction2_sqrt\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_\u001B[49m\u001B[43m(\u001B[49m\u001B[43meps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    441\u001B[0m     param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n\u001B[1;32m    443\u001B[0m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# ...\n",
    "\n",
    "# Scale the target variable\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "transformed_Y_train = scaler_y.fit_transform(Y_train.values.reshape(-1,1))\n",
    "transformed_Y_test = scaler_y.transform(Y_test.values.reshape(-1,1))\n",
    "\n",
    "# Scale the features\n",
    "scaler_x = MinMaxScaler(feature_range=(-1, 1))\n",
    "transformed_X_train = scaler_x.fit_transform(X_train)\n",
    "transformed_X_test = scaler_x.transform(X_test)\n",
    "\n",
    "# Create tensors\n",
    "x_train = torch.from_numpy(transformed_X_train).to(torch.float32)\n",
    "x_test = torch.from_numpy(transformed_X_test).to(torch.float32)\n",
    "y_train = torch.from_numpy(transformed_Y_train).to(torch.float32)\n",
    "y_test = torch.from_numpy(transformed_Y_test).to(torch.float32)\n",
    "\n",
    "# Create a dataset and data loader\n",
    "train_dataset = TensorDataset(x_train.unsqueeze(1), y_train)  # Add sequence dimension\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "# ...\n",
    "\n",
    "# Training loop\n",
    "for t in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        x_batch, y_batch = batch\n",
    "        x_batch = x_batch.reshape(-1, 1, input_dim)  # Reshape to (batch_size, seq_len, input_dim)\n",
    "        y_train_pred = model(x_batch)\n",
    "        loss = criterion(y_train_pred, y_batch)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        print(f'Epoch:{t+1}, Loss: {loss.item():.4f}')"
   ],
   "id": "b6d77e3f01279577",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c9ee1d078bf6799f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
